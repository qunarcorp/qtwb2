# 去哪儿旅行混沌工程落地实践

# **一、前言**

​      从Netflix提出混沌工程的概念，到如今各个公司都在探索混沌工程的实践，混沌工程作为服务韧性治理的系统性解决方案，已经证明了它的价值。但作为一门新兴的技术学科，从理论到落地实践，再到达到预期价值，还有着很长的路要走。
​      去哪网从2019年开始进行混沌工程实践，三年多的时间里，经历了从探索、发展到创新的不同阶段。并且每个阶段都与业务紧密结合，力求在每个阶段都做到业务价值最大化，秉持着从业务中来到业务中去，走出了一条与去哪儿业务特点及内部基础设施紧密结合的落地之路。如今台已经建设成了从测试到线上，从机房层到应用层，从简单策略到复杂组合策略，多维度、多场景自由组合的混沌演练平台。

​      回顾这几年去哪儿网的混沌平台建设，中间遇到过很多困难，也走过弯路。尤其是到了后期的创新阶段，没有了业界成熟案例的参考，挑战越来越大。
​      但我们始终坚持了以业务实际作为出发点，以内部生态作为落脚点，最终混沌平台与压测平台，自动化测试平台一起组成了公司服务韧性治理三剑客，保障了去哪儿庞大业务持续稳定运转。下面我将详细介绍混沌平台建设过程，希望能给到同样期望落地混沌工程的朋友一些启发。

# **二、背景**

## **2.1 频发的故障**

去哪儿网成立于2005年，作为一个老牌互联网公司，业务经历了多个阶段的发展，内部架构也在持续为了适应业务发展而在做调整。变化的业务，变化的架构给服务韧性带来了极大的挑战。2019年，公司发生了多次P1故障，几个典型故障列举如下：

    1. 携程机房断电，影响时间长达40个小时。
    2. ZK集群故障，故障时间15分钟。
    3. QMQ部分消息发送失败故障持续69分钟。
    4. KV系统主库导致金融服务不可用持续18分钟。
以上问题对去哪儿造成了重大的损失，但可以评估的是订单损失，不可评估的是用户信心，品牌形象。基础平台作为公司重要技术支撑部门，这些问题，更是对我们提出的拷问：

    1. 为什么应急预案没生效？
    2. 为什么解决时间这么长？
    3. 如何避免？

​        因此我们下定决心要找到服务韧性治理的系统化解决方案，对造成线上重大故障的问题，做到御敌于千里之外。彼时混沌工程理论已经成型，国内外都有一些成功实践，我们也在很早之前也有了用混沌进行稳定性治理的想法。但鉴于虽然已经有一些先驱公司进行了不同程度的探索，但实际投入产出比很难评估，我们对比国内外一线大厂在可投入资源上有着比较大的限制。因此我们的课题，是要用最具性价比的方式，解决核心问题，获得公司信任和支持，进而能够持续投入更多资源。基于以上，我们分析了公司技术和业务现状，期望找到最佳的切入点。
## **2.2 技术&业务现状**

### ***2.2.1 不可靠的基础设施***

![image-20220920001621170](/Users/wufan/GIT_HUB/qtwb/images/system_resiliency/2.2.1.png)


### **2.2.2 复杂的集群**

​        这里列举去哪儿旅行一些数据，线上跑着的活跃的应用有 3000 多个，dubbo 接口有 18000 多个，网关上注册的域名有 3500 多个，qmq13000 多个，技术栈有 5 种语言，大规模的系统群和生态很难保证完全可靠，任意一个系统有问题，都可能影响最终的结果。

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8X9iby5jLXBPLeRCJ2P1ico8Cu3ia6fiaic0b8YM8DdUL9z6uSiaKRl8yTibOA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### **2.2.3 常见故障类型**

​     我们按常见的故障原因，将故障划分为以下几类：

    1.机房问题：机房断电、网络不通、 网络延迟。
    2.中间件问题：zk集群故障、mq故障、 数据库故障、缓存故障。
    3.机器问题：load高、cpu满、 磁盘满、IO满。
    4.应用问题：fullGC、服务下线、 日志拖慢、线程池满。
    5.依赖问题：下游dubbo/http接口延迟、抛异常。

## **2.3 我们的切入点**
![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8X5ribvdZ8AGmJWrXwxuxfrOwM5oCLRaqOVjnCibNmAydomACDLb3xqTQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)      

​      基于以上架构层次划分，我们分析发现越是影响重大的故障，就越是底层问题，其中P1故障基本都与机房或中间件故障有关。综合以上我们优先做的是**机房演练**、**中间件演练**。

# **三、混沌工程探索** 

## **3.1** 技术选型

​            在确定了机房演练、中间件演练的目标后，我们开始了技术调研和准备。在当时的环境下得出如下结论：

开源工具完善度低。尤其对于大规模的网络尤其在我们非容器化的环境下，进行大规模网络、机器，以及特定中间件故障模拟如zk等很难找到有效的支持工具。完全借助模拟的方式实现，成本大且覆盖不完整，不能实现我们快速实施混沌工程，为业务快速贡献价值的目标。

​          因此我们决定，采用关机这种简单粗暴的方式直接进行演练，以最真实的效果，最快速的方式直接落地。当时我们的应用都是在kvm环境下，决定利用OpenStack API 来进行批量关机演练操作。根据混沌工程理论，最主要挑战就是：如何控制爆炸半径控制以及如何快速止损恢复。
## **3.2 问题及解决方案**
### **3.2.1 爆炸半径控制**

    1.机房聚合信息查询，方便应用改造。
    2.演练规模从小到大，梯次递增。
    3.业务低峰期进行。
    4.充分的的前期主备，详细的checklist(单机配置、本机的资源文件、服务的热点机群、外网域名等十几项前期需要check的点)。

### **3.2.2 止损恢复**

    1.自动建立沟通群，进度周知。
    2.人工值守。
    3.接入告警，告警事件关联推送。
    4.一键恢复脚本。
    5.与业务线联动，提前做好用户周知和补偿准备。

## **3.3 落地实施**

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8JcmhMTbWMygxTAGmdGibe7wmJ7GBl04w1AiaAwgxPQbKvsxgzEc9s6fA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

## **3.4 演练总结**
  发现 10+影响业务稳定性重要问题，举例：

    1.zk 稳定性问题。 
    2.客户端 QP 发布服务跨机房高可用问题。
# **四、混沌平台建设** 

### **4..1 概述**

​        有了以上关机演练的成功实施，以及对服务韧性立竿见影的效果。让我们确信通过混沌工程的实践，可以实现我们提高服务韧性的目标，让我们有了投入更大资源持续发展的信心。但上面演练中也暴露出了我们演练中存在的各种问题，自动化能力不足、人工成本大，故障模拟能力不足，稳态检测能力不足，爆炸半径控制精准度差。基于此，我们又进行了更深入的技术调研，并确立从场景覆盖度，底层到上层，线下到线上发展路径，从现如今的视角总结为如下阶段性成果。

### 4.2 混沌平台建设的三个阶段

​	第一阶段：基础能力建设-应用演练。

   	 	1.丰富故障场景，从中间件到依赖层场景全覆盖。
   	 	2.完善的稳态检测能力。
   	 	3.搭建控制平台，以应用为维度进行故障编排。

​	第二阶段：线上常态演练-攻防演练。

​			精准控制爆炸半径，可以在线上做到自由演练，屏蔽对真实用户的影响。	

​	第三阶段：架构治理-强弱依赖演练。

​            利用第二阶段自动化演练的能力，对线上复杂的业务通过创新性的演练编排发现架构的不合理之处，推进从架构层面进行持续改进，防止架构腐化。

### 4.3 基础能力建设-应用演练

​        有了目标，还要找到落地的抓手，做到底层建设与业务价值相结合，才能有的放矢，目标聚焦，我们这一阶段选择的落脚点为应用演练。总结起来就是：

		1.选择合适的开源工具，进行故障场景模拟。
		2.基于开源二次开发，支持公司内部自研中间件故障场景以及缺失的企业级场景。
		3.开发控制平台，以应用为组织维度，进行演练编排。
		4.打通公司watcher内部监控平台，radar自动问题发现平台，支持人工录入演练监控指标。
### 4.3.1 技术选型

​         当时维护比较好、用的比较多工具有 Chaosblade、  Chaos Mesh，对比如下：   

| 组件       | 支持平台 | 支持场景 | 开源 | 整体性            | 侵入型 |
| :--------- | :------- | :------- | :--- | :---------------- | :----- |
| Chaos Mesh | K8S      | 丰富     | 是   | 好                | 无     |
| Chaosblade | VM/K8S   | 丰富     | 是   | 差(当时只有agent) | 低     |

​        1.我们重视的场景丰富度上相近。

​        2.VM/K8S双平台支持只有Chaosblade，我们当时线上都kvm但正在规划容器化，需要双平台都支持。

综合考虑：我们选择了Chaosblade。



### 4.3.2 场景覆盖

chaosblade 支持多层面的故障演练，比如基础资源层面的、应用服务层面的。同时还支持 k8s 。并且它支持的场景非常丰富，基本上涵盖了需求的各个方面，但是还有一些场景是缺失的。主要如下：

- HTTP超时
- fullgc
- 日志拥堵
- 调用点区分
- 链路匹配

对于这些不支持的场景，在落地的过程中，我们支持了这些形态，并且将这些改动提交到社区，参与开源共建。

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8ogHOkN3SnlQa9s70j6dib7V054kV59N12OqAQiax6DdmDSJk8Cs6SXDA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 4.3.3 自研控制面

​			1.Chaosblade 提供了控制面Chaosblade-box，支持以host为组织维度，进行故障编排。但我们公司内部业务是以应用树-应用-host的层次结构进行组织的，我们实际的业务场景中可能会需要根据某个业务节点筛选演练的应用。直接使用开源方案，灵活性差，观测性差，并且还会涉及到很多数据同步的问题。

​			2.我们后面规划中会在依赖层，做很多与公司内部实际业务强相关的演练编排、动态交互，开源架构并不适合我们的业务场景。

​	基于以上两点，我们选择自研控制面板的方案，设计了以应用为组织维度，host为最小组成粒度的灵活的演练控制面板，具体如下：

![image-20220920030040850](/Users/wufan/GIT_HUB/qtwb/images/system_resiliency/4.1.3.3.png)

### 4.3.4 完善的稳态观测能力

​        在混沌理论中，稳态观测主要起到两个作用：

​        1.确定系统运行是否符合演练预期。

​        2.系统状态超出预期，快速止损。

​       因此完善的稳态观测能力，不仅是演练正确实施的前提，更是能够降低线上影响的核心手段。基于此，我们设计了基于qunar内部监控系统watcher并结合公司AIOPS工具radar做双重稳态观测手段。

​         其中watcher指标我们接入了

​        1.业务核心监控面板即公司的各个业务线的监控报警大盘。

​        2.机器稳定性报警。

​        3.人工单独标记的混沌报警。
### 4.3.5 小结
​        通过以上关键基础能力的建设，我们具备了对任意应用，进行从机器到依赖层面全场景的演练能力，将核心能力抽象到平台，为QA、DEV提供了强大的演练武器库，可以随时、随地自定义演练。做到了功能强大，成本低廉。

### 4.4 线上常态演练-攻防演练

### 4.4.1 思考

​          应用演练，在公司运行一段时间以后，统计平台数据后发现。90%以上的演练是在测试环境做为测试场景的模拟工具来进行使用的。即使线上进行的演练，也基本都是只对单应用、单机器、单策略的演练。但实际故障发生时，更多是全量机器，甚至多个策略、多个应用组合的下故障场景，但我们也很容易理解以上结果出现的原因：

    1.我们目前线上演练无法屏蔽对真实用户的影响，线上演练即有损演练。
    2.更大规模的演练，意味着对线上更大的损害。
    3.没有闭环机制保障。

​     如果能够降低演练对真实用户的影响，并建立一个闭环保障机制，便可以真正在线上推行常态化演练，充分发挥混沌平台的价值。

### 4.4.2 降低演练影响

  #### 4.4.2.1 方案选型
结合公司内部生态，经过调研，我们有如下两套可行的方案。
​          **方案一：**为混沌演练单独建立一套运行环境，该环境类似灰度环境，可以灰度用户比例。我们所有的演练运行在这套环境之上，对用户的影响便可以通过控制用户灰度比例来进行控制。
​          **方案二：**模拟用户请求，故障注入只针对模拟的用户请求。

|  方案 | 优势 | 劣势 |
|  :--:  | ----  |  ----  |
| 方案一 | 来自用户真实请求，能够完全模拟线上真实行为 |1.仍然对灰度的用户造成影响。 <br>2.构建单独的环境需要额外的成本，在公司当时的环境下，需要动态路由功能的支持，动态路由方案只在测试环境支持，迁移到线上方案重新论证，以及较大的改造成本和风险(**关于动态路由可参考-测试环境治理实践-环境治理-动态路由设计**)。<br>3.用户流量比例受限，可能因为流量低，不能发现真实问题。 |
| 方案二 | 完全消除用户影响 <br> | 并非真实用户请求，可能存在差异 。 |

​          基于以上方案对比，我们认为模拟的请求与真实请求的差异可通过技术手段尽量降低，并且模拟请求可以做到完全消除影响，因此决定选择用户请求模拟的方案。 

#### 4.4.2.2  方案详情

![](/Users/wufan/GIT_HUB/qtwb/images/system_resiliency/线上演练.png)



​                 **1.case生成。**

​                          通过自动化测试平台生成压测case。

​                  **2.流量染色。**

​                         压测脚本发压时，在公司链路追踪工具qtrace中埋下流量标识，通过qtrace将流量标识全链路透传。

​                 **3.开源java agent改造，支持故障模拟匹配流量染色。**   

​                          出于回馈开源的考虑，我们将此功能开发成了通用的业务参数匹配。并支持了两种匹配模式：  1）SPI模式，用户可以写代码自定义匹配逻辑。2）固定 key匹配模式，不用写代码只需在配置的http header 、dubbo attachment携带相应业务数据，即可自动读取匹配。

### 4.4.3 闭环机制-攻防演练

​           1、攻击点编排：选择历史高频故障进行场景设计。

​           2、攻击点上报：防守方定位排查后，上报给攻击方 攻击方确认，正确则得分。

​           3、攻击终止：防守方定位成功或者超时自动终止。

​           4、积分：根据定位时长、故障难易程度进行积分排名、公示。

​           5、复盘：过程中发现的问题进行修复。

​           6、线上各业务线定期执行。

### 4.4 架构治理-强弱依赖演练
### 4.4.1 强弱依赖定义
![](/Users/wufan/GIT_HUB/qtwb/images/system_resiliency/强弱依赖.png)     



- A、B、C、D、E代表应用。

- A1-E2分别代表各应用对外提供的接口。

- **调用链路：**分别有A1->B1,C1->D1 ，A1->B1,C1->E1，A2->B2,C2->D2，A2->B2,C2->E2 四条调用链路。

- **依赖：**A1->B1->D1调用链路中分别有A->B1,B1->D1两个依赖。

- **入口**：最外层接口我们成为入口，即上图A1、A2。

- **强弱依赖：**当依赖出故障时，对入口返回数据有影响即为强依赖，无影响及为弱依赖。

  举例说明：  

​          当B1调用D1出现故障时，如果A1可以正常提供用户服务则将依赖B1->D1称为入口A1的弱依赖。   

### 4.4.2 基于强弱依赖的架构治理

​        我们统计线上故障，相当一部分比例是因为某个依赖问题导致的级联故障，这种级联的影响，在整个调用链路中是否合理，是否存在不重要的接口影响了核心业务。在几百个应用在错综复杂的链路中，通过人工保障非常困难。如果能够通过自动化的方式分析出核心业务下游各个依赖的强弱情况，并不断推进研发降低强依赖比例，可以达到提高线上稳定性的效果。

​        基于公司分布式链路追踪工具qtrace，我们能基于入口接口拿到到调用链路和依赖信息。再加上混沌平台已经具备的线上演练能力，我们可以为每个依赖分别注入故障，对比注入故障前后相同请求参数下入口返回结果是否一致，来判断依赖的强弱。

​       基于上面思考，我们设计强弱依赖演练，完整演练流程如下图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8sCh5dvooZW2ks4TK7Xoz6Y76oaTTbaq3iaaPdFyj7uicOhfVicf7hIECQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)


### 4.4.3 难点

#### 4.4.3.1 依赖命中率 

​        先来解释下依赖命中的概念，在故障注入期间，有压测流量打到当前的依赖，才算命中。下图是某一个入口依赖的的拓扑图，我们的演练范围是要覆盖整个链路的每一个接口的，如果要保证命中率，那 case 筛选逻辑就很重要，我们希望的是通过筛选之后，能保证命中率在 90% 以上，这样演练才是有效果的。

​       case 筛选策略有两种，第一种是从入口处的应用，随机取若干条，从入口开始请求，这个方式比较简单粗暴，但是问题是覆盖率很难保证。举个例子，比如说用户在我们平台买机票，它可以买北京到上海的单程，也可以买北京到上海的往返。单程和往返这两种类型的报价，在服务端，是不同的系统来提供服务的，如果我随机筛选的 case 里，只有单程的请求，没有往返的请求，那就只能命中单程的链路，往返这条链路上的所有依赖就都没办法命中。第二种策略是精确匹配，比如系统 d 调用系统 e 提供的接口 f ，准确的找到系统 d 调用系统 e 的 f 接口的 trace，和入口做关联，发起请求，这样就能保证依赖被覆盖到。

​        但精准匹配的难点是系统我们链路追踪系统的支持，相当于要链路追踪系统收集到线上所有trace请求并记录相关的接口数据，然后提供查询功能。描述起来简单，但在数据量上对链路追踪系统是一个非常恐怖的要求，性能上存在较大问题。我们为此做了很多工作，来满足性能要求。具体改造点如下：

​       1.trace span  存储方案：从hbase 改为clickhouse，可以通过简历多维度索引的方式，支持类SQL的多字段匹配查询。

​      2.在自动化测试平台获取到的case 作为原始trace ，将这些tarace 给到clickhouse 作为原始过滤数据集，极大的降低了clickhouse的压力。

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8WHibdX2uiahQmuyb8nZoTFB2TI0kiafCQeDvZ12tktnhwjafWgtlxJywQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### 4.4.3.2  自动化断言     

​        自动化断言的核心逻辑是根据下游依赖注入故障前后，diff入口返回结果，来判断强弱依赖。这个问题的前提是，未注入故障的情况同样的请求能保持幂等性。要求是相同的请求参数，在任意时克请求，返回结果都是一致的。实际业务中并非所有的请求都是幂等的，有的与接口设计相关，比如接口中返回了时间戳字段等。有的则与业务相关，比如同样是北京到上海的航班列表，不同时间的返回结果在业务逻辑上就是不一样的。

​       综合以上影响业务结果不一致的主要有两种情况：

​     1.接口设计中一些非核心业务字段影响了断言结果，比如时间戳等。这种情况我们可以通过将时间戳这类非关键字段排除diff的方式解决，但如果人工在众多入口接口中识别这些非关键字段，工作量很大且容容易判断出错，我们的解决方案是：通过未注入故障的情况同时请求两次，自动diff结果差异，来进行智能降噪。

​     2.业务本身在不同的时刻请求就是不一致的。

​        这个在设计之初我们对这个问题的考虑是不足的，设计的基准请求和故障请求之间是是有2分钟左右的时间间隔的，后期我们做自动化断言准确性分析，才发现这样在2分钟间隔下幂等性消失的接口量非常大，会极大的影响我们断言的准确性，我们后面的改进的目标就是让基准请求和故障请求可以同时进行。在这里我们联合我们的压测平台进行了从发压层面的改动，来适配我们整体方案。请参见下图前后方案对比：

​              

​     

#### 

# **五、总结**

​        本文为了方便理解，将以上阶段进行了严格划分，在实际的逻辑过程中，以上各阶段实际是交织在一起的。我们故障场景的支持，很多是随着业务需要和技术发展不断完善的。攻防演练和强弱依赖演练，在实际建设中也是并行进行的。至于具体落地步骤，总体是遵循从底层到上层，从线下到线上，从单一场景到复杂场景，并紧密结合业务当前需要。建议同样进行混沌落地的朋友可以根据自己公司业务实际需要，灵活调整。

​        每个阶段有不同的复杂度，其中基础能力建设主要集中在底层技术，比如熟悉开源架构，agent源码改造。线上自动演练主要是充分利用公司的内部成熟技术，需要与公司其它平台一起做好架构规划，从公司整体技术路径上做好架构规划，需要公司整体的有力支持，在我们内部压测平台，watcher，radar等平台都在基础平台维护，因此从组织架构上也比较好的保障了协同规划。

​        架构治理这块的难点主要是业务复杂度，需要深入公司各个业务线的业务实际，在不断踩坑，不断纠错调整中，不断提高数据覆盖度和准确性，除了技术外，更重要的是业务挑战，而这些恰恰是做技术平台的研发欠缺的，本人也从这个阶段的建设中，对公司整体业务有个更深入的理解。

​      

