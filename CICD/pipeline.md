---
typora-root-url: ./..
---

## 1.背景

### 1.1效率质量的平衡

互联网背景下，如何更快也更高质量的交付我们的产品已经成为了一个越来越重要的课题；

一般看来快速和高质量有互斥的地方，需要做均衡和取舍；在技术发展的今天，各个模块功能愈加完备，提升效率的工具也越来越多，如何提高质量的方式和手段也愈加的花样繁多；

但一些提效的手段提高的效率也被越来越多的流程或者检查所拖累，各种功能模块之间的数据同步、信息流转甚至审查也越来越难以协调，需要花费大把的时间在此上，且每一次迭代都需要重复以上流程；

此种状态之下我们急需一种能够协调效率和质量的工具，降低每一个工程师及其它参与者的心智负担和学习成本，实现**效率和质量的双赢**；

### 1.2去哪儿面临的效率困境

在去哪儿，我们为了保证交付效率和质量做了各种工具建设，以下是我们的一个需求从产生到最终的上线要经历的全部过程：

![image](/images/CICD/pipeline1.png)

从上图可以看出这个流程是非常长的，然后又是不可或缺的，因此我们需要做的就是将这个流程尽可能的自动化，过程对用户无感知，结果将关键信息暴漏给用户即可，因此我们实现了codereview的自动创建、code-push静态代码自动扫描、软路由环境、一键部署等，即我们做了很多单点的建设，而且为了最大程度的发挥大家的主管能动性，这些单点建设分布在不同的团队里，不同团队的开发习惯、服务方式都不一样，这种情况在基础设施不完善的情况下，效率提升非常明显，但是基础设施已经建设完全，问题就逐渐暴漏：

- 集成成本变高：上述所说不同点的效率、质量保障可能是不同团队提供的，不同团队的开发风格差异非常大，可能是一个http接口、也可能是一个消息推送、还可能的是一条广播事件，但是最终都要集成到统一的流程中，比如安全同学提供了漏洞扫描组件、业务团队需要在部署完成进行业务检查等，由于事先没有统一的规范，那么最终只能对单独功能进行适配，开发维护成本都非常高；
- 上线风险变大：流程迭代越来越长，尤其是为了保证交付的质量，我们会做各种各样的自动化检查，比如部署完成的接口自动化，代码变更后的配置修改等，研发过程的流程驱动全靠人工，人工就难免遗漏，经常会听到测试人员抱怨开发不自测不提前做检查的声音，但是开发同学同样为写代码之外的工具流程学习成本叫苦不迭；再说上线，虽然我们会在提前对齐发布步骤，但是这些都是整个过程的回放，尤其是对于横跨几周甚至几个月的大项目，很可能造成步骤的遗漏，最终产生线上故障，我们已经遇到过好多次上线后忘记修改配置、忘记开启定时任务的故障；
- 学习成本变高：对应着流程的复杂度提升，开发测试同学的学习成本也逐渐提升，曾经我们做过统计，一个需求的交付过程需求切换6个以上的系统，这些系统即使你不需要彻底了解，但是起码也需要知道入口在哪，主要功能是啥，遇到问题怎们定位等，也许大家可能说习惯了就好了，但是互联网人员流动是常态，这种新入职员工的成本也不容小觑。

## 2.业务方案设计

### 2.1方案选型

由上述的背景分析可知，我们急需将研发过程的流程进行自动化串联，解决以上面临的问题，因此我们调研了业界的相关方案，发现主要是两种解法：

- **all in one云平台**：这种方式的好处是所以逻辑都由一个系统承接，所有的数据和信息也都是存在于一个系统，保证了数据源的统一性和用户体验的一致性，对于新的方案接入也可以保持方法统一；但是这种方法对于我们去哪儿这种有较长发展历史周期的公司不太适用，完全重头搭建成本太高，系统对接适配就会带来比较差的用户体验，同时对接的系统需要将各种数据对齐，比如说是以git为准、还是以一个应用为准等，同样需要一个数据适配层；

- **流水线调度**：调研业界研发效能平台的流水线，大部分是比较简单的构建部署（可能也有复杂的是笔者没有调研到），但是这种模式是我们可以借鉴的，因为在单点上我们已经做到足够好，需要做的就是利用流水线将所有的流程串联起来，同时将上游阶段（开发阶段）产生的数据信息，比如DB变更、配置变更等信息传递到下游，这样就避免了上线前人工编排的遗漏。而且通过流水线串联可以让用户很直观的了解流程节点信息但是又屏蔽掉了单点的具体信息，同时流程断点也可以很直观的定位，大大降低了用户的学习和使用成本，基于此我们选择了该方案。在真实的落地实现过程中我们主要需要解决以下几个问题：

  - 可持续发展，设计整体需要向后考虑，留下扩展和更改的可能性，因为我们可能后续会接入更多的步骤，提前考虑避免日后的运维成本；

  - 使用时候的学习成本要低， 避免用户接入 or 使用过程中需要通过大把时间来了解流水线的背景知识；

  - 模式要尽量统一， 流水线整体执行等要采用统一的执行流程或执行规范，避免架构的过多适配；

  - 可观测，不能完全是一个纯粹的黑盒，要能够观测到从开始到结束的过程及各个阶段耗时及数据和日志的变化；

### 2.2流水线类型拆分

在整个交付过程，我们通常的划分是设计阶段（产品关注）、实现阶段（开发关注）、测试阶段（开发、测试关注）、发布运维（开发、测试关注），在每个阶段关注的角色和执行的流程都不相同：

- **设计阶段：**主要是业务同学和产品同学关注，做的事情更多的是思考设计，所用的工具也比较统一，而且跟后续的流程连通性要求也没那么高，因此我们先不关注设计阶段；
- **开发阶段：**我们期望的是质量左移，即问题尽可能早的在开发阶段暴漏，因此我们要求开发同学及早的集成及早的解决问题；同时开发自测一般关注的是单应用维度，因此我们基于每次push进行全流程执行的效率复用度更高；
- **测试阶段：**需要开发同学快速修复、QA同学快速验证，因此也需要频繁的部署、测试、验证，而且在去哪儿我们目前正在推行开发自测自发，但是像测试环境的搭建、自动化测试之星这些测试同学是更擅长的，因此这些流程的学习执行成本降低的需求更强烈；
- **上线和运维阶段：**则需要非常慎重，一般情况每步都需要人工确认验证；而且一次发布对应的是一组应用，因此一般是对多个应用的组织编排，因此这个阶段更关注的是提前的组织编排能力和开发测试阶段的变更：包括DB、代码、配置的自动生成能力，而不是自动触发能力。

因此我们根据以上阶段拆分成几条流水线：

- **开发流水线：** 覆盖用户提交代码，到完整的自测过程，交付开发人员完整的在运行中的自测环境；
- **提测流水线：** 覆盖开发自测完成，代码提交给 QA同学进行测试的过程，解决之前的数据扭转等问题，及自动化的执行更多的如回归测试，性能测试等；
- **项目流水线：** 覆盖一个项目的周期，涉及多应用多模块的开发及测试，编排其执行顺序及统一的联调测试；
- **运维流水线：** 固化的运维场景及一些常用的流程性动作串联；
- **其他：** 用户自定义；

那么由上述的流水线拆分我们还发现一个问题，在不同的流水线中关注的应用个数是不同的，开发测试可能是单应用，而在需求上线阶段关注的却是多应用的组织编排，而且我们公司内部更希望将流程固化，避免流水线灵活自定义太大的运维成本，因此我们选择了固定应用流水线+灵活编排项目流水线的方式：

![pipeline2](/images/CICD/pipeline2.png)

​      

我们优先做了开发流水线，因为效率的提高及覆盖的场景无需一次性做到大而全，可以采用分阶段和渐进式的方式来完成，每完成一阶段提高一部分效率并降低部分成本即是好的，避免大而全带来的推广难度及不可控的问题,以下的设计和实现以开发流水线为例进行介绍，其他都可以服用此模式。

## 3.整体框架和实现

基于以上存在的问题及整体的设计思路来考虑整体的流水线执行逻辑可以简化为：

### 3.1单次任务执行逻辑

yaml（配置文件这里以yaml为例）提供了完整的执行流程配置，提交到执行层，后续由执行层完全接管执行；

执行层识别 yaml 中的最早执行的 step，执行并收集其执行结果；

执行层根据初始 step 的状态及 yaml 配置相关的的串并行逻辑及条件判断和循环执行逻辑继续推进 step执行，直到最终完成

![执行逻辑.png](/images/deploy_dashboard/pipeline1.png)

### 3.2系统模型设计

采用分层设计，总体分为三层，隔离和简化用户输入使用的 UI层，动作编排和执行及记录的适配层

- UI 和 API 层：提供用户配置和查看流水线的界面，将复杂配置逻辑简化仅留下用户使用的必填项，降低使用成本；

- 编排层：提供模板化和固化功能，实时根据参数及执行过程的编排填充完整的执行配置，最终通过一个个任务执行，且通过回调等方式留存执行过程及结果的记录

- 执行层： 驱动执行， 收集执行状态和数据，回调并记录结果；

**![整体执行架构.png](/images/deploy_dashboard/pipeline2.png)**

### 3.3落地实现

#### 3.3.1执行层

##### 3.3.1.1选型

**选型对比**

|              | Jenkins X | argo-workflows | 自研 |
| :----------- | :-------- | :------------- | :--- |
| 部署/维护    | 一般      | 简单           | -    |
| 简单使用     | 简单      | 简单           | -    |
| 复杂用法     | 难        | 一般           | -    |
| 配置友好程度 | 难        | 简单           | -    |
| 社区活跃     | 好        | 好             | 无   |
| 开发成本     | 一般      | 一般           | 高   |
| UI           | 一般      | 差             | -    |

作为平台方我们会对外提供服务注定会有很多复杂度很高的使用场景，这个时候Jenkins X就不太符合我们的需求了；

我们也比较关注与整个开发成本，完整的自己实现一遍需要关注的细节太多，成本较高也不符合预期；

另一个比较重要的选型依据是我们正在容器化的进程中，而argo-workflows是CNCF项目，天然的更契合云原生，所以我们的最终执行层选型为 **argo-workflows**；

**argo-workflows主要特点**

- 运行在 Kubernetes 上的自定义资源(CR)定义工作流，工作流中的每个步骤都是一个容器
- 可将多步骤工作流建模为一系列任务，也可以使用有向无环图（DAG）描述任务之间的依赖关系

##### **3.3.1.2考量**

1. 考虑独立的 k8s 集群，避免意外大量资源占用影响线上业务；
2. argo-workflow 执行pod时候除了任务本身还会有初始化和最终日志等清理镜像，部署时候应该注意将该镜像拉取到本地的仓库并改为本次仓库部署，减轻可能的每次执行拉取外部镜像的耗时；

##### **3.3.1.3实现**

部署方式参考各种搜索结果直接通过 yaml 安装即可；

#### 3.3.2编排层

##### 3.3.2.1考量

- 尽量提供统一且通用的抽象的可复用的模块来降低管理难度及减少执行层拉取新镜像的开销；
- 整体执行过程要留痕，可观测，不能完全作为黑盒，便于问题排查和统计分析；
- 提供通用的模板存储和复用逻辑，减少用户使用成本；

##### 3.3.2.2实现

编排层主要抽象三个服务：

- 通用镜像“httppod”，提供所有的支持http访问形式的模版接入和使用
- 通用的异步请求结果封装pipeline_gateway；
- 交互接口及适配层pipeline-x;
- 其他定制即兼容适配pipeline-adapters;

###### a.Httppod

**一个通用的镜像“httppod”，提供所有的支持 http访问形式的模板接入和使用**

**httppod 实现逻辑**

1. 通过环境变量的形式来决定请求方法，请求的目标地址、超时时间及是否同步请求还是异步请求；
2. 通过命令行参数来决定请求的参数，命令行参数使用类似get请求的方式来输入，最终的请求发出去时候会根据请求的方法等判断是否转为字典形式发送
3. 访问部分服务的时候可能仅仅是个通知或者是弱依赖的服务，其返回状态不应该阻止后续步骤的执行；
4. 不同服务返回状态的时候参数key不固定，或者value类型不同

**环境变量必须**

   METHOD         可选值 “GET” “POST”，    用于区别发出去的 http 请求采用哪种方法

   REQUEST_URL    请求目标 url，             需要以 http:// or https://  开头

   TIMEOUT         超时时间， 单位为 s，  int 类型的值，需要大于0 

环境变量可选

   IS_ASYNC        是否是异步执行， 默认是同步， 该 env 取对应的值的第一个字母 如果第一个字母为 y or Y， 则设置为异步

**命令行参数可选**

​    用于设置 http 请求的相关参数， 只支持 "xxx=xxx&x=aaa&xxxxx=bbb" 的写法， 且不含空格

**难点分析**

采用何种异步的形式

”一般的RPC调用“存在的问题在于不是每一个用户都能够驾驭，学习及使用成本较高，且比较黑盒隐藏了较多网络调用等等信息，不利于问题排查；**而HTTP虽然会要求我们发起请求的同时要起一个HTTP的server来供服务端回调**，但HTTP学习成本较低，且传输数据信息比较灵活所以这里采用 HTTP的形式来做；

唯一性标识应该放到哪

由于是HTTP异步调用那么需要能够精确的将结果回调发起端，需要有唯一标识；最初的阶段我们将唯一的标识放到发起请求的返回体中，但是实践过程中发现如果我们将唯一标识放到返回的请求体中，如果其他已经存在的服务要作为模块提供流水线使用，会对之前已经实现的服务等产生较大侵入性，会要求其修改返回的body；所以我们将唯一标识放到 http header请求头来解决该问题，既有服务增加一个http头返回的成本相对较少，也不容易产生副作用；

执行状态识别

同样的既有的服务采用了不同的形式的返回状态标记，我们需要一个比较灵活且能覆盖比较大范围的识别模板，且能够可配置；见下方配置示例；

执行状态后续动作

不同的服务提供的服务等级及服务本身的重要性其实是不同的，甚至部分服务可能只是个简单的通知类服务，成功与失败的状态不太重要需要有对失败情况的忽略；见下方配置示例；    

```
状态识别代码样例, 每一个代理的服务获取其对应的状态校验函数
func (t *TransportConfig) getStatusCheckFunc(_type string) func(string, string) bool {
    switch _type {
    case StatusCheckEqual:
        return func(s1, s2 string) bool { return s1 == s2 }
    case StatusCheckHasPrefix:
        return strings.HasPrefix
    case StatusCheckHasSuffix:
        return strings.HasSuffix
    case StatusCheckContains:
        return strings.Contains
    default:
    }
    return func(s1, s2 string) bool { return false }
}
```

```
某些服务的返回结果判断格式及是否忽略退出状态等配置
提供目标服务的统一的配置中心配置，无需在执行 httppod 时候在要求用户输入
"xxxxxxx": {
    "name": "xxxxxx",
    "response_status_key": "status",
    "response_status_type": "Int",
    "response_status_check_type": "Equal",
    "response_status_legal": [
        "0"
    ],
    "is_ingroe_step": true
},
```

###### b.Pipeline_gateway

**通过一个通用的模块 "pipeline_gateway"，提供所有已经实现了异步接口（但请求格式不是简单的字典等形式有其他格式要求）或通过IC消息（内部的一种异步消息队列，非http callback）来提供执行结果的服务抽象；**

**pipeline_gateway实现逻辑**

1. 通过抽象请求的目标、方法、参数及返回结果的数据验证方法将不同目标配置化；
2. 通过统一的配置中心配置，实现整体的配置动态热加载；
3. 为防止误操作及避免不必要的操作通过版本号等控制片段级别的更新；
4. 部分服务的认证方式不同，也需要适配和支持；
5. 不同服务的对于失败或者跳过定义不同；
6. 该服务是代理的直接通过 httppod无法请求的已经存在的服务，无可避免的也需要适配返回key的不同及value格式不同等问题；
7. 异步通过 IC 等返回状态的服务必须提供一种识别对应请求的唯一ID，该ID需要统一存储到固定的地方，使该服务无状态可水平扩展；

**难点分析**

1. 如何识别代理的服务返回的状态，如同httppod也需要识别状态等，我们也统一采用将对应格式的数据转为 String后来判断状态；
2. 不同的服务有header认证，有采用用户认证等形式，我们需要适配不同的认证方式；
3. 部分服务的消息通知等模块是后实现的，其实现过程中甚至存在消息使用的成功与失败与直接请求返回的成功与失败的字段及类型不同，有部分服务改动成本较高也需要我们来实现；
4. 不同的服务调用的方式不同，由于调用参数及格式等变化不多我们统一抽象为模板的形式来固化；
5. 会接入很多的服务，需要尽量的降低每个服务的接入成本；

配置和服务模板等示例

```
主配置文件片段样例, 控制接入哪些服务
{   
    "version": 1,
    "name": "xxxxxxxx",
    "request_url": "http://xxxxxxxxx/api/xxxxxxx",
    "method": "POST",
    "is_need_auth": false,
    "auth_style": "",
    "add_request_header": [],
    "add_request_header_params": [],
    "response_uuid_key": "appTaskId",
    "response_uuid_type": "String",
    "response_status_key": "status",
    "response_status_type": "Int",
    "response_status_check_type": "Equal",
    "response_status_legal": [
        "0"
    ],
    "ic_topic": "aaaaaaaaaaaaaaaaa",
    "ic_uuid_key": "appTaskId",
    "ic_uuid_type": "String",
    "ic_status_key": "status",
    "ic_status_type": "Bool",
    "ic_status_check_type": "Equal",
    "ic_status_legal": [
        "true"
    ],
    "template_file": "xxxxxxxxx.tpl"
}
```

```
模板配置文件样例   xxxxxxxxx.tpl， 对应服务的请求模板
{
    "build_params": {
        "app_params": [
            {
                "app_code": "{{ .xxxxxxxx0 }}",
                "xxxx_forced": {{ .xxxxxx1 }},
                "xxxx_stage": {{ .xxxxxxx2 }},
                "env_list": [
                    {
                        "env_id": "{{ .env_id }}",
                        "servers": [{{ .servers }}]
                    }
                ],
                "input_tag": "{{ .input_tag }}",
                "pmo_id": "{{ .pmo_id }}",
                "restart_healthcheck": {{ .restart_healthcheck }}
            }
        ],
        "xxxxx_order": "",
        "xxxxx_order": "",
        "xxxxx_type": "{{ .deploy_type }}"
    },
    "channel_id": "{{ .channel_id }}",
    "xxxxxxxxx": "{{ .xxxxxxxxxxx }}"
}
```

###### c.pipeline-x

**实现逻辑**

1. 定位于交互接口及适配层；
2. 提供执行过程及统计等信息的外部存储和查看能力；
3. 根据模板生成待执行任务的完整配置并交付执行；
4. 防腐设计，后来发展的系统不可避免的需要有各种的兼容和定制逻辑，这些功能尽量做到这里；

问题和迭代

1. 流水线开始和结束如何通知用户？需要在开始的时候获取执行的流水线 name，留存记录，并在结束的时候带着该name 回调pipeline-x;
2. 减少多数用户配置和使用的时间？提供标准的固定的流水线模板，如**开发流水线、提测流水线**等固定的执行动作；
3. 尽量覆盖多数用户且用户接入成本足够的低且减少整体时间？ 用户代码push之后即开始触发流水线；
4. 需要减少无用流水线等执行？ 同项目同分支等每次触发前终止之前未执行完成的任务，减少不必要开销；
5. 用户明确已知不触发流水线可在代码push的时候message中添加排除信息；
6. 接入哪些服务需要通过配置文件在配置中心配置，防止大范围的无意义信息骚扰用户；

###### d.pipeline-adapters其他定制即兼容问题

1. 由于发布策略的问题，我们要求每次最新的发布都是从 master拉取最新代码改动后，然后分支发布后合并回master，但用户往往在push代码的时候没有合并master，这时候我们提供一个通用的解法来帮助用户自动合并(无冲突的情况)master并提交；
2. 我们的自动化测试系统在执行过程中，是以环境的ID维度来进行的，但整体执行过程中信息的填充会是环境的名称，统一缺失了Name到ID的一个过程，则在整个执行过程中封装个特点的pod，将环境Name转为ID供后续流程使用；

#### 3.3.3UI层

##### 3.3.3.1考量

1. 对于配置层面，我们希望公司内采用相同的流程规范，因此开发流水线不支持用户自定义配置stage和step；执行过程，用户只需要关心执行的步骤及失败的节点即可，因此argo本身的界面已经满足我们的需求，不做二次开发；
1. 回到流水线的最终目标是提效，根据我们的流水线定义可以知道执行的周期还是相对较长，那么我们并不希望用户等待反馈，因此增加了IM消息结果反馈的方式，只是执行开始和结束关键节点通知，而且只是暴漏关键失败信息，提升问题排查定位的效率。

##### 3.3.3.2实现

- 前端直接采用argo的界面；
- 消息通知集成内部的IM消息在开始结束阶段推送消息通知，并包含关键信息。![失败通知.png](/images/deploy_dashboard/pipeline4.png)![执行过程样例.png](/images/deploy_dashboard/pipeline6.png)

## 4.总结规划

### 4.1总结

由流水线的落地实践我们有两点感悟：

- 流水线能实现很好的编排组合，但是灵活性也意味着复杂度，所以当我们要落地的时候需要考虑主要解决的问题什么，如果是效率那么我们可以支持灵活定义，如果是标准那么最好就要流程统一，因此做决策的时候一定要参考自己的规模体量和主要诉求；
- 建模好的流水线可以编排一切，但是实现的时候还是要考虑投入产出，比如线上的编排虽然能提升效率降低风险，但是它带来的效率相对于其他领域可能效益并不明显，所以我们就将其延后了，因此投入的时候还是要衡量好收益。

### 4.2规划

目前我们已经完整的落地了开发流水线，而且公司的活跃应用都已经接入，而其他的流水线还没有开始，主要原因对于这种流程整合类型的方案效果较难量化，后续我们确定了量化指标后会继续进入；

其他方面我们已经搭建了较好的流水线编排引擎，因此后续我们可以识别具有流程和数据传递要求的服务进行流水线适配，探索更多的使用场景，比如AIOPS等。
