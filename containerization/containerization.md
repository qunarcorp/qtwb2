---
typora-root-url: ./..
---

## 1 背景与收益
### 1.1 背景
近几年随着数字化时代的到来和云原生技术不断地成熟，大家已经认识到云原生是可以帮助企业实现数字化驱动和快速响应市场需求的最短路径。因此我们看到众多企业在不断地拥抱云原生，并尝试利用容器、k8s、service mesh 等新生技术解决企业面临的各种问题，比如资源快速扩容、资源成本高、研发效率低、环境一致性差、SDK 升级困难等。 在云原生落地之前，我们面对的问题主要有：

* **环境不一致，稳定性差**
研发人员在测试联调过程中经常会因为某些特定场景而修改服务配置（比如 nginx 配置）、机器配置（比如 dns 配置），待上线后忘记恢复变更，这导致其他研发人员在测试联调新功能时环境不顺畅，需要花时间排查问题，十分耗时。

* **环境交付慢**
过去业务线会针对一个新的需求新建一套环境来进行测试联调，一套环境包括了多个应用、中间件、db 等资源，最大的环境会有上百个应用，交付时间从 15 分钟到 30分钟不等。其中1个 kvm的交付是分钟级的，包括了 kvm 创建、各类软件的初始化等操作，如果所有这些组件可以并行，环境交付还是很快的, 不过由于应用和应用、应用和 DB、 应用和中间件之间是有依赖关系的，整个环境的构建其实是一个 有向无环图，并不能完全并行。对于大环境来说，在 kvm 资源交付上就会耗费不少时间，而容器化正好可以消除这部分时间
* **服务器运维人力成本高**
每当宿主机硬件故障需要进行运维操作时，ops 同学会先联系宿主机上 kvm 的应用负责人，待这些应用负责人确认后才能进行运维操作，这个过程研发同学大概需要花费5分钟。宿主机数量少的时候这个运维人力成本可以忽略不计，但是当宿主机数量到达一定规模上千台，上万台时，这个人力成本就非常高了

* **服务器资源利用率低**
受疫情影响，各公司都把成本控制当作重中之重，而服务器资源成本是成本中很大一部分。过去通过 openstack 提供 kvm 服务器的方式在资源利用率上有很大浪费，我们希望能尽可能的提高服务器资源利用率来降低资源成本

* **公司技术栈迭代缓慢**
过去我们的技术栈都是基于 kvm、dubbo 等技术体系构建的，并且已经稳定运行了很长时间，这套技术体系已经没有太大的演进空间，针对这套体系的优化带来的价值也是非常有限的。如何通过技术演进而推动和赋能业务是一个难题。

面对上述痛点问题，云原生架构是我们要找的解决方案，它是新一代技术架构的最佳演进路径，同时也是既定事实。而容器化是云原生架构的底座，它能解决扩容慢、环境交付效率慢、也能解决服务器资源利用率低和运维人力成本高的问题。 因此我们首先做的是容器化的落地，于是由基础平台牵头、联合 ops 、dba 、各业务线多个团队协同配合下把容器化这个系统工程成功落地了。 截止到 2021年底，生产环境已经有 3000+ 的应用完成了容器化。

### 1.2 收益

以下是我们容器化的收益：

**直接收益：**

* 服务器资源利用率提升 **76%**
* 发布效率提升 **45%**
* 发布成功率从过去的 **91%** 提升到 **95%**
* 业务人员不再需要关心机器维护、扩缩容等操作，专注于业务


**工程师文化的提高:**
* 工程师更多的参与云原生落地过程的方案讨论、内部分享、外部分享
* 工程师的工作和学习热情更高，更快的成长

## 2. 实践框架

### 2.1 容器化难点
由于各个公司都会有一定程度的技术债，容器化落地过程不可能是一帆风顺的，在进行实践之前我们也针对一些难点做了调研分析，例如

**1.容器场景下 IP 会频繁变化，基于固定IP的系统如何适配**

我们有些应用间的访问是需要防火墙白名单的、比如涉及金融的服务，还有应用到 DB的授权也是基于固定IP的，需要提前申请，申请过后几乎不会有变动。这些授权操作在容器化之前都是通过工单形式半自动化实现的。而容器化后，每次发布、驱逐都会导致容器的 IP 发生变化，并且容器的 IP 是在容器创建过程中才能获取到，因此之前的授权系统已经不再适用。要想实现容器化，这些授权必须在容器创建过程中自动化地进行授权。
这个问题有2个解决思路：

* 通过 CNI 网络插件实现固定IP

  这种方式可以让业务同学以最小的改造成本接入容器化。不过在调研了 Neutron、calico、cilium 等多个网络插件后发现这些插件不能满足需求，并且对 CNI 的研发能力要求很高，因此放弃了这个思路

* 通过动态的统一授权

   通过收敛各类授权操作到一个系统来实现授权，实现上可以结合 k8s 的 hook、init 容器机制等来实现。其中init 容器在初始化的时候做授权，这个时机正好是在 IP 成功分配后和应用启动前，可以完成授权的操作， preStop hook 在销毁的时候执行，权限回收在这个阶段做正合适。其中授权过程还有几个考虑点，授权过程保持幂等和授权系统的防护问题，防止大批量应用发布过程的 qps 突增而导致后台的授权系统被打爆。具体动态授权的实现细节可以在中间件改造的章节看到。

**2.从 kvm 到容器，如何确保用户习惯顺畅地过渡***

在 kvm 使用场景中，研发人员使用最多的功能就是应用发布、服务器自助运维、查看监控、在线远程debug调试 等操作，这些操作的结果在 UI 界面上都有所展示，当用户想查看更具体的细节时会直接登陆机器进行操作，这个操作已经是研发人员的习惯。在容器场景下我们希望做到和 KVM 同样的可观测性和易用性，用户才更有意愿配合迁移到容器。不过落地过程我们也遇到了一些难题：

- **发布过程无法实时获取标准输入输出**

发布过程中应用启动失败，查询容器的标准输入输出没有返回。这个问题在后面的坑点里会详细介绍

- **容器场景下如何支持 java 应用单个 pod 远程debug**

开发人员在测试环境调试、线上实例排查问题时为了快速定位问题经常会用到在线 debug 的功能，kvm 场景下这个的操作流程是 选择指定 kvm 机器摘流量 -> 更改debug配置 -> 重启 java 应用 -> 接流量（可选) -> 调试。容器场景下这个操作流程是行不通的，容器不支持只针对一个 pod 进行配置变更，因为所有的 pod 都是通过一个 pod 模版创建的，配置都一样，如果要更改一个，只能更改模版，更改模版就会对所有的 pod 生效，这样会影响业务，不符合预期。

为了支持单个 pod 进行远程 debug，最后我们通过把 pod 中的 java 配置持久化到 volume 中，这样修改 debug 配置后重启应用也不会丢失配置。这里我们的重启没有使用 kill 进程的方式，而是通过阿里开源的 Kruise CCR 旁路组件来实现的，它的好处是对原生 k8s 流程没有任何侵入性，同时也能保证容器的生命周期 正确完整地执行，可以确保业务流量不丢失。总结容器场景下的实现方案是 选择指定的pod-> exec 容器并更改debug 配置 -> kruise CCR 重启应用容器 -> 接流量 (可选) -> 调试。这个流程和 kvm 的流程是一致的。

另外为了优化使用体验，我们针对所有测试环境的容器镜像默认开启了远程 debug 配置，这样用户操作流程就不需要重启了，达到了秒级实现远程 debug, 用户反馈也特别好。而线上环境由于开启远程 debug 配置对性能会有毫秒级的损耗，很多关键应用是不能容忍的，因此只在测试环境默认开启远程 debug 配置。

- **用户希望容器发布终止后 pod 不接流量**

kvm 场景下发布过程中如果发现有异常报警，用户会直接终止发布，正在发布的 kvm 机器上的应用也会在当前状态终止，不会接入流量。
由于惯性思维，用户在容器发布过程中进行同样的终止发布，以为正在发布中的 pod 会终止或者销毁了，不会接入流量，但实际上由于 k8s 的异步机制和自愈机制，后台 pod 还会不断重启，如果 liveness 和 readiness 检测通过，就会自动接入流量, 这就和操作人员的预期不一致，很可能造成线上故障。会给用户排查问题造成困扰，导致故障恢复时间较长。

为了杜绝这类问题的再次发生，我们在产品层面上加了引导，让用户意识到容器场景下即使发布流程终止了，后台的 pod  也是运行的，线上存在多个版本有隐患，建议用户通过重新发布或者回滚来解决。

**3 怎样降低用户的迁移成本**

我们的目标是把 3000+ 应用完全容器化，如果每个应用的升级迁移成本是 1 pd （其中包括 jar 包升级、配置变更、测试、上线等流程），那整个迁移成本就是 3000pd, 这么多的人力成本业务线肯定不会同意。最终我们通过实现自动化升级和自动验证解决了这个问题，可以在系列文章中中间件自动升级看到具体的实现细节。


### 2.2 容器化落地策略

应用容器化是一个十分复杂的系统工程，在迁移方案的设计上我们参考了公有云的上云策略，并结合自己的使用场景总结出一套具有普适性的容器化落地路线图。

**公有云上云的三种方式**:

**re-host**: 把应用部署的载体从虚拟机、宿主直接更换成容器的方式。这种上云方式会有更好的稳定性和可管理性，迁移成本也比较低。


**re-platform**: 除了上述应用迁移到容器之外，基础服务如mysql、redis、mq 等组件也需要迁移到云组件中。这种方式对于公有云用户来说是可行的，对于私有云来说有很多额外成本的，需要慎重考虑。比如我们有专门的 DBA 团队维护 mysql、redis 等组件，并且有完善的运维体系，这些系统如果要改造容器化会花费很大的人力成本，整个投入产出比非常低的。因此这部分基础设施我们没有进行容器化。


**refactor**: 对于不符合云原生应用标准的应用，进行系统重构改造。比如一些有历史技术债的应用、依赖于固定 ip 的应用进行小范围的重构消除这些依赖，才能进行容器化。

结合我们的场景、最后我们选择的是虚拟机直接迁移到容器和应用小范围重构的方式来完成容器化落地的，并如期地完成了任务。

### 2.3 容器化落地路线图

在落地过程中，我们根据遇到的问题、难点、以及解决方案总结了一个有效的实践路线图，

![image](/images/container/practise_roadmap.png )

#### 2.3.1 价值认同

做容器化落地这件事情之前，老板经常会问到这件事情的 ROI 是多少，怎么证明？

经过实践发现，有三点是有效的:

**1 自证价值- 自己的服务先容器化**

  想要说服其他人认可容器化的价值，最好的方式就是用数据说话，这样才有说服力。建议自己团队内部的服务先容器化，体验容器化带来的优势，把容器化的好处能更客观地传递给其他业务线。
  与此同时，自己团队在容器化过程提前趟了一遍坑点，把过程中遇到的问题解决，不断打磨容器化的平台，让平台好用，这样业务线同学接入使用过程中会更有信心。

**2 放大价值- 解决业务线实际问题**

   如何切实地让用户感受到容器化带来的好处，这个是需要花心思的。建议多分析下用户反馈的问题列表，找到用户当前最痛的点作为抓手，比如资源交付效率、服务稳定性等方面，把这些问题用容器解决好，用户认可度会更高。

**3 技术宣讲- 价值宣传，找 VIP 用户**

容器化是一个系统工程，只靠一个团队是不可能完成的，因此找战友，壮大队伍是十分必要的。而在公司内部技术宣讲是最有效的方式，对新技术感兴趣的同学会主动找来询问并讨论相关的价值点，其中最有意向的团队就是我们的 vip 用户，他们的诉求在后续的支持过程中要优先考虑。
#### 2.3.2 规范制定
为了方便后期容器化周边系统的改造, 前期制定一些规范和标准化是十分必要的。我们实践过程的标准化主要包括:

规范 | 说明
---|---
应用部署节点 | 1. 应用不需要关心部署在哪个机房，默认都是多机房部署  <br> 2.应用不应该依赖节点和 IP 信息
应用日志 | 1. 业务日志不要打到 catalina.out 中 <br> 2. Java 应用统一使用 logback 管理日志, 格式统一
应用部署信息 | 1. 部署路径和端口号统一 <br> 2. 应用启动后 hook 不再支持，支持启动前、上线前、和下线前的 hook

#### 2.3.3 工具能力建设

在容器化落地过程中需要改造的组件很多，整体架构如下
![image](/images/container/tools_architecture.png)

上图彩色画像的组建都是需要改造的，这里我们重点介绍**应用画像**、**授权中心**、**CI/CD** 部分

* **应用画像**

  云原生倡导的是声明式配置，统一数据源，这和 devops 提倡的以应用为中心的应用画像本质上是一致的。基于这个理念，我们统一了应用的数据源集成到了应用画像中, 包括应用的基本信息、环境属性、发布参数、依赖信息四个部分:
  ![image](/images/container/app_code.png)


* **授权中心**
    容器化之前的授权过程都是半自动化的，需要开发人员提交工单并人工审核后授权才能成功。在云原生场景中，这些操作是需要自动化来实现无人值守。


  新的授权架构设计如下：

![image](/images/container/db_grant.png)

 **一个 pod的授权流程**：
  1. pod 创建过程，init 容器会调用授权中心的接口针对当前 pod 进行授权操作
  2. 授权中心接到授权请求后，会根据应用的依赖配置信息执行授权，在 db user 表中添加用户和 ip 信息
  3. 授权成功后 init 容器结束，业务容器的应用开始启动

  **一个 pod的权限回收流程**：

  1. 当操作删除 pod 时，sidecar 容器配置的 preStop hook 会调用授权中心的回收接口


* **CI/CD**

  容器化后用户最直观的感受是发布效率快了，除了容器的启动快的因素外，还有一个原因就是容器场景下发布策略的优化改进。



_ | KVM 发布策略 | 滚动更新 | 双 deployment 更新
---|---|---|---
更新方式 | 原地更新，先减后增 | 原生的 rolling update 策略 | 分批次更新，先扩后缩
支持分批 |  支持 | 不支持| 支持
缺点 | 实例个数少的服务发布过程中响应时长可能会增加 |1. 更新速率不好控制，太快不稳定,  太慢体验差 <br> 2. 不支持分批，整个发布流程不可控| 资源要有一定的 buffer
优点 | 过程可控 | 1. 整个发布过程自动化 <br> 2.资源可控，不占用外 buffer  |1. 发布过程可控  <br> 2.操作复杂性低, 只有 create 和 patch 操作

发布流程图:

![image](/images/container/deploy_strategy.png)


* **Openresty**

Qunar 使用商业版本的 Openresty 作为流量网关，在 kvm 场景下通过提工单半自动的实现 upstream 机器列表的更新。在容器场景下，pod 实例的 ip 是频繁变化的，所以过去的方式是行不通的。关于如何打通 k8s 多集群和 Openresty 实现动态更新 upstream, 方案有 push 和 pull 两种模型

   **push:** 通过监听 endpoint 对象变更，调用 Openresty 接口实时更新 upstream 列表。这个方案在集群和应用规模不是特别大的时候是没问题的，但是当集群和应用规模大了会导致更新 Openresty 超时，严重情况下会造成流量损失，因此最后我们选择了 pull 模型。

   **pull:** Openresty 支持了自定义 k8s upstream 和 service, 当对应的 endpoint 发生变更时， Openresty 会自动拉取对应的 endpoint 列表。这个功能 Openresty 商业公司单独做了性能优化，解决了 push 模型更新超时的问题

* **k8s 多集群管理**

我们内部 k8s 的使用方主要有 ops 运维团队和基础平台的发布系统，运维团队主要关心的是运维便捷性、可视化、安全性等特性，而发布系统主要关心的是接口的实时性、一致性和性能。在初期集群规模不是很大的时候，选择开源的一些产品 rancher、kubesphere 等多集群管理方案可以满足多方的需求，但是当接入容器化的使用场景增多、接口调用指数增加后，性能问题也随之而来，因此就需要对数据接口有针对性的优化。

  基础平台的k8s查询优化：当自动化测试、第三方系统都接入容器化后，接口调用会成为性能瓶颈，这个瓶颈会严重影响 k8s 集群的稳定性。因此对 k8s 的数据做一层数据缓存是非常必要的，所有的查询入口都走这个数据缓存层。

* **HPA 弹性扩缩容**

弹性扩缩容是云原生场景下的一个核心能力，它可以应对业务场景上有突发流量、也可以针对资源使用率低的服务进行缩容从而减少资源成本。在酒店、机票、火车票等业务场景中，节假日等特殊日子里流量是不确定的，这个时候弹性扩缩容就有用武之地了。

我们的多集群 HPA 实现方案：
![image](/images/container/hpa.png)

HPA 使用流程：

1）用户配置扩缩容的指标阈值：

<img src="/images/container/hpa_config.png" width="600px" height="800px" div align="center">

2）在多集群生成下发 HPA 配置

3）HPA 动态拉取  cpu/memory, 用户自定义的指标并决策是否进行扩容或缩荣

4）扩容或者缩容成功后，回填应用画像中应用的副本数

在使用 HPA 过程中的一个最大难点就是阈值设置多少合适，通过历史的监控值并不能准确的判断当前的容量水位，因为服务是动态变化的，每天都有新功能上线，过去的阈值很可能已经失效。我们的做法是业务线同学在低峰阶段利用压测平台阶梯式地对当前应用进行压力测试，压测数据也是来源于线上的真实流量数据，通过实时监控应用的核心链路指标情况并不断调整发压数据，最后可以确定一个合理真实的容量水位。

#### 2.3.4 迁移落地
业务线的应用迁移到容器我们分解成6个步骤来完成，每个步骤尽量做到自动化，让用户更容易地迁移到容器。
![image](/images/container/migrate.png)

迁移流程:

1. **前置校验**: 编译前自动校验应用有哪些方法不符合标准化规范，比如 getHostName 等依赖 host 信息的方法，发现后会提示用户改造
2. **测试环境验证**: 这个阶段我们会自动帮业务线升级 SDK, 升级到支持容器化的版本。(具体升级方案后面的章节会介绍)
3. **线上验证**: 应用容器自动发布到线上后首先不接入线上流量，业务同学会跑自动化 case 并做回归验证。如果没问题会把这部分容器接入线上流量
4. **混合部署**: 为了保证迁移过程的服务 SLA, kvm 和容器会混部一段时间
5. **全量部署**: 混合部署没有问题后，会把 kvm 流量全部切到容器
6. **观察**: 观察一段时间后 （我们是观察1周），容器服务没有问题则回收 kvm 机器

#### 2.3.5 验收

容器化后为业务带来了哪些价值，用哪些指标衡量, 这个是所有人都关心的问题。我们主要从3个指标来看:

![image](/images/container/value.png)

**计算方式**：
* **每天节省运维人力**: 宿主机数量 * 宿主机上的 kvm 数量 * 应用的 app owner * 1个人处理运维的时间
* **交付效率提升**：
 KVM 发布的平均时长: 3.8 min
  容器发布的平均时长: 2.1 min
* **资源利用率**:
  每个宿主机部署的 kvm 数量: 17
   每个宿主机部署的容器数量: 30


另外还有一些看不见的无法直接量化的价值，比如说工程师文化的提高，云原生架构给大家带来了更多的成长机会，公司内部成了了SIG 兴趣小组，大家交流学习的机会更多了，这些都是技术人非常看重的。

## 3 容器化实践过程坑点
### 3.1 发布过程读不到标准输入输出

**场景介绍**: 应用容器为了兼容 kvm 时期业务线可以自定义启动前 hook 脚本的功能，测试环境中这个脚本业务线主要用来准备测试数据，是必不可少的。因此我们把用户自定义脚本放到了应用容器的 postHook 中。实际运行过程中当用户的自定义脚本出错、比如 npm 安装超时并 hang 住的时候，用户发布页面看不到实时的标准输入输出，直至10分钟超时，这个给用户的体验非常不好，不知道发生了什么，可观测性非常差。

**问题点**: 发布过程 k8s api 为什么拿不到应用容器的标准输入输出?

**答案**: 经过查阅官方文档和非官方文档，终于在非官方文档 https://unofficial-kubernetes.readthedocs.io/en/latest/concepts/containers/container-lifecycle-hooks/ 中找到了相关描述，如果 hook hang 住，容器的状态也会hang 住，这个时候 api 读取日志是读不到的。

问题介绍:
![image](/images/container/hook_issue.png)

**解决方案**:
通过引入 sidecar 的方式解耦 postStart hook 和 容器的生命周期。

![image](/images/container/run_user_hook.png)

具体流程:

1. 第1个 init 容器执行资源初始化、db 授权操作
2. 第2个 init 容器 copy 应用的数据、用户自定义的 hooks 到共享 volume
3. 应用容器、sidecar 挂载 volume, 并启动容器
4. sidecar entrypoint.sh 启动过程会执行用户自定义的脚本
5. 发布模块调用 k8s api  查询应用容器的标准输入输出。无论自定义执行成功与否，应用的标准输入输出都可以读到

### 3.2 批量发布过程中个别 pod 失败

**场景介绍**: 在使用 deployment 发布过程中, 所有 pod 都是从一个模版中派生出来的，没有任何差异。因此理想状态是: 只要有一个 pod 发布成功，所有的 pod 都应该是成功的。但现实情况比较复杂，有很多情况会造成 pod 失败，比如后端缓存、db 连接池满、授权系统挂掉、宿主异常、镜像拉取超时等问题。在发生个别 pod 失败的场景下，采取什么策略是需要根据场景仔细思考的

**问题点**: 如何处理个别失败的 pod 来提升发布效率

**参考答案**: 处理个别失败 pod 的策略主要有3种:

1. **让整个发布失败**: 这个策略比较严格, 要求所有 pod  都成功，不太现实。
2. **重建失败的 pod 直至成功**: 针对失败的 pod 执行删除操作，让 k8s 重新调度直至 pod 启动成功。这种方式对发布来说是最安全的，兼容了发布效率和质量。
3. **按比例忽略失败的 pod**: 当发布失败的 pod 数量低于规定的阈值，继续发布，不做任何失败处理，等新流量完成切换后再做处理。这种方式发布效率是最高的，不过有一定的质量隐患。对于回滚场景来说这种策略是比较合适的。

  我们当前采用的策略是第二种，重建失败 pod 的方式


### 3.3 容器实时日志方案怎样实现性价比最高

**场景介绍**: 容器的可观测性是用户的一项基本要求，用户在发布过程中会查看实时日志，接到告警排查问题的时候也会查看日志，有时还会查历史的日志。过去 kvm 可以通过 ELK 查看实时日志，当然还可以直接登陆 kvm 机器查看，但是容器场景下就不现实了，因为容器在重新发布或者驱逐后就销毁了，想看的日志也不确定在哪个机器上。

**问题点**: 满足用户的需求，同时要考虑机器成本

**参考方案**:

1. **ELK**: ELK 是实时日志的一个标配方案，之前业务同学也有使用它的经验，所以我们的日志方案第一期就是采用的 ELK，但是试用一段时间后发现机器成本很高、查询速度等使用体验上达不到用户的要求，因此最终也就放弃了这个方案。
2. **Loki**: Loki 是 grafna 针对云原生场景定制开发的日志查询系统，它的查询方式和体验都非常好，包括提供了实时 tail 日志的功能。但是当用户对实时日志的保留时间提出更高要求后，Loki 的资源占用也逐渐升高，最后也只能放弃
3. **通过 ws api 实时查看pod日志**: 最终的方案是调用 ws api 实时查看 pod 的日志。这种方式的最大优点就是耗费的资源少，经济实惠。不过也有个缺点，当 pod 销毁后，就查不到相关日志了。因此需要离线日志的配合，当 pod 不存在或者发生驱逐，通过离线日志工具取查询。

  我们当前采用的是 ws api 查看实时日志，hdfs 实现离线日志的查询，这种实现方式的实现成本非常低，同时也能满足用户需求


## 4.总结展望

回过头复盘容器化落地的整个过程，它的成功可以归结为以下三点:
* **价值认同**
 这里引用朱熹《朱子语类》的一句话 "知之不深，则行之不笃； 知之愈明，则行之愈笃"。 我们通过技术宣讲等手段让大多数人认同容器化的价值，大家的目标一致，成功是必然的

* **产品同理心**
 自己一定要是容器化项目的第一个用户，提前发现并填好坑点，这样用户才会有信心、放心地配合迁移容器
* **工程化方法**
  最大化的减少用户迁移成本，比如通过自动化前置检测、自动升级 SDK、自动化迁移等手段来减少用户的迁移成本。

 在云原生的路上容器化只是一个起点，后续我们会结合 service mesh、serverless 、OAM 、OpenTelemetry 等技术的能力来为业务研发同学提供更多好用、实用的开发者体验，为业务的研发效能提保驾护航。
