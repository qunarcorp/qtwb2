---
title: Qunar基于云原生的研发效能实战
top: false
cover: false
toc: true
mathjax: true
date: 2022-09-30 15:27:31
password:
summary:
tags:
- 博客
categories:
- 随笔
---


# 去哪儿网（Qunar）云原生实战



<img src="/medias/images/cover" alt="image-20220927125136291" style="zoom:150%;" />











编写说明

编写单位：北京趣拿软件科技有限公司

总编

张春芳、刘淼

内容审核者

王克礼、李晓悦、肖双、陈靖贤

编写组成员（按目录排序）

邹晟、王克礼、李佳龙、李子洋、屈晶晶、武凯亮、李晓悦、鲁国宁、梁成琰、吴燕军、吴凡、于海影、沙丹丹、聂振宇、王鹏、肖双











# 序

数字化转型最早在2012年由国际商业机器公司（IBM）提出，强调了应用数字技术重塑客户价值主张和增强客户交互与协作。我国政府自2017年以来已经连续四年将“数字经济”写入政府工作报告，并在十四五规划纲要中提出“以数字化转型整体驱动生产方式、生活方式和治理方式变革”，数字化转型从企业（组织）层面上升为国家战略，同时企业数字化转型是我国推动经济社会发展的重要战略手段，是企业的核心竞争力。而云原生已经成为以数据为核心生产要素构建业务的方法和工具，是驱动业务增长的重要引擎，是企业数字创新的最短路径。

云原生概念是Pivotal公司Matt Stine于2013年首次提出，它是一组架构模式。主要有几大原则：服务化原则、弹性原则、可观测原则、韧性原则、所有过程自动化原则、零信任原则、架构持续演进原则，用来帮助企业应用更好地利用云计算优势，充分释放云计算技术红利，让业务更敏捷高质量交付。目前CNCF围绕云原生已经建立了比较完善的生态环境，给企业落地云原生提供了良好的技术和产品支持，对于企业的技术演进提供了很大的便利。云原生核心是将底层的基础知识如网络通信等屏蔽，同时实现最大程度的自动化，让技术同学更加专注于业务创新和客户服务，有效提升企业价值交付的能力。无论是数字化还是云原生，最终的目的都是为了帮助企业提升市场响应速度和持续交付价值能力，即研发效能，这也是企业发展的核心竞争力。

去哪儿网作为一个深耕旅游行业多年的技术型互联网公司，具有规模大，复杂度高的特点。为了支撑数千个应用服务，应对各种复杂多变的业务需求，为提速增效提供有力保障，增强企业竞争力，去哪儿网近两年来结合国家的数字化发展战略和云原生先进的技术理念，结合企业特性，进行了基于云原生的研发效能落地。去哪儿网的技术演进有10年之久，而且之前的建设是基于openstack的，云原生对于我们无疑是颠覆式的变革，但是我们最终取得了不错的成绩，近两年也在各种社区大会进行了模块经验分享，受到了大家的一致认可。在互联网，跟我们类似规模、类似经验的企业不在少数，我们非常渴望能够系统地将落地经验进行分享，为正在进行数字化转型、云原生落地的企业提供一些参考和借鉴，同时促进企业间的互相交流和共同成长。

研发效能发展至今在整个行业已经积累了相当丰富的经验和通用的解决方案。我们的落地过程也极大地享受到了这波红利，但是除此之外，结合企业发展的背景和内部的一些特性，做了很多本土化适配，这个应该也是当代企业更关注的经验与实践。在建设过程中我们遵循云原生服务化、弹性、韧性、可观测性等主要原则，同时结合数字化转型思路，搭建了基于技术五要素：技术先进性、用户体验、效率、质量、成本的研发效能度量体系，这本书就是对我们实践过程经验的总结，它并不是基础技能和通用知识的普及，而是对我们的亮点实现方案的经验分享，因此更适合有些基础建设想要寻求突破或者某个领域遇到瓶颈寻求亮点的用户，期望能为大家带来思路和启发。

以下为去哪儿网基于云原生的研发效能落地整体框架图，在接下来我们将从基础设施容器化、服务化建设、开发提效、质量保障、CICD、稳定性保障、可观测性建设、数字化这几个部分分别为大家介绍我们的实现方案和路径：

1. **基础设施容器化**

   介绍从理念宣讲、价值同步，到最终落地的完整过程以及从资源层、中间件层、发布系统到周边配套的完整改造过程。

2. **开发提效**

   介绍在serverless方向的探索和实践，包括前端低代码平台、FAAS服务平台、云开发三个部分。

3. **质量保障**

   介绍研发过程的质量保障手段，包括灵活的环境管理方案、高效的自动化测试平台、完善的覆盖率保障方案。

4. **CICD**

   介绍基于云原生的发布过程可观测性建设--发布驾驶仓，以及提升交付能力的流水线建设。

5. **可观测性**

   介绍可观测性基于云原生的演进路线和当前建设现状，包括Logging、Tracing、Metrics。

6. **稳定性保障**

   介绍流量高峰期，基于混沌和全链路压测的稳定性保障方案。

7. **数字化实践**

   介绍研发效能数字化建设方案，包括通用的度量模型、闭环的落地过程、数字化的实践应用--效率办公等。

下面欢迎大家跟我们一起开启这趟旅程。

![image-20220928155644445](/medias/images/total2.png)

















































# 目录

[TOC]





# -----------------

# Part 1	基础设施容器化

容器化是云原生的基石，它给我们带来了环境一致性、动态伸缩等天然的优势，让技术人员可以更专注地进行业务创新，提升价值交付的能力。容器化是云原生落地建设的第一步，但是对于我们这样有较长发展历史的中大型互联网公司，基础建设及周边的配套基本都是围绕 kvm 来的，因此它给我带来的冲击也是极大的，包括：

1. **文化层面**

   各层级技术人员的技术认知，对其价值的认可与否，严重影响落地的效率和效果。

2. **技术层面**

   新的技术必然带来新的学习成本，相应的人员是否能够接受。

3. **管理层面**

   从底层到上层的完全变更势必带来极大的成本，公司是否能够接受。

带着这么大的挑战，我们从2021年开始进行容器化落地尝试，目前已经基本完成，本章将带着上述三个问题给大家介绍我们的容器化落地过程，希望能让大家在先进技术引进的完整落地流程和容器化落地技术改造方案上有所收获。













# 第一章	容器化落地



## 1.1	背景与收益

### 1.1.1	背景

近几年随着数字化时代的到来和云原生技术不断地成熟，大家已经认识到云原生是可以帮助企业实现数字化驱动和快速响应市场需求的最短路径。因此我们看到众多企业在不断地拥抱云原生，并尝试利用容器、k8s、service mesh 等新生技术解决企业面临的各种问题，比如服务扩容慢、资源成本高、研发效率低、环境一致性差、SDK 升级困难等。 在云原生落地之前，我们面对的问题主要有：

- [ ] #### **环境不一致，稳定性差**

  研发人员在测试联调过程中经常会因为某些特定场景而修改服务配置（比如 nginx 配置）、机器配置（比如 dns 配置）、待上线后忘记恢复变更，这导致其他研发人员在测试联调新功能时环境不顺畅，需要耗费大量的时间进行问题排查、定位与修复。

- [ ] #### **环境交付慢**

  过去业务线会针对一个新的需求新建一套环境来进行测试联调，一套环境包括了多个应用、中间件、db 等资源，最大的环境会有上百个应用，交付时间从 15 分钟到 30分钟不等。其中1个 kvm 的交付是分钟级的，包括了 kvm 创建、各类软件的初始化等操作，如果所有这些组件可以并行，环境交付还是很快的，不过由于应用和应用、应用和 DB、应用和中间件之间是有依赖关系的，整个环境的构建其实是一个有向无环图，并不能完全并行。对于大环境来说，在 kvm 资源交付上就会耗费不少时间，而容器化正好可以消除这部分浪费。

- [ ] #### **服务器运维人力成本高**

  每当宿主机硬件故障需要进行运维操作时，ops 同学会先联系宿主机上 kvm 的应用负责人，待这些应用负责人确认后才能进行运维操作，这个过程研发同学大概需要花费5分钟。宿主机数量少的时候这个运维人力成本可以忽略不计，但是当宿主机数量到达一定规模上千台、上万台时，这个人力成本就非常高了。

- [ ] #### **服务器资源利用率低**

  受疫情影响，各公司都把成本控制当作重中之重，而服务器资源成本是成本中很大一部分。过去通过 openstack 提供 kvm 服务器的方式在资源利用率上有很大浪费，我们希望能尽可能的提高服务器资源利用率来降低资源成本。

- [ ] #### **公司技术栈迭代缓慢**

  过去我们的技术栈都是基于 kvm、dubbo 等技术体系构建的，并且已经稳定运行了很长时间，这套技术体系已经没有太大的演进空间，针对这套体系的优化带来的价值也是非常有限的。如何通过技术演进而推动和赋能业务是一个难题。

   面对上述痛点问题，云原生架构是我们要找的解决方案，它是新一代技术架构的最佳演进路径，同时也是既定事实。而容器化是云原生架构的底座，它能解决扩容慢、环境交付慢、服务器资源利用率低和运维人力成本高的问题。 因此我们首先做的是容器化的落地，于是由基础平台牵头、ops 、dba 、各业务线多个团队协同配合，最终成功完成了全司范围的容器化系统工程的落地。 截止到 2021年底，生产环境已经有 3000+ 的应用完成了容器化。

### 1.1.2	收益

- [ ] #### **直接收益**

1. 服务器资源利用率提升 76%。
2. 发布效率提升 45%。
3. 发布成功率从过去的 91% 提升到 95%。
4. 业务人员不再需要关心机器维护、扩缩容等操作，更多精力专注于业务。

- [ ] #### **工程师技能的积累**

1. 工程师更多地参与到云原生落地过程，对公司、行业输出更多的专业技能知识。

2. 更多的工程师成长为领域专家。

   

## 1.2	实践框架

### 1.2.1	容器化难点

由于各个公司都会有一定程度的技术债，容器化落地过程不可能是一帆风顺的，在进行实践之前我们也针对一些难点做了调研分析，例如：

- [ ] #### **容器场景下 IP 会频繁变化，基于固定IP的系统如何适配**

我们有些应用间的访问是需要防火墙白名单的，比如涉及金融的服务，还有应用到DB的授权也是基于固定 IP 的，需要提前申请，申请过后几乎不会有变动。这些授权操作在容器化之前都是通过工单形式半自动化实现的。而容器化后，每次发布、驱逐都会导致容器的 IP 发生变化，并且容器的 IP 是在容器创建过程中才能获取到，因此之前的授权系统已经不再适用。要想实现容器化，这些授权必须在容器创建过程中自动化地进行授权。这个问题有2个解决思路：

1. **通过 CNI 网络插件实现固定 IP**

  这种方式可以让业务同学以最小的改造成本接入容器化。不过在调研了 Neutron、calico、cilium 等多个网络插件后发现这些插件不能满足需求，并且对 CNI 的研发能力要求很高，因此放弃了这个思路。

2. **通过动态统一授权的方式**

    通过收敛各类授权操作到一个系统统一管理，实现上可以结合 k8s 的 hook、init 容器机制等来实现。其中init 容器在初始化的时候做授权，这个时机正好是在 IP 成功分配后和应用启动前，可以完成授权的操作， preStop hook 在销毁的时候执行，权限回收在这个阶段做正合适。其中授权过程还有几个考虑点，授权过程保持幂等和授权系统的防护问题，防止大批量应用发布过程的 qps 突增而导致后台的授权系统被打爆。具体动态授权的实现细节可以在中间件改造的章节看到。

- [ ] #### **从 kvm 到容器，如何确保用户习惯顺畅地过渡**

在 kvm 使用场景中，研发人员使用最多的功能就是应用发布、服务器自助运维、查看监控、在线远程debug等操作，这些操作的结果在 UI 界面上都有所展示，当用户想查看更具体的细节时会直接登陆机器进行操作，这个操作已经是研发人员的习惯。在容器场景下我们希望做到和 kvm 同样的可观测性和易用性，用户才更有意愿配合迁移到容器。不过落地过程我们也遇到了一些难题：

1. **发布过程无法实时获取标准输入输出**

   发布过程中应用启动失败，查询容器的标准输入输出没有返回。这个问题在后面的坑点里会详细介绍。

2. **容器场景下如何支持 java 应用单个 pod 远程 debug**

   开发人员在测试环境调试、线上实例排查问题时为了快速定位问题经常会用到在线 debug 的功能，kvm 场景下这个的操作流程是选择指定 kvm 机器摘流量 -> 更改 debug 配置 -> 重启 java 应用 -> 接流量（可选) -> 调试。容器场景下这个操作流程是行不通的，容器不支持只针对一个 pod 进行配置变更，因为所有的 pod 都是通过一个 pod 模版创建的，配置都一样，如果要更改一个，只能更改模版，更改模版就会对所有的 pod 生效，这样会影响业务，不符合预期。

   为了支持单个 pod 进行远程 debug，最后我们通过把 pod 中的 java 配置持久化到 volume 中，这样修改 debug 配置后重启应用也不会丢失配置。这里我们的重启没有使用 kill 进程的方式，而是通过阿里开源的 Kruise CCR 旁路组件来实现的，它的好处是对原生 k8s 流程没有任何侵入性，同时也能保证容器的生命周期正确完整地执行，可以确保业务流量不丢失。总结容器场景下的实现方案是选择指定的 pod-> exec 容器并更改 debug 配置 ->  kruise CCR 重启应用容器 -> 接流量 (可选) -> 调试。这个流程和 kvm 的流程是一致的。

   另外为了优化使用体验，我们针对所有测试环境的容器镜像默认开启了远程 debug 配置，这样用户操作流程就不需要重启了，达到了秒级实现远程 debug， 用户反馈也特别好。而线上环境由于开启远程 debug 配置对性能会有毫秒级的损耗，很多关键应用是不能容忍的，因此只在测试环境默认开启远程 debug 配置。

3. **用户希望容器发布终止后 pod 不接流量**

   kvm 场景下发布过程中如果发现有异常报警，用户会直接终止发布，正在发布的 kvm 机器上的应用也会在当前状态终止，不会接入流量。

   由于惯性思维，用户在容器发布过程中进行同样的终止发布，以为正在发布中的 pod 会终止或者销毁了，不会接入流量，但实际上由于 k8s 的异步机制和自愈机制，后台 pod 还会不断重启，如果 liveness 和 readiness 检测通过，就会自动接入流量, 这就和操作人员的预期不一致，很可能造成线上故障。会给用户排查问题造成困扰，导致故障恢复时间较长。

   为了杜绝这类问题的再次发生，我们在产品层面上加了引导，让用户意识到容器场景下即使发布流程终止了，后台的 pod 也是运行的，线上存在多个版本有隐患，建议用户通过重新发布或者回滚来解决。

- [ ] #### **怎样降低用户的迁移成本**

我们的目标是把 3000+ 应用完全容器化，如果每个应用的升级迁移成本是 1 人天 （其中包括 jar 包升级、配置变更、测试、上线等流程），那整个迁移成本就是 3000 人天， 这么多的人力成本业务线肯定不会同意。最终我们通过实现自动化升级和自动验证解决了这个问题，可以在系列文章《中间件自动升级》看到具体的实现细节。


### 1.2.2	容器化落地策略

应用容器化是一个十分复杂的系统工程，在迁移方案的设计上我们参考了公有云的上云策略，并结合自己的使用场景总结出一套具有普适性的容器化落地路线图。公有云上云的三种方式如下：

- [ ] #### **re-host**

  把应用部署的载体从虚拟机、宿主直接更换成容器的方式。这种上云方式会有更好的稳定性和可管理性，迁移成本也比较低。

- [ ] #### **re-platform**

  除了上述应用迁移到容器之外，基础服务如 mysql、redis、mq 等组件也需要迁移到云组件中。这种方式对于公有云用户来说是可行的，对于私有云来说有很多额外成本，需要慎重考虑。比如我们有专门的 DBA 团队维护 mysql、redis 等组件，并且有完善的运维体系。这些系统如果要改造容器化，会花费很大的人力成本，整个投入产出比非常低。因此这部分基础设施我们没有进行容器化。

- [ ] #### **refactor**

  对于不符合云原生应用标准的应用，进行系统重构改造。比如一些有历史技术债的应用、依赖于固定 ip 的应用进行小范围的重构消除这些依赖，才能进行容器化。

结合我们的场景，最后我们选择的是虚拟机直接迁移到容器和应用小范围重构的方式来完成容器化落地的，并如期地完成了任务。

### 1.2.3	容器化落地路线图

在落地过程中，我们根据遇到的问题、难点、以及解决方案总结了一个有效的实践路线图，如图1-1所示。

![practise_roadmap](/medias/images/container/practise_roadmap.png)

​                                                                      

<center>图1-1 容器化落地线路图</center>

- [ ] #### 价值认同

做容器化落地这件事情之前，老板经常会问到这件事情的 ROI 是多少，怎么证明？经过实践发现，有三点是有效的：

1. **自证价值- 自己的服务先容器化**

   想要说服其他人认可容器化的价值，最好的方式就是用数据说话，这样才有说服力。建议自己团队内部的服务先容器化，体验容器化带来的优势，把容器化的好处能更客观地传递给其他业务线。
   与此同时，自己团队在容器化过程的坑点提前趟了一遍，解决过程中遇到的问题，不断打磨容器化的平台，让平台好用，这样业务线同学接入使用过程中会更有信心。

2. **放大价值- 解决业务线实际问题**

   如何切实地让用户感受到容器化带来的好处，这个是需要花心思的。建议多分析下用户反馈的问题列表，找到用户当前最痛的点作为抓手，比如资源交付效率、服务稳定性等方面，把这些问题用容器解决好，用户认可度会更高。

3. **技术宣讲- 价值宣传，找 VIP 用户**

   容器化是一个系统工程，只靠一个团队是不可能完成的，因此找战友，壮大队伍是十分必要的。而在公司内部，技术宣讲是最有效的方式，对新技术感兴趣的同学会主动找来询问并讨论相关的价值点，其中最有意向的团队就是我们的 vip 用户，他们的诉求在后续的支持过程中要优先考虑。

- [ ] #### 规范制定

为了方便后期容器化周边系统的改造, 前期制定一些规范和标准是十分必要的。我们实践过程的标准化主要包括如图1-2所示要点。

| 规范         | 说明                                                         |
| ------------ | ------------------------------------------------------------ |
| 应用部署节点 | 1. 应用不需要关心部署在哪个机房，默认都是多机房部署  <br>2.应用不应该依赖节点和 IP 信息 |
| 应用日志     | 1. 业务日志不要打到 catalina.out 中 <br>2. Java 应用统一使用 logback 管理日志, 格式统一 |
| 应用部署信息 | 1. 部署路径和端口号统一 <br>2. 应用启动后 hook 不再支持，支持启动前、上线前、和下线前的 hook |

<center>图1-2 容器化过程标准化</center>

- [ ] #### 工具能力建设

在容器化落地过程中需要改造的组件很多，整体架构如图1-3所示。
<img src="/medias/images/container/tools_architecture.png" alt="image" style="zoom:150%;" />

<center>图1-3 容器化过程标准化</center>

上图1-3中彩色画像的组建都是需要改造的，这里我们重点介绍 **应用画像**、**授权中心**、**CI/CD** 、**Openresty** 、**k8s 多集群管理 **、 **HPA 弹性扩缩容** 部分。

**应用画像**

云原生倡导的是声明式配置，统一数据源，这和 devops 提倡的以应用为中心的应用画像本质上是一致的。基于这个理念，我们统一了应用的数据源集成到了应用画像中, 包括应用的基本信息、环境属性、发布参数、依赖信息四个部分，如图1-4所示。
![image](/medias/images/container/app_code.png)

<center>图1-4 应用画像集成数据源</center>

- **授权中心**
  容器化之前的授权过程都是半自动化的，需要开发人员提交工单并人工审核后授权才能成功。在云原生场景中，这些操作是需要自动化来实现无人值守。


  新的授权架构设计如图1-5所示。

   ![image](/medias/images/container/db_grant.png)

  <center>图1-5 容器化授权过程</center>

  - 一个 pod 的授权流程：

    pod 创建过程，init 容器会调用授权中心的接口针对当前 pod 进行授权操作。

    授权中心接到授权请求后，会根据应用的依赖配置信息执行授权，在 db user 表中添加用户和 ip 信息。

    授权成功后 init 容器结束，业务容器的应用开始启动。

    一个 pod 的权限回收流程：

    当操作删除 pod 时，sidecar 容器配置的 preStop hook 会调用授权中心的回收接口。

  

- **CI/CD**

  容器化后用户最直观的感受是发布效率快了，除了容器的启动快的因素外，还有一个原因就是容器场景下发布策略的优化改进，如图1-6和图1-7 所示。

  | _        | KVM 发布策略                                 | 滚动更新                                                     | 双 deployment 更新                                           |
  | -------- | -------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | 更新方式 | 原地更新，先减后增                           | 原生的 rolling update 策略                                   | 分批次更新，先扩后缩                                         |
  | 支持分批 | 支持                                         | 不支持                                                       | 支持                                                         |
  | 缺点     | 实例个数少的服务发布过程中响应时长可能会增加 | 1. 更新速率不好控制，太快不稳定,  太慢体验差  2. 不支持分批，整个发布流程不可控 | 资源要有一定的 buffer                                        |
  | 优点     | 过程可控                                     | 1. 整个发布过程自动化                                      2.资源可控，不占用外 buffer | 1. 发布过程可控                                                                  2.操作复杂性低, 只有 create 和 patch 操作 |

  <center>图1-6 容器化场景下的发布策略</center>

  

  ![image](/medias/images/container/deploy_strategy.png)

  <center>图1-7 容器化场景下的发布流程图</center>



- **Openresty**

  Qunar 使用商业版本的 Openresty 作为流量网关，在 kvm 场景下通过提工单半自动的实现 upstream 机器列表的更新。在容器场景下，pod 实例的 ip 是频繁变化的，所以过去的方式是行不通的。关于如何打通 k8s 多集群和 Openresty 实现动态更新 upstream，方案有 push 和 pull 两种模型。

  - push：通过监听 endpoint 对象变更，调用 Openresty 接口实时更新 upstream 列表。这个方案在集群和应用规模不是特别大的时候是没问题的，但是当集群和应用规模大了会导致更新 Openresty 超时，严重情况下会造成流量损失，因此最后我们选择了 pull 模型。
  
  
    - pull：Openresty 支持了自定义 k8s upstream 和 service，当对应的 endpoint 发生变更时， Openresty 会自动拉取对应的 endpoint 列表。这个功能 Openresty 商业公司单独做了性能优化，解决了 push 模型更新超时的问题。
  


- **k8s 多集群管理**

  我们内部 k8s 的使用方主要有 ops 运维团队和基础平台的发布系统，运维团队主要关心的是运维便捷性、可视化、安全性等特性，而发布系统主要关心的是接口的实时性、一致性和性能。在初期集群规模不是很大的时候，选择开源的一些产品 rancher、kubesphere 等多集群管理方案可以满足多方的需求，但是当接入容器化的使用场景增多、接口调用指数增加后，性能问题也随之而来，因此就需要对数据接口有针对性的优化。

  - 基础平台的 k8s 查询优化：当自动化测试、第三方系统都接入容器化后，接口调用会成为性能瓶颈，这个瓶颈会严重影响 k8s 集群的稳定性。因此对 k8s 的数据做一层数据缓存是非常必要的，所有的查询入口都走这个数据缓存层。

- **HPA 弹性扩缩容**

  - 弹性扩缩容是云原生场景下的一个核心能力，它可以应对业务场景上有突发流量、也可以针对资源使用率低的服务进行缩容从而减少资源成本。在酒店、机票、火车票等业务场景中，节假日等特殊日子里流量是不确定的，这个时候弹性扩缩容就有用武之地了。下面详细介绍我们的多集群 HPA 实现方案和使用流程。

    用户配置扩缩容的指标阈值，详见图1-8。

  <img src="/medias/images/container/hpa_config.png" width="600px" height="900px" div align="center" style="zoom:;" >

  <center>图1-8 用户配置扩容的指标</center>

  - 在多集群生成下发 HPA 配置。

  - HPA 动态拉取  cpu/memory, 用户自定义的指标并决策是否进行扩容或缩容。
  - 扩容或者缩容成功后，回填应用画像中应用的副本数。

在使用 HPA 过程中的一个最大难点就是阈值设置多少合适，通过历史的监控值并不能准确的判断当前的容量水位，因为服务是动态变化的，每天都有新功能   上线，过去的阈值很可能已经失效。我们的做法是业务线同学在低峰阶段利用压测平台阶梯式地对当前应用进行压力测试，压测数据也是来源于线上的真实流量数据，通过实时监控应用的核心链路指标情况并不断调整发压数据，最后可以确定一个合理真实的容量水位。

### 1.2.4	迁移落地

业务线的应用迁移到容器我们分解成6个步骤来完成，如下图1-9所示，每个步骤尽量做到自动化，让用户更容易地迁移到容器。
![image](/medias/images/container/migrate.png)

<center>图1-9 应用迁移到容器流程</center>

- [ ] #### 迁移流程如下：

1. **前置校验**

   编译前自动校验应用有哪些方法不符合标准化规范，比如 getHostName 等依赖 host 信息的方法，发现后会提示用户改造。

2. **测试环境验证**

   这个阶段我们会自动帮业务线升级 SDK ，升级到支持容器化的版本。(具体升级方案后面的章节会介绍)。

3. **线上验证**

   应用容器自动发布到线上后首先不接入线上流量，业务同学会跑自动化 case 并做回归验证。如果没问题会把这部分容器接入线上流量。

4. **混合部署**

   为了保证迁移过程的服务 SLA、kvm 和容器会混部一段时间。

5. **全量部署**

   混合部署没有问题后，会把 kvm 流量全部切到容器。

6. **观察**

   观察一段时间后 （我们是观察1周），容器服务没有问题则回收 kvm 机器。

### 1.2.5 验收

容器化后为业务带来了哪些价值，用哪些指标衡量，这个是所有人都关心的问题，我们从图1-10 的3个指标来看。

![image](/medias/images/container/value.png)

<center>图1-10 容器化的价值</center>

- [ ] #### **计算方式**：

1. **每天节省运维人力**

   宿主机数量 * 宿主机上的 kvm 数量 * 应用的 app owner * 1个人处理运维的时间。

2. **交付效率提升**
   KVM 发布的平均时长: 3.8 min。
   容器发布的平均时长: 2.1 min。

3. **资源利用率**
   每个宿主机部署的 kvm 数量: 17。
   每个宿主机部署的容器数量: 30。

另外还有一些看不见的无法直接量化的价值，比如说工程师专业知识与技能的提升；云原生架构给大家带来了更多的成长机会；公司内部成了了SIG 兴趣小组；大家交流学习的机会更多了等等，这些都是技术同学非常看重的。



## 1.3	容器化实践过程坑点

### 1.3.1	发布过程读不到标准输入输出

- [ ] #### **场景介绍**

  应用容器为了兼容 kvm 时期业务线可以自定义启动前 hook 脚本的功能，测试环境中这个脚本业务线主要用来准备测试数据，是必不可少的。因此我们把用户自定义脚本放到了应用容器的 postHook 中。实际运行过程中当用户的自定义脚本出错、比如 npm 安装超时并 hang 住的时候，用户发布页面看不到实时的标准输入输出，直至10分钟超时，这个给用户的体验非常不好，不知道发生了什么，可观测性非常差。

- [ ] #### **问题点**

  发布过程 k8s api 为什么拿不到应用容器的标准输入输出？

- [ ] #### **答案**

  经过查阅官方文档和非官方文档，终于在非官方文档 https://unofficial-kubernetes.readthedocs.io/en/latest/concepts/containers/container-lifecycle-hooks/ 中找到了相关描述，如果 hook hang 住，容器的状态也会hang 住，这个时候 api 读取日志是读不到的。

- [ ] #### 问题剖析

  

![image](/medias/images/container/hook_issue.png)

<center>图1-11 Hook Handler Execution 文档</center>

- [ ] #### **解决方案**

  通过引入 sidecar 的方式解耦 postStart hook 和 容器的生命周期，具体流程和图示1-12 如下。

![image](/medias/images/container/run_user_hook.png)

<center>图1-12 解决流程</center>



1. 第1个 init 容器执行资源初始化、db 授权操作。
2. 第2个 init 容器 copy 应用的数据、用户自定义的 hooks 到共享 volume。
3. 应用容器、sidecar 挂载 volume, 并启动容器。
4. sidecar entrypoint.sh 启动过程会执行用户自定义的脚本。
5. 发布模块调用 k8s api  查询应用容器的标准输入输出。无论自定义执行成功与否，应用的标准输入输出都可以读到。

### 1.3.2	批量发布过程中个别 pod 失败

- [ ] #### **场景介绍**

  在使用 deployment 发布过程中， 所有 pod 都是从一个模版中派生出来的，没有任何差异。因此理想状态是：只要有一个 pod 发布成功，所有的 pod 都应该是成功的。但现实情况比较复杂，有很多情况会造成 pod 失败，比如后端缓存、db 连接池满、授权系统挂掉、宿主异常、镜像拉取超时等问题。在发生个别 pod 失败的场景下，采取什么策略是需要根据场景仔细思考的。

- [ ] #### **问题点**

  如何处理个别失败的 pod 来提升发布效率。

- [ ] #### **参考方案**

1. **让整个发布失败**

   这个策略比较严格, 要求所有 pod  都成功，不太现实。

2. **重建失败的 pod 直至成功**

   针对失败的 pod 执行删除操作，让 k8s 重新调度直至 pod 启动成功。这种方式对发布来说是最安全的，兼容了发布效率和质量。

3. **按比例忽略失败的 pod**

   当发布失败的 pod 数量低于规定的阈值，继续发布，不做任何失败处理，等新流量完成切换后再做处理。这种方式发布效率是最高的，不过有一定的质量隐患。对于回滚场景来说这种策略是比较合适的。

我们当前采用的策略是第2种，重建失败 pod 的方式。


### 1.3.3	容器实时日志方案怎样实现性价比最高

- [ ] #### **场景介绍**

  容器的可观测性是用户的一项基本要求，用户在发布过程中会查看实时日志，接到告警排查问题的时候也会查看日志，有时还会查历史的日志。过去 kvm 可以通过 ELK 查看实时日志，当然还可以直接登陆 kvm 机器查看，但是容器场景下就不现实了，因为容器在重新发布或者驱逐后就销毁了，想看的日志也不确定在哪个机器上。

- [ ] #### **问题点**

  满足用户的需求，同时要考虑机器成本。

- [ ] #### **参考方案**

1. **ELK**

   ELK 是实时日志的一个标配方案，之前业务同学也有使用它的经验，所以我们的日志方案第一期就是采用的 ELK，但是试用一段时间后发现机器成本很高、查询速度等使用体验上达不到用户的要求，因此最终也就放弃了这个方案。

2. **Loki**

   Loki 是 grafana 针对云原生场景定制开发的日志查询系统，它的查询方式和体验都非常好，包括提供了实时 tail 日志的功能。但是当用户对实时日志的保留时间提出更高要求后，Loki 的资源占用也逐渐升高，最后也只能放弃。

3. **通过 ws api 实时查看pod日志**

   最终的方案是调用 ws api 实时查看 pod 的日志。这种方式的最大优点就是耗费的资源少，经济实惠。不过也有个缺点，当 pod 销毁后，就查不到相关日志了。因此需要离线日志的配合，当 pod 不存在或者发生驱逐，通过离线日志工具取查询。

我们当前采用的是第3种方案：通过 ws api 查看实时日志，hdfs 实现离线日志的查询，这种实现方式的实现成本非常低，同时也能满足用户需求。




## 1.4	总结展望

回过头复盘容器化落地的整个过程，它的成功可以归结为以下三点：

- [ ] #### **价值认同**

这里引用朱熹《朱子语类》的一句话 "知之不深，则行之不笃； 知之愈明，则行之愈笃"。 我们通过技术宣讲等手段让大多数人认同容器化的价值，大家的目标一致，成功是必然的 。

- [ ] #### **产品同理心**

自己一定要是容器化项目的第一个用户，提前发现并填好坑点，这样用户才会有信心、放心地配合迁移容器。

- [ ] #### **工程化方法**

最大化的减少用户迁移成本，比如通过自动化前置检测、自动升级 SDK、自动化迁移等手段来减少用户的迁移成本。

 在云原生的路上容器化只是一个起点，后续我们会结合 service mesh、serverless 、OAM 、OpenTelemetry 等技术的能力来为业务研发同学提供更多好用、实用的开发者体验，为业务的研发效能提保驾护航。









# 第二章	中间件容器化适配



## 2.1	背景

随着容器化技术的逐步成熟，越来越多的公司开始拥抱云原生技术。一方面容器化后扩容缩容更加灵活，能提升资源利用率、降低总体成本。另一方面，容器化后整体环境标准化程度提升，业务的发布、运维效率都能得到提升。经过慎重考虑和权衡，去哪儿网内部也决定全面容器化。

kvm和容器化差异较大，我们最终的目标是要在整个公司落地容器化，为云原生建设打下基础，所以我们需要在推动业务团队容器化之前做好中间件的容器化适配工作，屏蔽 KVM 和容器这 2 种不同环境的差异，降低业务团队容器化的成本。



## 2.2	实践路径

在本节中，我们会详细介绍基础架构团队在支撑业务团队容器化方面具体做出的准备工作，从业务直接使用的中间件组件到为了支撑容器化而新增的公共服务都会涉及。

### 2.2.1	组件梳理

首先，我们先简单介绍一些公司内部比较重要的组件，他们包括：

1. **公共核心库**

   这是一个核心的接入层依赖包，用于项目自动接入公司的应用体系，可以为其他中间件提供应用、机器环境等基础信息。

2. **qconfig**

   公司内的配置中心，统一管理应用的配置，支持配置热更新。

3. **qmq**

   公司内的消息队列，主要处理业务相关的消息。

4. **qschedule**

   公司内的任务调度系统，用于业务统一管理定时任务。

5. **dubbo**

   公司内使用的 RPC 框架，包含了一些内部定制化的功能。

6. **redis & mysql client**

   公司内的数据源客户端，屏蔽了集群细节和具体 IP 地址，支持动态集群变更。

### 2.2.2	容器化环境特点

首先，我们从公司内部的虚拟机环境说起。使用虚拟机时，新项目上线或者是现有项目扩容，都需要开发人员预先提交机器申请，经过审批、交付获得机器后才能发布。如果项目本身对 JDK、tomcat 或其他软件有要求，还需要自行调整新机器的环境。虚拟机一旦交付，机器的 IP 和 hostname 基本就保持稳定了，发生变动的情况很少。虚拟机环境的项目发布时，发布系统选择的策略是一批批下线、更新、上线的策略，这个过程中提供服务的虚拟机是先减少再恢复正常。

容器环境整体上和虚拟机环境有一些明显差异。容器由 k8s 统一管理，不再需要业务同学提前申请了，业务同学在发布时确定好容器数量即可，发布时会由 k8s 自动创建出足够数量的容器。项目的容器环境也是预先定义好的，不再需要业务同学一台台单独调整。由于容器是发布时由 k8s 自动分配，所以机器 IP 和 hostname 都是随机的。在发布阶段，公司内的发布团队选择了双 k8s deployment 更新的模式，上线过程从容器数量上看变成了扩容、上线新版本、下线老版本、缩容老版本，整个过程中是提供服务的容器数量先增长再减少。

从上面的描述可以看出虚拟机环境和容器环境最大的差异有 2 点：

1. 容器化后 IP 和 hostname 都从相对稳定变成了完全随机
2. 项目发布从逐批替换发布变成了先扩容再缩容

### 2.2.3	适配核心思路

为了让业务服务能够迁移到容器化环境，中间件必须首先适配容器化环境，否则业务根本无法迁移。中间件适配容器化环境的核心问题有 2 个，中间件自身的正常运行和尽量屏蔽环境差异。由于环境的变化，很多中间件功能也都无法直接在容器化环境中运行，要让中间件功能适配容器化环境，能够适配的功能要尽量隐藏 2 种环境的差异，无法适配的功能需要考虑屏蔽或者重新根据容器化环境的特点设计功能。

- [ ] #### 环境识别


无论是兼容不同环境还是针对不同环境单独重新设计功能，都需要先识别出不同环境，这是后续一切工作的基础。客户端需要识别不同环境，根据环境做相应的兼容操作，尽力屏蔽环境差异，保持对外接口行为的一致性。服务端也一样，除了尽力保持功能的一致性外，还需要针对无法兼容的功能单独设计开发对应的新功能。那我们该如何做环境识别呢？目前公司里提供了 2 种方案。

1. 请求一个统一的中心服务，这个中心服务根据 IP 返回对应的环境信息。
2. 在容器和虚拟机上添加一个描述文件，里面包含对应的环境信息。

在客户端中，为了减少外部依赖，提高可靠性，我们选择了描述文件的方式。由发布系统在发布时生成描述文件，描述文件中包含了当前的机房信息、环境信息等。之后再由我们的公共核心库读取描述文件统一对外提供环境识别的接口，由于其他公共组件库之前已经接入核心库，就不需要再自己做环境识别的工作了。

在服务端中，则需要根据具体的业务场景做不同的处理。对于直接和客户端交互的服务，通常客户端会主动汇报目标节点的环境信息，或者是在请求中带上环境信息，这种情况下服务端会直接使用客户端传递的环境信息，减少外部交互。而对于不会和客户端交互的服务端部分，则需要根据 IP 从中心服务上查询出对应的环境信息。

- [ ] #### 适配思路


环境识别问题解决后，就要考虑各个组件中的各项具体功能如何适配了，要判断哪些功能能够兼容，哪些功能需要重新设计。我们之前总结了虚拟机和容器环境的 2 个主要差异点，可以看到影响比较大的主要是单机相关的部分，所以这里根据功能是否涉及单机分为 2 类来考虑。

1. 对于非单机功能，由于不需要针对不同的单机做处理，所以能够识别环境后基本都可以屏蔽具体差异，提供一致的服务行为。

2. 对于单机功能，由于组件较多，整体就比较复杂了。我们逐个分析了各个中间件里涉及单机的功能，下面是一些例子：


* 公共核心库和应用中心：主要涉及到单机的环境识别
* qconfig 配置中心：主要涉及到单机配置推送、单机版本锁定等需要指定固定机器的功能
* qschedule：主要涉及到执行节点手工上下线功能
* qmq：主要涉及到广播消费功能
* dubbo：主要涉及到服务单实例配置相关的功能

从上面可以看出，适配容器化的主要问题点在单机功能部分。单机功能适配的主要问题是随机 IP 和 hostname。为了解决这个问题，我们最初考虑了 2 个方案：

1. 引入一个唯一 ID 生成服务，为容器化应用的每个实例都产生一个固定 ID
2. 接受随机性，从功能层面上适配随机 IP

在综合考虑实现难度、运维复杂度、功能必要性、未来技术方向等各方面因素之后，我们决定选择接受随机性，从功能层面做适配和改造。不依赖一个唯一 ID 生成系统，整体的可靠性会更好。对各个系统中针对单机的功能进行分析后，发现大部分都可以做适配。容器化后的技术方向就是屏蔽单机，从服务维度考虑，因此那些不兼容的功能我们也可以从服务维度重新设计实现。



## 2.3	适配实践

### 2.3.1	公共核心库容器化适配

公共核心库本身是由很多模块组成的，比如：

1. common-core，包括了应用接入、上下线管理、公共核心 API 等部分。
2. common-web，针对 spring 环境的各种定制化增强功能模块。
3. common-http，内部封装的 http client，添加了很多增强功能，比如外网访问代理、链路追踪接入等等。

公共核心库中最核心的功能就是 common-core 模块提供的应用身份及环境信息识别功能。common-core 模块会上报应用 token 数据和当前运行的机器环境数据到应用中心，应用中心会校验应用身份、环境等信息，通过后生成一个临时动态 token，最后会将 token 和应用环境信息再返回给 common-core。其他中间件和一些业务组件都会依赖 common-core 提供的 token 和环境信息。

这一次公共核心库容器化适配最关键的改动就是环境信息识别部分。在虚拟机环境中，公司机器的 hostname 是包含了环境、机房等信息的，所以 common-core 模块和应用中心依赖 IP 和 hostname 判定环境、机房信息，在容器化场景下这种做法就行不通了。同时，为了避免每个组件都自己判断当前是虚拟机环境还是容器化环境，核心库也得提供一个辨别具体环境的 API。

参考前面环境识别中描述的方案，common-core 中最重要的改动就是放弃从 hostname 中获取环境信息，转向读取机器上的服务器环境描述文件。应用中心上也是同理，不能再依赖 IP 反解获取 hostname，而是只能使用 common-core 中上报的信息。

### 2.3.2	Dubbo 容器化适配

Dubbo 容器化适配的主要问题点有 2 个。一是单机配置相关的部分，包括上下线功能、业务单机配置等。二是公司内部开发的注册中心故障保护功能。下面会分别介绍这 2 个方面的详细适配方案。

这里，我们先简单介绍下公司内 dubbo 的服务注册与上下线功能。我们内部目前是使用 zookeeper 作为 dubbo 的注册中心，dubbo provider 启动后会在 zookeeper 注册，consumer 订阅 zookeeper 上的对应节点从而及时获取 provider 机器列表、动态配置等数据。原始的 dubbo 实现中，这里会遇到一个问题，provider 服务的注册是全自动的，代码初始化之后就会自己在 zookeeper 上写入节点信息，这时候 consumer 端就能够获取到最新数据并建立连接发送请求了。但是一种可能出现的问题场景是 provider 已经注册，应用却尚未启动完成，此时处理接收到的 consumer 端请求会报错。为了解决这个问题，我们引入了上下线机制。在 provider 注册到 zookeeper 上之前，我们会在 zookeeper 上的相应配置节点下写入一条下线配置，将这个节点提前标记为下线状态。等到应用收到上线信号后，我们会主动删除之前写入的下线配置，让 provider 节点上线。为了避免应用发布关闭时继续接收请求导致报错，我们也会在应用发布关闭前提前写入下线配置，让 provider 提前下线。

容器化之后，对于大多数业务方来说，dubbo 不做任何改动也能在容器化环境中正常提供服务注册、服务上下线、服务调用等核心功能。但是容器化环境中的随机 IP 还是会带来一些额外的问题。一是上下线机制加上频繁的 IP 变更会导致 zookeeper 上残留大量的过期数据，因为每次应用发布提前下线时都会写入一条下线记录。二是随机 IP 会导致和 IP 相关的配置失效，比如单机配置、路由配置等等。为了 zookeeper 服务的稳定性和 dubbo 功能的完整性，必须得针对容器化环境做相应的适配工作。

我们先介绍下容器化环境里的上下线方案。最初，考虑了两种解决容器化环境上下线的方案：

1. 修改 dubbo，让 provider 注册的数据延迟到最终上线时再写入。
2. 修改上下线逻辑，将下线记录由永久节点改为临时节点，永久节点需要主动删除，临时节点应用关闭后就自动被 zookeeper 清除了。

第1种方案涉及到修改 dubbo 本身的代码，整体会更加复杂。第2种只需要修改上下线部分的代码，这部分代码本身是作为插件形式存在的。我们最终选择了方案 2，主要是整体的代码修改量更少。

上下线配置都是临时性的，可以切换为临时节点。但是其他单机配置是不行的，因为很多配置是需要一直保留下去的。由于每次发布后都随机分配 IP，固定 IP 的单机配置发布后自然就失效了。最初，我们考虑整体方案是想要引入唯一 ID 生成系统来唯一标识某个单机，但是最终我们还是放弃了。一来引入唯一 ID 会带来额外的中心依赖，可靠性会降低。二来我们这边选择的发布模式是类似先扩容再缩容的模式，唯一 ID 也不好处理。目前我们是放弃了长期生效的单机配置，推荐业务使用整个服务级别的配置，这种级别长期看是更有好处的，避免了单机的不一致问题。

Dubbo 故障保护机制是我们为了避免 zookeeper 故障导致服务 provider 大量节点下线最终出现服务无法调用而开发的容错功能。正常情况下，如果 zookeeper 直接彻底挂掉，consumer 端的影响是不大的。但是我们历史上出现过 zookeeper 因为压力过大导致大量节点过期被清理掉的情况，这种情况下 consumer 端是有可能感知到 provider 大量下线的，此时很容易出现大量请求失败。

因此我们设计实现了故障保护机制。当 consumer 发现 zookeeper 上存在的 provider 数量低于正常值的 35% 时，保护机制自动启用，consumer 会将所有预先缓存的 provider 视为正常的provider 进行调用。保护机制处于启用状态时，当 consumer 发现 zookeeper 上存在的 provider 数量大于等于正常值 75%，保护机制自动关闭，consumer 只会调用 zookeeper 上存在的 provider。Provider 数量正常值指最近 6 小时zookeeper 上存在过的 provider 数量，缓存的 provider 为最近 6 小时 zookeeper 上存在过的 provider。

容器化环境中先扩容再缩容的发布模式和 dubbo 故障保护机制刚好冲突了，发布批次少的应用很容易触发这个机制，导致业务日志中出现大量异常，但是实际上这个又并不会影响业务正常请求。针对这个功能，我们的改良点主要是两个。一是完整 provider 列表由累计最大改为瞬时最大，避免每次发布 provider 完整列表都翻倍，降低触发概率。二是触发 dubbo 故障保护功能后通过应用中心接口强制获取当前应用的完整在线容器列表，剔除掉确定下线的记录，避免大量报错。

### 2.3.3 QMQ 容器化适配

QMQ 是公司内部开发的一个消息队列，目前已经开源，想要详细了解的话可以参考 https://github.com/qunarcorp/qmq。QMQ 容器化适配主要涉及了 3 个问题，这些问题也都是由随机 IP 和新发布模式导致：

1. 是广播消费问题。
2. 大量过期数据的清理问题。
3. 使用持久消息和事务消息时的数据库授权问题。

QMQ 支持的消费模式和常见的 kafka 有些差异，所以这里我们先简单介绍下广播消费。QMQ 的消费进度都是是按消费组管理的，消费组内可以有一台或者多台机器，消息会被组内的消费者均分，增加组内的消费者数量就能提升消费能力。一般来说，大部分的业务消息都是希望多台机器均分消费的。不过也有一些场景中业务希望应用的每一台机器都能消费到主题的所有消息，比如触发本地缓存刷新的消息。广播消费就是针对这种场景设计的消费模式，在 QMQ 里实际上就是每个消费者都生成一个单独的消费组。

了解了广播消费的模式，大家应该很容易考虑到容器化后随机 IP 带来的问题，那就是广播消费的消费进度管理问题。在虚拟机环境中，机器是比较稳定的，无论是数量还是具体的 IP 等，这种场景下广播消费的进度是可以稳定管理的，每次应用重新启动都可以定位到之前的消费组并接着之前的消费进度继续消费。而容器化后，随机 IP 配合先扩容再缩容的发布模式，会导致广播消费的消费组非常不稳定，我们不得不重新考虑这种场景下消费进度该如何管理。

最初，为了解决广播消费消费进度管理的问题，我们也考虑采用固定 ID 生成服务。但仔细考虑后发现这样也还是不能满足要求，主要冲突的点在于发布模式的改变。采用固定 ID 的本意是发布时消费组的归属自动由老容器转移到新容器。但由于采用先扩容再缩容的发布模式，消费组归属自动转移就没法做了，因为发布时并行存在的消费组数量总是比正常情况下要多。这个问题我们纠结了挺久，后来才突然意识到一点：扩容缩容本身其实和广播消费不冲突。在虚拟机环境下，业务本身也是得扩容缩容的，这就说明业务本身使用广播消费时必然已经处理了扩容缩容带来的问题，否则他们自己的应用在虚拟机环境下也无法正常运作。所以我们最终放弃了固定 ID，完全接受随机 IP，将其当做扩容缩容场景对待。

广播消费接受了随机 IP，我们也要做相应的适配工作，除了 client 端根据环境做简单适配外，还有 2 个涉及到 QMQ 服务端的问题：一个是随机数量变多，会产生很多过期数据；二个是现有的消费进度默认实现会导致新消费组大量消费旧消息。第一个问题处理起来比较简单，梳理并增加相应的数据清理任务，加快清理频率就可以了。第二个问题是最初 QMQ 为了避免消费组先上线且主题后发送消息时依旧丢失最初发送的消息而设计的，让消费组默认从历史位置开始消费，这就导致容器化场景下每次发布广播消费的新机器都会变成从历史开始消费。但这里直接改成从最新位置消费很容易丢失消息，因为主题是发送消息时才分配具体的 QMQ 服务端节点，这就导致必然是先收到发送请求才能收到消费拉取请求。为了解决这个问题，我们调整了初始消费进度的判断算法，确定最初消费进度时额外考虑主题的创建时间，近期创建的从最旧开始，否则从最新位置开始。

为了给消息发送方提供更高的可靠性，QMQ 借助业务数据库提供了持久消息和事务消息。QMQ producer 支持发送消息前将消息先保存到业务数据库实例中，发送成功时自动删除，发送失败时从业务库里补偿发送，这就是持久消息。而事务消息则是在持久消息的基础上做了一致性增强，业务在数据库事务中发送的消息会等到事务成功提交后再发送，如果事务回滚则不会发出这条消息，这样业务的 DB 操作和消息发送就通过事务保持一致了。由于持久消息和事务消息都需要保存消息到业务数据库实例里，所以实际上数据库实例里会有一个 QMQ 专用的库，实例的使用方共享这个专用库，这里就会涉及到数据库访问权限的问题。容器化之前业务发送持久消息和事务消息时都需要在管理平台上提交上线申请，上线申请提交后会自动创建 QMQ 专用库，同时为业务机器授权。容器化带来的问题也就很明显了，随机 IP 导致数据库权限无法预先授权。这里的授权问题不仅仅是 QMQ 会遇到的，业务自己的 DB 授权也一样有这个问题，这个问题最终是引入了一个自动授权系统来解决，我们后面会详细介绍。

### 2.3.4	其他组件容器化适配

剩余的各个中间件的改动就比较少了，主要都是一些单机功能的取舍问题。下面我们做一些简单介绍。对于 qconfig，容器化适配主要是 2 个改动点：

1. 由于容器每次重启都会重建，所以容器环境下只好放弃 client 端本地配置缓存。
2. 第二个是放弃管理界面上针对单机的功能，改为组级别的模式，比如指定机器锁定版本、长期指定机器灰度配置等。

对于 qschedule，我们只是放弃了一些单机功能，比如指定任务的某些执行者下线功能。



## 2.4	DB 自动授权

公司里很多应用都需要使用数据库，线上应用访问数据库都需要提前申请权限，然后 DBA 按照机器维度进行授权。这种模式在容器化环境中无法继续使用，因为容器环境每次发布 IP 都会随机变化，并且无法提前知道 IP，因此无法提前申请机器级别的权限。同时 IP 变化速度加快，必须及时回收过期的权限，否则权限记录会不断膨胀。

- [ ] #### 以 IP 级别进行授权

  从安全方面考虑，DBA 没有放开整个容器网络的访问限制，授权依旧需要按照 IP 级别进行。经过沟通，我们联合 CM 和 DBA 团队开发了自动授权服务，用来做容器环境下数据库的授权和回收。DB 自动授权系统按照应用管理数据库账号，根据外部的请求执行授权和回收操作。CM 团队负责让容器创建后自动触发授权操作，容器关闭时自动触发回收操作。DBA 团队负责将自动授权系统接入之前的权限管理体系中。

- [ ] #### 授权记录管理

  DB 自动授权系统的第一个重点功能是授权记录管理功能，需要根据将数据库授权记录组织到应用级别，同时对外提供对应的修改接口。授权记录中保存了账号名称、数据库表名称、账号权限等。同时为了保证安全，库中不能直接记录任何的命名密码，只能记录相应的密码 hash。授权记录的来源主要有 2 个。第一个是业务申请账号和权限后，DBA 的自动推送。第二个是业务提交 QMQ 上线申请后的自动推送。为了降低业务切换到容器的成本，我们做了一次初始数据导入，由 DBA 导出所有数据库实例中的单机授权记录，我们将这些授权记录转换整理为应用级别的授权记录并保存。

- [ ] #### 权限的授予和回收

  有了比较完善的授权记录后，下面就是权限的授予和回收了。在应用发布时，CM 增加了一个初始化容器，让 pod 创建后主动触发授权操作。Pod 销毁时则是主动触发回收操作。授权和回收时，请求会带着应用和 IP 信息，自动授权系统根据应用查找到所有的授权记录，然后遍历授权记录，由自动授权系统直连各个业务库执行指定 IP 和账号的授权或回收操作。这里的授予和回收都需要考虑幂等。授予操作本身是幂等的，重复执行不会有问题。回收操作不是幂等的，需要处理重复执行时的异常。

除了功能方面需要满足要求外，自动授权系统作为一个阻塞发布流程的应用，必须要提供足够的服务可靠性保证。我们做了以下措施来保证可靠性：

1. 授权记录同时在 DB 和 Redis 中保存，数据不过期。优先读 Redis，Redis 故障可以降级到数据库。数据库故障不影响授权，只影响新授权记录的推送。

2. 服务多机房部署，避免单机房故障。

3. 授权记录数据库多机房部署，避免数据库单机房故障。 

4. 限制单个数据库实例授权的并行度，避免高度并行影响授权效率。

   

## 2.5	总结

在整个中间件容器化适配过程中，我们的所有核心改动点都是围绕固定 IP 变为随机 IP 和发布模式变为先扩容再缩容这 2 个核心问题进行。在客户端组件中尽力屏蔽环境差异，保证客户端的兼容性。在服务端中屏蔽差异的同时适当放弃单机功能的支持，推动业务切换到基于应用环境维度的管理模式上。目前，公司内部已经完成了容器化切换过程，中间件无缝的支持为业务的切换提供了坚实的基础。

















#  -----------------

# Part 2	服务化建设



云原生中有个很重要的实践就是 servicemesh，它是云原生架构模式的一个重要体现，它屏蔽了分布式系统通信的复杂性（负载均衡、服务发现、认证授权、监控追踪、流量控制等等），服务只需要管组业务逻辑，做到了真正的语言无关，服务可以用任何语言编写，只需和 service mesh 通信即可，对应用透明，Service Mesh 组件可以单独升级，它甚至被称为下一代微服务技术的代名词。

去哪儿网现在也采用的是微服务架构，但是这么多年最头疼的无非两件事情：

1. 基础部门想要提升基础能力建设，需要整个业务部门配合，成本高耗时长，严重影响了技术架构的迭代速度。
2. 频繁的安全漏洞需要全司中间件升级，业务同学忙着业务还要支持升级，常常叫苦不迭。

基于以上两个问题我们结合 servicemesh 和公司已经具备的基础能力进行了探索，本章将分两个部分分别介绍对这两个问题的解法。











# 第三章	ServiceMesh 探索与实践



## 3.1	背景

### 3.1.1	ServiceMesh 定义

在云原生架构里，单个应用程序可能由数百个服务组成；每个服务可能有数千个实例；而且这些实例中的每一个都可能处于不断变化的状态，因为它们是由像 Kubernetes 一样的编排器动态进行调度的，所以服务问通信异常复杂，Service Mesh 就是用来解决应用间稳定高效的通信问题产生的，以下是 Buoyant 的创始人 Wiliam Morgan，于 2016年9月在 SFMicroservices 的 Meetup 上第一次提到 Service Mesh，并且在其公司的博客上给出了 Service Mesh 定义：

>A Service Mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern， cloud native application. In practice， the Service Mesh is typically implemented as an array of lightweight network proxies that are deployed alongside application code， without the application needing to be aware.
>
>服务网格（Service Mesh）是处理服务间通信的基础设施层。它负责构成现代云原生应用程序的复杂服务拓扑来可靠地交付请求。在实践中，Service Mesh 通常以轻量级网络代理阵列的形式实现，这些代理与应用程序代码部署在一起，对应用程序来说无感知。

通俗来说，Service Mesh 就是增加了一个额外进程，代理业务进程的流量，在这个额外进程中集成各种治理能力，sdk 仅负责服务编解码以及与额外进程的通信。Service Mesh 通常由两部分组成：

1. **数据平面**

   这个额外进程即为数据平面，常称为 sidecar。sidecar 的存在形式通常是与业务进程共享一个 pod，由于和业务进程隔离，基础组件的同学可对 sidecar进行独立升级部署，业务同学只需专注业务逻辑，减轻其心智负担。 

2. **控制平面**
   控制平面通常是一个集中式服务，与数据面进行通信，下发治理参数，如路由规则，实例列表，重试超时配置等。

### 3.1.2	去哪儿网服务治理遇到的问题

去哪儿网内部已实现微服务架构多年，协议以 dubbo / http 为主，开发语言以 java 为主，同时存在 golang 、python、node 等，随着业务的不断发展，一些服务治理上的问题逐渐凸显：

1. 业务与中间件强耦合，迭代周期长，当sdk出现问题，需要业务方跟着升级或回滚。生产环境版本众多，每次迭代都需要做好向前兼容。
2. 多语言治理复杂，每个治理逻辑需要开发不同语言版本。工作冗余，维护成本高。
3. 公司内部 dubbo 治理相对完善，但 http 治理相对薄弱，业务方主要使用 openRestry 进行治理。其他的需求零散分布在不同平台，这对开发同学极不友好。
4. 治理能力的不统一导致重复轮子的出现。费时费力，增加了维护成本。

基于以上痛点，我们期望构建这样一个服务治理体系：

1. 统一治理，http/dubbo，java/golang/python多语言多协议都可以在服务治理平台统一治理。
2. 治理能力增强，能够补充当前缺失的服务治理功能。化零为整，降低用户去多个平台逐一治理的使用成本。
3. 治理能力下沉，基础组件开发周期和业务开发周期解耦。快速迭代，快速推广。
4. 操作友好，尽量保留用户的使用习惯，以产品思维，从用户角度出发。

在2021年我们逐步进行了容器化落地，因此结合 Service Mesh 我们开始重建自己的服务治理体系。



## 3.2	实践框架

### 3.2.1	技术选型

由于 Service Mesh 在业界已经有非常成熟的产品，因此我们不需要重复造轮子。基于已有能力快速享受 mesh 化带来的红利。我们调研了国内外部分厂商的 Service Mesh 产品以及落地方案，如下图3-1（如有差异，请联系我们及时更正）。


| 产品                   | 数据面       | 控制面                                      |
| ---------------------- | ------------ | ------------------------------------------- |
| (google/ibm/lyft)istio | envoy(c++)   | pilot(golang)，与k8s强耦合                  |
| (腾讯)TSF              | envoy(c++)   | pilot(golang)，基于pilot二次开发，解耦了k8s |
| (头条)ByteMesh         | envoy(c++)   | 自研                                        |
| (美团)OctoMesh         | envoy(c++)   | 自研为主                                    |
| (蚂蚁金服)SofaMesh     | mosn(golang) | pilot(golang)，开源可对接istio              |
| (微博)WeiboMesh        | 自研(golang) | golang                                      |
| (华为)cesMesh          | 自研(golang) | golang                                      |
| (唯品会)OSPMesh        | 自研(java)   |                                             |
| (陌陌)MOAMesh          | 自研(java)   | golang                                      |

<center>图3-1 ServiceMesh产品及落地方案</center>

在选型时，我们考虑了以下7点因素。


1. 产品是否成熟。
2. 社区活跃度。
3. 性能是否满足公司需求。
4. 开发和维护难度， 部门内主要使用 java 开发语言，尽可能控制不同语言的引入。
5. 最大程度使用开源产品，避免重复造轮子。
6. 控制面：istio 的 xds 协议已经成为 Service Mesh 的事实标准，因此在控制面我们选择 istio。
7. 数据面：考虑性能需求，学习成本，扩展性，社区活跃度等，我们选择和 mosn 合作共建。非常感谢在落地期间，社区提供的很多帮助。mosn 是蚂蚁金服开源的一款数据平面，既可以替代 envoy ，也可以集成 envoy 在网络层高性能的优势作为 mosn 的网络插件。


### 3.2.2	整体架构

qunarMesh 整体分为如下图3-2中三部分，数据面、控制面、运维面。

![image](/medias/images/servicemesh/servicemesh1.png )

<center>图3-2 整体架构</center>



#### 3.2.2.1	解耦k8s

调研时，我们发现 istio 与 k8s 耦合严重，主要体现在：

1. 作为注册中心，负责实例注册，状态维护。
2. 作为配置中心，存储各种流量策略，安全策略等资源。
3. sidecar 管理，基于 k8s 的 admissionController 机制，控制 sidecar 注入。
4. istio 启动配置，依赖 k8s 的 ConfigMap 。
5. 网络模型，通过 initContainer 修改 iptables 规则劫持流量。调用时，依赖 k8s 的 service 模型。

是否要强依赖 K8S，需要思考几个问题：

1. 内部容器和 kvm 共存，如何兼容？
2. k8s 存储是以应用为维度，但是 dubbo 以 service 为维度，如何解决维度不统一问题？
3. 内部有比较成熟的注册中心和配置中心，一刀切必然引入很多适配问题以及运维成本。
4. 如有定制化需求，修改 k8s 的成本和影响范围过大。

因此我们解耦了 k8s， 使用内部注册中心和配置中心，并自研 MCPServer 模块替代 k8s 对接 istio 。

#### 3.2.2.2	数据面

接入 Service Mesh 之后，sdk 只需要负责编解码和与 sidecar 通信。 那么 sdk 原本的功能就需要 sidecar 来实现，这里面有3个问题需要我们思考。

* 1.服务状态维护问题，如服务注册、心跳、健康检查、下线等。
* 2.sdk 与 sidecar 通信问题，流量如何拦截到 sidecar？通信方式和协议是什么？
* 3.每个请求由原来两个节点间通信变成四个节点间通信，如何确保通信的稳定性？

- [ ] ##### **业务进程的生命周期管理**

原生 istio 依赖 k8s 管理服务生命周期，通过 kubelet 模块来检测容器状态，并注册到 etcd。解耦 k8s 之后，需要我们去维护业务进程的状态，我们基于mosn 提供的灵活的扩展机制，对接了内部注册中心。

- [ ] ##### **流量拦截**

如何把流量从 sdk 拦截到 sidecar？业界主要使用两种模式：

1.iptables 拦截

 2.流量转发

考虑到 iptables 可运维性和可观察性差，在配置较多时，会出现性能下降。更多的公司采用了流量转发的机制。

如何将流量转发到本机sidecar，我们想到的4种方式：

1. 升级 sdk，在 sdk 中直接将请求转发到127.0.0.1。
2. 对于域名访问，在解析时进行拦截，例如使 用dnsMasq 将 xx.qunar.com 的请求，转发到本机 127.0.0.1。
3. 基于名字系统，在服务发现返回时，重写实例地址为 127.0.0.1。
4. 基于 agent 动态修改请求地址。

结合公司内部情况，我们采用了1，2两种方式。

- [ ] ##### **uds 通信**

因为业务容器与 sidecar 容器同属一个 pod，我们使用 unix domain socket 进行通信，规避网络栈，优化性能。

- [ ] ##### **dubbo协议优化**

sidecar 在接收到请求后，第一步是获取路由信息，用以服务寻址，按照 dubbo 原有协议设计，路由信息（ service / method / group 等）存放在 body 中，为避免不必要的 body 反序列化，我们对内部 dubbo 协议做了扩展，将路由信息放置到扩展 header 。

- [ ] ##### **流量安全**

引入 Service Mesh 后，原来两个服务的调用，现在变成了四个服务间的调用，不稳定性增加，因此需要确保，当 Service Mesh 出现问题时，尽可能的保障流量的安全。

1. **sdk 自动调用降级**
   当业务进程与 sidecar 连接断开（基于健康检查），降级为直连调用。
2. **sidecar 节点故障剔除** 
   sidecar 会对 upstream 的节点列表进行健康检查，一旦连接异常，快速剔除。
3. **一键切换 Service Mesh** 
   在 captain（新的服务治理平台）中，用户可以一键切换直连模式。
4. **柔性可用** 
   控制面出现问题时，sidecar 使用缓存数据，保证可用。
     当注册中心或配置中心出现异常，MCPServer 以及 istio 使用缓存数据，保证可用。

#### 3.2.2.3	控制面

控制面的主要职责是与数据面通信，下发 XDS 配置。  

这里简单介绍下 XDS ，X 代表一系列的发现服务，如 CDS（ cluster ），RDS（ router ），EDS（ endpoint ），LDS（ Listener ）等。 简单来说，数据面收到 XDS 配置后，就可以知道当前的 sidecar 需要监听哪些端口，接收到流量后，如何路由，如何负载均衡，如何调用等等。   前面我们说过，istio 依赖 k8s 作为注册中心和配置中心，那么

1. 解耦 k8s 之后，我们如何打通内部系统？   
2. istio 在设计之初，完美支持了 http 服务，那么在接入私有协议时，又有哪些问题需要考虑？       

3. XDS 作为标准协议，是否一成不变的落地到服务治理当中？  


在调研和落地过程中，我们也在思考这些问题。

- [ ] ##### 自研MCPServer

istio 提供了 mcp（ mesh configuration protocol ）协议，旨在解耦 istio 与底层平台的强依赖。本质上是通过 grpc 双向流，MCPServer 携带DestinationRule，VirtualService，ServiceEntry 等 istio 对象数据，传递给订阅端（istio）。
![image](/medias/images/servicemesh/servicemesh10.png )   

<center>图3-3 自研MCPServer</center>

通过上图3-3可以清晰的看出，MCPServer 所处的位置以及功能。

MCPServer 实现起来逻辑并不复杂，主要是围绕 istio 对象（即 ServiceEntry，VirtualService 等）的生成、转化、存储、传输等。

- [ ] ##### 统一注册模型

1. 对于 http 服务，注册中心数据以应用为维度，即应用-实例。  

2. 对于 dubbo 服务，注册中心数据则以接口为维度，即接口-实例，这会导致一些问题。

- [ ] ##### **性能问题**

注册数据冗余，当集群规模变大时，服务数据激增，下图3-4为应用-实例注册模型和接口-实例注册模型的对比。

![image](/medias/images/servicemesh/servicemesh4.png )

<center>图3-4 模型对比


- [ ] ##### **异构体系下互通问题**

注册模型不一致，会导致互通时寻址问题，如图3-5所示。

![image](/medias/images/servicemesh/servicemesh5.png )

<center>图3-5 互通问题


- [ ] ##### **生成istio资源复杂度问题**

![image](/medias/images/servicemesh/servicemesh6.png )

<center>图3-6 生成istio资源复杂度问题


我们为每个应用生成一个 VirtualService（可参考 https://istio.io/latest/docs/reference/config/networking/virtual-service/ ），VirtualService 中生成不同的 router。

上图3-6以 group / service 两个维度为例，展示了其中的一个 router，这个 router 是 MCPServer 负责生成和维护的，如果再增加 method 维度，或者其他路由维度，那需要维护的 router 数量将是指数级的增长，且在实际设计和编码过程中，考虑非常多的 case，复杂性和可维护性都会是个问题。这个本质上是路由粒度问题，粒度越细，问题会变得越复杂。


基于以上三点考虑，最终的方案是统一注册模型，以应用为维度。我们看到 dubbo 在3.0版本，也是使用了应用级别服务发现，全面拥抱云原生。

- [ ] ##### xds 配置按需下发

原生 istio 会将集群中所有的 xds 资源下发到每个 sidecar，导致 sidecar 内存过大，资源冗余，单个资源的变更会推送到所有 sidecar。 资源占用以及推送风暴大大制约 Service Mesh 集群规模。

![image](/medias/images/servicemesh/servicemesh7.png )

<center>图3-7 xds配置按需下发




1. sidecar 提供 sub 接口，用于业务进程订阅服务列表。
2. 业务进程启动时，sdk 会扫描要调用的服务，请求 sidecar 订阅接口。
3. sidecar 在获取 xds 配置时，会携带订阅信息（此处扩展了 istio 协议）。
4. istio 按需下发配置。
5. 为了避免 sidecar 重启或者升级导致的订阅信息丢失，会持久化到 pod 存储。

- [ ] ##### 保留用户习惯

按照 xds协议，在进行服务治理时，通常用到 VirtualService ，DestinationRule 这两个自定义资源对象，我们以一个路由设置为例。

1. 让用户设置路由来源，比如 header 中包含 user=qunar，以此来生成 VirtualService 的 match 部分。  
2. 设置路由目的地，比如 labels 包含 env=prod1 ，以此来生成 DestinationRule 的 subset 以及 VirtualService 的 router 部分。

根据这两步，就生成了一个路由规则，包含了两个资源（ DestinationRule 和 VirtualService ），在此路由规则中，能够设置超时、重试、负载均衡等治理参数。
当一个请求过来时，如果 header 中包含 user=qunar，会路由到 labels 包含 env=prod1 的实例集合。

但是在实际使用中，超时时间等一些治理参数和路由的相关度较低，用户更习惯于，作为服务提供方，配置某个接口的超时时间，或者作为服务调用方，配置下我要调用的那个服务的超时时间。

在 xds 协议中，超时时间等治理参数都是配置在服务提供方，且与路由绑定， 因此，我们做了部分扩展，用户可以从服务提供方，调用方两个角度来配置，以接口为维度，而不是以路由为维度。这些配置包括超时、重试、备份请求、负载均衡策略等， 调用时生效优先级为 调用方配置>服务方配置>兜底。

#### 3.2.2.4	运维面

运维面主要负责 sidecar 的配置，部署，升级，回滚，灰度等，各种协同系统对接。接入 Service Mesh，pod 中增加了 sidecar 容器，在解耦 k8s 之后，在容器注入，启动，配置等有哪些要注意的？

- [ ] ##### **sidecar注入和升级**

![image](/medias/images/servicemesh/servicemesh2.png )

<center>图3-8 sidecar注入和升级


1. **注入**   

   服务部署时，发布平台会请求 sidecar 管理平台，获取 sidecar 配置，包括是否注入、sidecar 版本、环境变量、配额信息等，然后生成服务的 yaml 配置。

2. **升级**

   sidecar 升级分为两个场景，1. sidecar 原地升级，不需要服务的重新部署，此时需要确保升级对用户透明，流量无损 。2.服务部署时升级。

   - **原地升级** 

     sidecar 容器和业务容器是同属一个 pod 的两个独立容器，MosnAgent 进程作为 sidecar 容器的1号进程，用于管理 sidecar 的生命周期，例如启停，健康状态，升级等。   

     在新版本发布时，会将 sidecar 镜像推送到镜像仓库，同时将二进制包推送到对象存储。   

     我们通过 sidecar 管理平台，可以对 sidecar 容器发送升级指令，mosnAgent 会去拉取对应版本的 mosn 包，之后启动新版本 mosn，并通过 fd 迁移机制，完成无损升级流程。

   - **部署时升级**   

     通过在 sidecar 管理平台修改配置，管理应用的 sidecar 版本，在下次部署时自动升级到对应版本。

   - **sidecar 容器启动顺序问题**

     在 sidecar 容器未启动成功或者配置未拉取成功，但是业务容器已经 ready，会导致请求失败。可以设置 sidecar 先于业务容器启动，并通过配置 k8s 的 postStart 钩子函数，来保证正确的启动顺序。

   - **配置下发顺序问题**

     istio 下发配置时，可能导致异常发生，新增路由时，应先下发 DestinationRule，再下发 VirtualService 。删除路由时，先下发 VirtualService，再下发 DestinationRule。

     其次可以通过 istio merkel tree 来跟踪资源下发的进度，通过暴露接口，来查询每个资源，是否全量下发到 sidecar。

   - **协同系统的对接**

     对接内部系统，如内部配置中心，全链路监控系统，报警系统，对接镜像仓库，对接镜像仓库，对象管理系统等，实现sidecar的资源管理




## 3.3	治理功能扩展

引入 Service Mesh，对于中间件开发者来说收益无疑是巨大的，与业务解耦，缩短推广周期。那么如何让业务开发同学真实感受到接入 Service Mesh 后的收益呢？我们基于 Service Mesh 体系，构建了 captain 服务治理平台，实现治理功能的增强。

### 3.3.1 统一治理

mesh 化前，dubbo 和 http 分开治理，功能分布在多个平台，现在统一到 captain，简化用户操作。

### 3.3.2	备份请求能力

为减少长尾请求，提供了备份请求（BackupRequest）功能，可选择如图3-9动态、静态策略。

![image](/medias/images/servicemesh/servicemesh8.png )

<center>图3-9 备份请求配置


### 3.3.3	多种限流策略

1. **简单限流**

   不区分流量来源。

2. **基于应用限流**

   根据流量来源，appCode 的不同，设置不同的限流阈值。

3. **基于优先级限流**

   根据不同的流量来源，设置不同的优先级，优先处理有价值的请求。

![image](/medias/images/servicemesh/servicemesh9.png )

<center>图3-10 多种限流策略




如图3-10，当流量达到设定的阈值时，触发限流，这时，流量会进入到多个优先级队列，按照优先级处理。

### 3.3.4	智能推荐

很多情况下，业务同学设置的超时时间或者限流阈值都比较大或者比较小，未能起到应尽的作用，我们根据监控数据以及合理算法，智能推荐超时，限流等参数。

### 3.3.5	其他能力

1. **引流**

   sidecar 接管了所有流量，因此天然比较适合做流量复制。

2. **预热**

   部分服务需要一定时间达到最佳服务效果，原因可能是JIT优化或者数据缓存等，在接入 mesh 之前，没有统一的预热方式，因此出现了重复造轮子的现象。接入 mesh 之后，可以实现基于调权或引流两种方式，对 http 、dubbo 服务进行预热。

3. **复杂路由**

   支持基于目标环境路由，基于 trace 上下文数据的路由，支持内部软路由等多种路由方式。

## 3.4	总结规划

目前一期已经结束，在部分业务线推广使用。

之后会在可观察性、性能优化、多语言支持上持续发力。

也希望去哪儿网的 Service Mesh 探索与实践能给大家带来一些帮助。









# 第四章	中间件自动升级



## 4.1	背景

作为公共基础服务的提供方，去哪儿网基础架构团队开发并维护了大量的客户端 SDK ，方便业务接入基础服务。经过多年的发展迭代，目前维护的客户端近30个，线上运行着数百个版本。

客户端 SDK 在给业务提供使用便利的同时，也给升级推广带来一定的挑战。例如，大规模 0day 漏洞的安全升级（ fastjson/xstream 等）, 重点功能落地推广大规模升级（容器化/ trace /全链路压测等）。每一次的大规模升级，涉及上千个应用，时间和人力的开销巨大，升级过程常常持续数月，严重影响推进效率。此外，随着迭代增加，众多的版本给维护造成了很多困难。一方面，开发新功能时，许多老的API需要费时费力去兼容；另一方面，对于中间件 SDK 本身依赖的第三方包（例如 netty / guava ），不能轻易的升级其版本，否则可能导致业务代码依赖出现兼容性故障。一些业务方为了使用三方包新功能，强行更改第三方包的版本，给兼容性埋下隐患。

基于以上的现状和问题，基础架构团队构思并建立了一套中间件 SDK 自动升级机制，用较小的成本实现中间件 jar 包版本的自动升级，为后续的 SDK 迭代铺平了道路。



## 4.2	实践框架

### 4.2.1	难点概述

1. **jar 包升级兼容性问题**

   传统规范上，jar 包的版本由用户全权控制。一般来说，业务在开发功能时引入相关 jar 包依赖，选择当下合适版本。测试上线后，版本就固化下来。通常，如果没有碰到 bug ，没有性能瓶颈和新功能需求，业务方很少会主动升级这些 jar 包版本。因为升级意味着引入兼容性风险并需要重新回归。因此要让升级自动，首先要解决兼容性问题。

2. **升级过程可感知**

   升级过程要自动化，打通 CI/CD 系统，升级过程业务可感知，升级历史可追溯，紧急状态下可回滚、可跳过。

3. **版本升级要保证质量**

   业务手动升级 jar 包时，通常需要做功能回归，以确保质量可靠。自动升级后，风险来到基础部门，相关交付质量保障措施同样需要配套。

目前我们基于这三个难点落地了批量自动化升级方案，整体流程如下图4-1，后面我们将从这以上三个难点来分别阐述。

![image](/medias/images/middleware/sdk_auto_upgrade_2.png)

<center>图4-1 批量自动化升级方案流程</center>

### 4.2.2	升级方案选型

- [ ] #### **jar 包的兼容性问题**

1. **向后兼容**

   中间件 SDK 本身 API 的兼容性, 所有对外 API 接口，必须保证向后兼容。如遇废弃 API ，必须确认业务代码中实际无依赖才能真正删除。

2. **间接依赖的兼容**

   中间件 SDK 间接依赖的第三方 jar 包的兼容性。业务与中间件 SDK 共同依赖的 jar 包，例如 netty / guava / hessian 等。这些 jar 包的各版本 API 存在或多或少的兼容性问题，一旦中间件或业务方单方面升级三方 jar 版本，势必会带来冲突。

相对来说，前者兼容性问题较容易把控，因为是我们自己维护的 SDK ，可以保证新 API 只增加不删除。而后者的三方包冲突，一直是困扰基础架构团队的痛点。举例来说，中间件依赖的 guava ，多年来一直是16.0版本，有业务方反馈版本太老希望我们升级到18.0。但是如果贸然升级，由于18.0与16.0部分 API 不兼容，则一些老用户下次升级中间件就会被影响。因此这些三方包版本几乎无法更新迭代。一些激进的业务方甚至选择冒风险自行升级18.0，不仅流程复杂，隐患也不小。针对三方依赖冲突的问题，我们开始探索依赖隔离的方案。

- [ ] ####  **依赖隔离方案**

1. **进程隔离**

   进程隔离方案来源于容器化环境 sidecar 机制，通过将中间件部署到独立sidecar进程中，来实现与业务依赖的完整隔离，这种方式理论上隔离最彻底。但当时公司内还没有推广容器化，缺乏必要的基础环境。此外，进程隔离更适合一些 rpc 通信框架客户端，但不能覆盖所有客户端，例如序列化/监控等，不太适合独立进程。

2. **类加载器隔离**

   类加载器隔离方式，来源于蚂蚁开源的 ark。通过定制修改运行容器，采用特殊的编译打包结构和类加载模式。通过不同的类加载器来隔离中间件和业务代码依赖的三方包类。该方案实现复杂，依赖较多的定制化设施。另外，实现中需要考虑 API 接口类的转换翻译（因为分别来自不同类加载器）。同时中间件内部一些相互依赖的隔离需要解决。目前还没有看到大规模落地的案例。

   第三种包路径隔离方式是我们最终选择的方案。方案实现非常简单，首先将第三方 jar 包 shade 重新打包, 将所有包路径加上 qunar 特定前缀。由于包路径变了，在 jvm 层面原包和新包的同一个 class 加载后就是不同的类。而后修改中间件，将原生的第三方依赖都换成 shade 之后的三方依赖。从而解决同业务依赖的三方包冲突问题。该方案不需要复杂的周边设施，业务代码无需修改，无感知。也能够最大化的兼容既有客户端。

3. **包路径隔离**

   我们梳理公司现有的中间件三方包依赖，去掉一些稳定依赖无需处理（版本不再更新，业务和中间件都使用统一版本），剩余大约10多个依赖需要做shade ，改动范围不大，同时中间件内部对这些依赖的版本可以做到统一。同时调研发现 flink / spring / es 等大型开源组件均使用 shade 的方式来一定程度避免依赖冲突，因此论证方案基本可行。

- [ ] #### shade 隔离方案落地遇到的困难

1. **接口签名中含有第三方依赖的类**

   我们统计出这些三方类数量只有几个，且都是语义稳定的基础类，例如 ListenableFuture 。因此，通过开发转换工具方法，实现原生类与 shade 类相互转换。

2. **反射/ SPI 等调用方式变化**

   扫描了 shade 的三方包，里面没有发现 SPI 的使用方式。而反射的场景，工具已经做了正确的类包名字符串替换。

3. **序列化/反序列化第三方依赖的类**

   原因是基础组件 SDK 接口中暴露的 Map 直接使用了 guava的ImmutableMap ，业务方 dubbo 远程调用时，直接序列化了该三方类，当对方没有 shade时，则反序列化出错。我们将对外暴露的 map 实现换成 jdk HashMap 即可。


- [ ] #### 如何规范中间件 SDK 的开发和使用

1. public 类标记 @TCPublic 、 @Internal ，配合 sonar 质量门禁规范接口使用。业务代码不能依赖 @Internal 标记的类（即使它是可见的 public 类）。
2. pom 内将 shade 包统一设置为 runtime scope ，防止业务代码误用。（否则业务开发用 IDE 补全很容易误引用到 shade 包）。
3. 中间件新增接口不再依赖第三方类，全部使用原生类型或自定义类型。

### 4.2.3	升级的自动化设施

- [ ] #### **升级时机**

我们选择在编译期升级 jar 包，通过在编译开始前修改 pom 文件, 能够固化变更历史到代码仓库，能够充分利用编译期检查。对于一次编译多次部署以及代码回滚等场景，能够保证产物一致性。缺点是依赖于业务代码的主动编译发布，我们后续会通过自动发布来解决。

- [ ] #### **升级工具**

为了实现编译期间的 pom 文件修改，我们开发出一套 maven 插件。通过编译前执行 maven shadow:upgrade 命令，对 pom 文件就地修改。执行结果如下图4-2。

![sdk_auto_upgrade_1](/medias/images/middleware/sdk_auto_upgrade_1.png)

<center>图4-2 执行结果</center>

在 CICD 流水线中，插件会在测试环境编译前执行。插件通过解析 pom ，得到现有依赖版本列表。然后查询后台，判定是否满足升级条件，并获取可用的新版本。而后执行 pom 文件修改，修改完成后将本次改动打上 tag 存入版本仓库固化下来。后续再用该 tag 发布线上时，不再执行升级（即使有更新版本)。确保线上版本和本次测试的中间件版本完全一致，尽量让升级风险在测试过程中暴露。

为了统一管理和控制升级行为，我们还开发出一套升级管理后台辅助升级插件。后台可以配置管控各包的版本，做一些前置约束条件检查（例如 jdk 版本是否满足)，以及特殊情况下添加一些黑名单应用跳过升级。

- [ ] #### **规模化自动升级**

针对升级需要依赖业务代码编译发布的痛点，后台还提供了批量全自动升级功能。允许批量的发起部署任务，周知相关人员，自动检出业务代码 master 分支版本，并编译升级，而后线上灰度部署，一段时间后全量上线。这样无需业务过多介入就能够完成大批量应用的升级。在

最近的几次安全漏洞升级和公司全链路压测推广的升级活动中，该功能大大提升了推广速度，以前需要数月完成的升级计划，现在只要2-3天就能完成，显著提升了推广效率，有效减少了对业务的打扰。尤其是全链路压测的推广，在业务试用过程中，各类问题不断暴露，压测客户端本身也不断迭代，几天就有一个新版本。由于产品的特殊性，需要链路上下游的服务都完成升级，那么每一次升级都是不小的工作量。配合批量升级后台，我们每周都能完成1-2次客户端升级，有效保障了迭代测试效率，这在以往是完全无法实现的。

### 4.2.4	质量保障机制

在自动升级这一概念推广过程中，业务方最大的疑问在于质量如何保证。由于 SDK 类问题原因众多，我们尽可能采用自动化手段守住质量底线。

1. **三方包的兼容性测试**

   由于部分 SDK 接口中历史遗留有三方包的类，业务的三方包版本完全自由定义，这部分类的兼容性必须测试通过。

2. **接入了业务接口自动化测试平台，覆盖70多个应用**

   在中间件 SDK 发版前，借助业务自动化测试平台，测试新版的 SDK 是否对业务接口有影响。

3. **测试环境业务应用启动测试**

   由于中间件 SDK 特点，很多问题会导致启动失败。借助测试环境自动跑启动测试，也帮助我们发现了不少问题。在 SDK 完成测试后，为了确保未测出隐藏问题影响范围可控，我们在后台管理平台上植入了灰度发版机制。通过应用重要性级别，给应用按照 P1、P2、P3 分组，不同分组对应升级 SDK 不同版本。因此，新发布的 SDK 会先覆盖边缘的应用，数天后再逐渐覆盖全量应用。遇到极端问题时，不至于影响范围太大。



## 4.3	总结展望

### 4.3.1	总结

当我们刚开始思考中间件升级方案时，觉得这是一个不可能完成的项目，其中的风险、挑战都是不可想象的，但是当频繁的安全漏洞升级、中间件依赖和业务依赖冲突带来的成本越来越高时，我们就不得不开始思考这个问题，因此我们通过定义问题（中间件升级困难、成本高）、测量（每次升级横跨好几个月、投入100PD+）、分析（自动升级的关键点：方案选型、质量保障、批量自动化升级等）、改进（方案落地）、控制（提前验证、过程质量控制、逐步接入更多组件等）将自动化升级落地了。目前我们取得了如下成果：

1. 中间件自动升级机制已覆盖公司绝大部分活跃后端应用。使用 SDK 版本在3个月以内的应用数占比近80%，版本在1个月以内的应用数占比60%，新版覆盖效率大大提升。
2. 已经接入组件除了基础架构的中间件所有 SDK，还包括许多部分业务基础组件，例如机票基础数据等。
3. 配合批量升级机制，目前全公司大规模的升级可以在几天内完成。
4. 质量方面，目前自动升级系统运行稳定，未出现严重故障。业务方从一开始不敢尝试到现在主动要求帮助他们升级。

### 4.3.2	展望

未来我们将会搭建公司级的二方/三方包管理平台，包括依赖数据收集、依赖版本分析、依赖版本控制、依赖自动升级，真正的实现中间件的治理闭环。











#  -----------------

# Part3	开发提效



整个研发生命周期包含开发、测试、上线、运维等多个阶段，同时也涉及产品、开发、测试、运维等多种角色，但是纵观我们的整个建设过程，之前更多的关注的是开发测试运维角色的效率以及流程的效率，更多的是避免浪费，原因是这几个领域的解法效果更更快，包括我们 DEVOPS 实践、质量保障等，而产品、开发这两个角色是我们关注较少的，但是这两个角色对于我们企业的生存和发站又是至关重要的，因此随着整个行业的快速发展，当前已经进入了相对稳定的阶段，大家也开始探索开发效率和价值交付，云原生的 serverless 给出了很好的解法。

Serverless 的全称是 Serverless computing 无服务器运算，又被称为函数即服务（Function-as-a-Service，缩写为 FaaS），是云计算的一种模型。以平台即服务（ PaaS ）为基础，无服务器运算提供一个微型的架构，终端客户不需要部署、配置或管理服务器服务，代码运行所需要的服务器服务皆由云端平台来提供。 国内外比较出名的产品有 Tencent Serverless 、AWS Lambda 、Microsoft Azure Functions 等。Serverless 比较典型的实践是低代码和云开发，低代码让产品可以快速的进行想法验证，云开发可以让技术同学节省了大量的环境搭建学习成本。本章将从这两个维度分别介绍我们的思考和实践过程。











# 第五章	开发提效



## 5.1	BFF

### 5.1.1	背景

大前端中心拥有大量的前端工程，带有 appcode 的 node 端工程200+，qzz + rn + 小程序工程300+。当前的现状是疫情影响、人力紧张、需求堆积较多、前端人力维护现有工程已经捉襟见肘。如何在当前人力紧张的情况下，服务好用户、做好用户体验的前提下高效的维护好这些工程，那么提效就是我们的主要抓手，除了之前已经做过的低代码平台以外，我们如何继续提效，如何找到低成本的方案进行大幅提效成为我们迫在眉睫的核心诉求。

我们首先分析了当前会影响到前端效率的一些问题点，梳理下来主要有以下一些问题：

1. UI 逻辑处理争议（比如代金券的状态有待领取、已领取、已作废、已过期等状态，状态是后端计算得到的，不同状态展示不同，站在后端角度不需要关心前端 UI、站在前端角度不同状态展示不同希望有人给算好展示相关的数据和样式，前端可直接渲染，由于低代码的组件是跨业务复用的，不同业务的后端立场不同，接口返回也会有差异，前端后端关于UI逻辑处理存在争议）
2. 多接口数据整合争议（有些业务场景下前端一个组件的数据是由后端多个团队提的接口，接口整合工作的划分也存在争议）
3. 相同代码在不同前端工程存在多份（由于有些情况下前端无法说服后端去做相关数据处理逻辑，那么大量数据处理逻辑就放在了前端，前端可能不同端、不同 team 都会用到某接口数据都需要进行数据处理，这种场景下相同前端的数据处理逻辑代码就散落在不同的工程代码库中）
4. 前端同质化 node 服务多（有些情况下，为了前端为了搞一个服务于前端的服务来方便前端开发，前端会自建 node 服务，各个 node 服务其实处理的事情大致是相同的，但是每个场景下都单独建立了各自的服务）

以上的问题在不同项目中不断重复的影响着前端的开发效率，所以我们的目标就聚焦到了：梳理一个可以解决以上问题的合适方案。



### 5.1.2	方案选型

下图5-1是我们对于整体方案的核心诉求和行业内的解决方案。



![bff-01](/medias/images/cloudbase/bff-01.png)

<center>5-1 方案选型</center>

我们希望整体方案，可以完全自控、代码安全不托管、保持 DevOps 不变，且能数据处理逻辑改动后够按照线上的 case 数据自动化的回归测试，我们对比的国内阿里云，和国外亚马逊的 AWS Lambda 都无法满足，所以我们走了自研的方案。

![BFF-02](/medias/images/cloudbase/BFF-02.png)

<center>5-2	方案对比</center>

核心点是开发、构建、测试、发布、线上问题排查等去哪儿都有自己的一套完整流程。首先业界的都做的比较完善，功能齐全，能满足我们的基本诉求。其次，问题排查，目前我司的排查问题整体链路已经非常明确，工具也相对完善，通过用户名、手机号可以看到每一个节点的数据输出，如果使用了外界的产品，这一部分的数据输出就是个黑匣子，会给我们排查问题增加很大的难度。最后，自动化测试，去哪儿内部的一些用户输入输出我们是可以拿到的并进行真实 case 自动化测试的，但是如果使用外界产品，无法拿到线上真实案例。

所以这里核心点我们主要是借鉴业界的思想，进行自研实现。

我们的方案核心思想总结为一句话就是：以 Serverless 思想，搭建 BFF 层，建设一个 FAAS 服务平台，前端人员在平台上写云函数即可完成业务开发，不需要关心服务器端知识、不需要搭建工程，可以云端复用，可以进行自动化的线上 case 验证。



### 5.1.3方案详解

整个方案的核心可以用以下这张图5-3进行概况，接下来进行详细的展开介绍。

![bff-03](/medias/images/cloudbase/bff-03.png)

<center>5-3	方案对比</center>

整个方案，以 Serverless 思想，搭建 BFF 层，建设一个 FAAS 服务平台，定位的角色是服务层，边界是处理数据、但不处理 dom。

1. 易用性：前端人员可零成本的上手使用、在线调试，无需额外的学习成本。

2. 问题排查全链路可回溯：从发起端到后续各个环节可以可视化查看全链路调用，进行精确定位，也可根据关键信息，查询各个节点输入输出，也可以根据线上 case 还原回溯调试处理逻辑。

3. 代码跨工程复用：由于运行先服务端，不受端的限制，云函数可自由复用。

4. 自动化的线上 case 验证：修改云函数代码后可根据线上 case 数据，自动进行回归验证。

5. 高性能&高可用：作为统一的服务，对应搞性能高可用也有较高的要求。




### 5.1.4	架构讲解

- [ ] #### 系统架构


系统架构如下图5-4：

![bff-04](/medias/images/cloudbase/bff-04.png)

<center>5-4	系统架构图</center>

整个架构两条分支对应了两种场景：

1. 第一种后端站在服务端的角度认为数据很合适，但是前端认为不合适，后端接口数据需要处理下，给前端用的。
2. 第二种需要 BFF 层去组合多接口的场景



- [ ] #### BFF 架构


BFF 架构如下图5-5：

![bff-05](/medias/images/cloudbase/bff-05.png)

<center>5-5	BFF架构图</center>

1. BFF 的界面配置层用公司内的低代码系统搭建平台进行搭建。
2. 服务层主要使用容器化部署方式支持方便支持自动扩缩容。

3. 持久化存储采用 MySQL 关系型数据库进行存储。




### 5.1.5	系统介绍

- [ ] #### 配置系统


配置系统主要可以进行单函数、编排的在线编写维护工作。系统单函数管理界面如下图5-6。

![image](/medias/images/cloudbase/bff-06.png)

<center>5-6	系统单函数管理界面</center>



系统编排管理界面入下图5-7。

![bff-07](/medias/images/cloudbase/bff-07.png)

<center>5-7	系统编排管理界面</center>

- [ ] #### 单函数介绍

单函数的使用场景：对于较为简单的数据处理可以使用单函数。单函数规范如下图5-8。

![bff-08](/medias/images/cloudbase/bff-08.png)

<center>5-8	单函数规范</center>

其中语法检查主要利用 babel 进行校验，避免常规错误，比如：返回 promise 的函数前面必须跟 await，函数必须有返回值，不能有声明但未使用的变量等。

- [ ] #### 编排介绍


编排的使用场景：组件多、数据节点多层级深、依赖接口多、整体逻辑复杂等场景下，适合使用编排进行逻辑的拆分和组装处理，可以降低单函数的复杂度，增加复用性。编排结构如下图5-9：

![bff-09](/medias/images/cloudbase/bff-09.png)

<center>5-9	编排结构</center>

编排的执行过程如下图5-10：

![bff-10](/medias/images/cloudbase/bff-10.png)

<center>5-10	单函数规范</center>

- [ ] #### 平台能力

1. 平台主要提供了以下6中通用能力，供开发者在编写函数时进行调用。

2. 函数调用的能力，可通过 getHandle 传入函数标识，进行函数调用。
3. 获取 QConfig 配置文件的能力，通过 getQConfig 方法进行配置文件获取。
4. 可以进行 redis 操作，通过 redis 对象进行 redis 操作。

5. 进行请求，通过 request 进行请求。

6. 通过logger对象进行，日志打印操作。
7. 中断编排执行，可通过 composeBreak 进行编排的中断。


- [ ] #### 版本方案


我们的会有 beta、仿真、线上3种环境，版本方案如何选择，我们的核心期望是：版本一致、开发易用、方案清晰。最终我们的方案如下图5-11。

![bff-11](/medias/images/cloudbase/bff-11.png)

<center>5-11</center>

如图5-12，多人协作时类似git版本管理，每个人可独自产生小beta版本进行各种开发，最终merge的放进行合并上线。

![bff-12](/medias/images/cloudbase/bff-12.png)

<center>5-12</center>

- [ ] #### 易用性设计

为了提升易用性，我们的平台支持以下几方面的内容：

1. **全链路问题定位**

   线上的一个请求我们可以通过traceId串联起各个调用环境和输入输出，可视化的方式进行展示，也可以按照关键信息进行各个环节的日志查询。

2. **在线调试**

3. 我们可以根据 traceId 进行线上数据调试，也可以自己伪造mock数据进行在线调试。

4. **搜索定位**

   代码可搜索定位。

5. **引用函数跳转**

   类似本地编辑器可以按住 ctrl 建点击跳转到引用的函数体中。

6. **服务器端能力抹平**

   我们的编辑器是在 web 浏览器端运行的，但是打日志、qconfig 文件的读取等能力操作是必须运行在node服务端的，为了给开发者屏蔽掉这个区别，方案直接在线编写、在线运行调试看效果，我们进行了服务端能力抹平。

7. **可视化流程图**

   编排中函数关系又可视化的流程图进行展示。

- [ ] #### 性能安全高可用


为了保障整体服务的安全高可用，我们做了以下功能。

1. 代码在线 CR

2. 多环境验证

3. 线上真实 case 自动化回归测试

4. 上线审核

5. 支持限流配置

6. 服务双环境秒切




### 5.1.6	落地效果

- [ ] #### 系统性能


落地系统性能，如下图5-13，单函数、编排执行耗时 P90 优化到了 10ms 以下：

![13](/medias/images/cloudbase/13.png)

<center>5-13	落地系统性能</center>

- [ ] #### 接入场景

目前主要的接入场景：

1. 营销活动：双旦集卡、五一大促、中秋、十一大促等。
2. 门票主流程：POI 货架、POI 头部、L 页。
3. 酒店 neeko 标签系统：neeko 标签系统是展示在酒店D页酒店卡片上的标签数据。

- [ ] #### 接入效果


门票接入效果和接入页面如下图5-14：

![14](/medias/images/cloudbase/14.png)

<center>5-14	接入效果和页面</center>

- **代码层面**

  - 2w行降至6500+行

  - 更专注于展示

- **迭代效率**

  - 不需要发QP包
  - 不需要QA

  - 1pd -> 十几分钟

- 酒店 D 页标签处理逻辑，从 neeko 标签系统迁入 BFF 效果

  - P99 从 400ms 降低到 152ms 降低72%

  - P50 从 96ms 降低到 24ms 降低75%

### 5.1.7	未来展望

未来规划主要有一些5方面的事情可以落地继续推进：

1. 缓存预热，减少缓存冷启动带来的性能损耗。

2. 自动扩缩容。

3. 在线修改记录查看。

4. 业务接入&推广。

5. 按业务进行集群物理隔离，避免某个业务影响全局的问题出现。




## 5.2	CMS

### 5.2.1	背景

去哪儿网在旅游行业，拥有庞大的用户群体，除了日常的购票之外，我们也想和用户做更多的触达，让用户感受到平台的温度，这个触达主要分为三个部分：

1. **日常的促销**

   比如机票、酒店购买完毕，可以抽奖、发起砍价拿现金等活动，增加用户的粘性，切实给用户带来实惠。

2. **新客的引导**

   app 端和小程序都有新人专区，有各式各样的活动、任务引导用户购票转化。

3. **节日活动**

   逢年过节时，平台会推出大力度的促销活动，进一步的提高用户转化，提高品牌影响力。

上面主要的三个场景，去哪儿网各业务线也都有对应的产品运营团队来负责，那么问题来了，触达用户是需要页面的，如此高频度，大规模的页面，该如何产生呢？



### 5.2.2	解决方案

如果按照常规的项目实施流程，成本和周期都是无法满足需求的，我们先畅想下方案，再看落地的问题：

1. **页面数量多** - 走复用思路，不必每个页面都重新开发。
2. **变动频率高** - 支持配置化，不必每次变动都重新发布。

第1个问题，如果要走复用思路，就要对营销的不同场景进行抽象，并且和 UI 同学达成设计上的规范统一，进而达成页面风格的统一。

第2个问题，如果要支持配置化，首先需要对页面组件可变化的点进行预先设计，这样一来才能够把这些做成配置选型，集成到系统里面去。

所以说到这里，非常明确需要技术人员去落地的东西就出来了：

1. 设计一套系统允许开发人员将组件开发好，然后集成到系统，运营人员可以在系统里灵活组装出自己想要的页面。

2. 页面上线之后，如果有修改的点，允许运营人员在系统里对组件配置项进行修改，并且预览，效果好了，就推到线上。

   

### 5.2.3	 系统设计

- [ ] #### 页面管理系统


要想使得页面配置化上线，首先我们需要设计一个页面管理系统，允许对组件进行拖放、编辑，来完成页面的组装和上线。

这个系统如图5-15所示，我们给他命名为：CMS=Component Management System ——组件管理系统，和行业的" CMS "内容管理系统略有不同，接下来都用 CMS 来简化表述。

组件是 CMS 系统最为宝贵的素材，我们的愿景和实际行动也是按照如下理念去做的：

**页面不是开发出来的，页面是组装出来的，运营可以根据组件库灵活的去组装，组件库的每一次迭代都在为运营赋能。**

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7nYPTPO4KoSVSjkiaHTfu16HJpMz3u5WmrRKUMD5IwvgPia11AlvIW6CokZsTk99Ry0D98jL5yJ3CsQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

<center>5-15	页面管理系统</center>

- [ ] #### **开发组件接入**

组件是需要迭代的，一个组件的生命周期是：开发->测试->部署->集成→运营使用，中间不可避免的需要修改，因此整个链路需要精心进行设计。

- [ ] #### **面临的问题**

业务众多、组件数目多，如果都在一个工程进行迭代，势必会越来越臃肿，所以我们设计了分布式工程开发方案，就是开发人员可以自己建一个 git 工程，然后集成到主系统。

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7nYPTPO4KoSVSjkiaHTfu16HLykWfvNhYNuvUFPo5QH6WnFqPReHTwnPj3R6gibc8kbr8EHR3IjjqNA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

<center>5-16</center>

对于开发完成的组件，可以自行发布，到主系统进行集成：

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7nYPTPO4KoSVSjkiaHTfu16HM6svaYzN9tcOEp3vMrkuQgwbpTXLgJz8T3XQ4C9xQiahulsM4YWsjqQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

<center>5-17</center>

组件集成完毕，我们可以在营销分类管理界面将需要的组件都集成进来：

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7nYPTPO4KoSVSjkiaHTfu16HKSiakFXdYn1SvothUUWSAuoDTn3BVj4m0po5icALAqiax5SaDfc3K9O4g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

<center>5-18</center>

这样一来运营同学就可以在组件列表里看到这些组件，直接拖放到页面，进行编辑上线了。

- [ ] #### **页面渲染系统**


关于页面渲染，分为主要三种情况：

1. app 端内渲染，hybrid 支持离线包。
2. H5 浏览器端，支持服务端渲染。
3. 小程序端渲染，天然离线包模式。

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7nYPTPO4KoSVSjkiaHTfu16Hq0xPfDcHe2ANg1by2zfcjW4dnuQsxicVaiaDnT7vcCWnKF08dl43S0pA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

<center>5-19 页面渲染系统</center>

如此，各个端我们都争取到了较好的渲染效果，这其中每一步都有一些问题需要解决：

1. **app 端内渲染**

   我们在打包资源的同时，要考虑运营修改配置实时生效，那么 node 端页面承载系统，就要支持这种延后请求配置的情况。还有断网时候，自动使用上一次配置，降级渲染。

2. **H5 浏览器端**

   如果对于性能要求高的场景，可以把模板同步到 node 端，这样会计算好最终的 html 返回给浏览器，直接进行渲染。

3. **小程序端**

   代码打包出来是以原生模式渲染，天然离线包模式，也做了断网时候，自动使用上一次配置，降级渲染。

   

### 5.2.4	演进之路

最初的 CMS 系统页面只支持 H5 端的渲染，并不支持小程序端原生渲染。随着去哪儿网砍价项目的逐步发展，会有当前页弹出授权、订阅、分享等一系列和原生能力打通的诉求，如果是 H5 运行在小程序里，那么势必要做一个原生中间页去承载，这种第一是方案复杂、第二用户体验也不好，于是才有了下面的探索。

- [ ] #### **多端渲染**


因此我们需要去探索一种基于一套源码，在微信小程序端也能按照原生方式去渲染的方案，于是调研了如下几种方案：

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7nYPTPO4KoSVSjkiaHTfu16HGzM3TCnqtib7W1Z8bfibJ8H8IbGDqZ4T8ASYb5a1oZovTJkSqsBXqx8g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

<center>5-20	方案选型</center>

综合成本和工程、组件现状，最终采用了自研的方式，将 CMS 组件以原生方式运行在小程序端，基本的思路是在两端分别开发一个容器，这个容器在会在运行时将组件的调用都适配到小程序端。

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7nYPTPO4KoSVSjkiaHTfu16Hho9oKqgD57l3HibcnLLsLibITBziamsvO0toFPre9C8XmFYbqSjL2Dxiag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

<center>5-21</center>

毕竟是截然不同的两个端，中间页遇到了非常多的细节问题，目前均已解决：

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7nYPTPO4KoSVSjkiaHTfu16HJXddlSoxwjauPPex8wNiaE8vibLk2CyLic9MwH3JPK3PhHPINRDxMNkuw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

<center>5-22</center>

- [ ] #### **关于性能**

小程序端更新页面的方式就是 setData，那么对于复杂的页面，setData 会导致渲染卡顿，参考官方输欧盟：

*https://developers.weixin.qq.com/miniprogram/dev/framework/performance/tips.html*

因此 CMS 采取的方案是，使用微信的 Component，这样一来可以做到局部刷新，即便是页面组件比较多，性能也可以优化控制。

关于 CMS 组件自动适配到 Component，主要涉及到一个组件动态包装问题，具体按照小程序的规范，感兴趣可以看下官方文档：

*https://developers.weixin.qq.com/miniprogram/dev/reference/api/Component.html*

它的生命周期是比较复杂且特殊的，因此需要在 CMS 小程序容器端对这些生命周期进行磨平，setState 到 setData 进行自动转换，这两个问题解决，基本就适配了。

- [ ] #### **风险控制**


有人操作的场景就会存在风险，尤其对于已经在线的页面，每一次组件属性的配置、组件皮肤的调整都有可能引起故障，那么站在系统维度，我们做了如下事情：

1. **组件遵循版本管理**

   也就是说开发增加了一个新版本，对于线上已有页面，除非运营主动点击更新，否则是不会使用最新版本的，这样可以降低页面呲掉的风险。

2. **组件属性编辑校验**

   对于一个组件来说，有些属性是必填的，有些属性必须是满足某个格式的，那么如果没有一个约束，就可能会导致页面呲掉，所以组件属性允许配置校验规则，可以一定程度规避这个问题。

3. **页面配置草稿箱**

   每一次修改默认都是进草稿箱，只要不点击上线，那么就不可能影响到线上页面。

4. **页面上线需要审核**

   对于重点页面，每一次点击上线，都需要审核，审核人可以在系统 diff 变化的部分，进行测试校验，通过之后才能最终上线。

5. **系统运维完善**

   页面组件的渲染率，JS 代码执行的错误，以及图片等资源加载错误情况，统一接入实时监控，确保尽可能主动发现线上问题。

- [ ] #### **流程沉淀**


系统体系搭建起来了，那么要想发挥真正的复用价值，还需要依赖流程的规范化；如果需求没有统一的一个收口，那么还是会导致组件的重复开发。所以如下四方的协作关系需要做一个规定：

1. **运营**

   需求基本都来自于运营，运营同学有了需求之后，如果是简单的会直接提报给设计，如果复杂的会提报给产品，当然有时候也会直接找技术。

2. **设计**

   对展示和交互需求进行统一把关，初步决策是否复用现有组件。

3. **产品**

   对功能和逻辑需求进行统一把关，初步决策是否复用现有组件。

4. **技术**

   结合三方的诉求和建议，以及组件代码设计角度，给出进一步建议，是否复用，以及复用的改造程度；如三方有争议，还是会结合场景具体分析决策。

有了流程，覆盖到大部分场景，但是实际中会有绕过、跳步的情况，所以技术这边最终的把关就非常重要。设计、产品、技术三方收到需求，对于有争议的及时升级其他两方确认，也非常的主要。



### 5.2.5	效果总结

- [ ] #### **成本角度**


去哪儿网此前有三套营销前台管理系统，随着机酒事业群的诞生，CMS 经历了一期、二期的迭代，已经能够将机票、酒店、市场的营销需求接入进来，统一了设计、开发、运营流程。

带来的直观效果就是维护系统的人员变少了，设计同学专注于组件皮肤的设计，精力也得到了释放，运营同学有大量可复用组件，上活动频次也更高了。

除了日常的营销上线之外，融合之后上线了：双镇店铺、999秒杀、十一大促三个重点项目，抽奖、秒杀、代金券、货架组件在这三个里面都得到了复用，尤其是 机票、市场、酒店三个业务的十一大促活动，做到了0开发，直接配置上线。

- [ ] #### **体验角度**


1. 体验一致性：此前三套系统，背后的 UI 团队也不同，那么雷同组件皮肤也不一致，伴随着 UI 团队的统一，页面展示效果一致性更好了。

2. 页面可用性：在同一套系统收口，CMS 在 app 离线包和小程序离线包场景都做了降级渲染处理，以及图片加载自动重试等优化，提升弱网环境体验。


- [ ] #### **承载业务**


系统目前承载了 机票、酒店、市场的砍价业务，每天为平台拉来了大量的新客，赢免单、话费充值、支付后抽奖等高频次业务，以及机票的日常航司运营活动、酒店、玩乐的日常运营活动，都可以运营自主配置上线。



### 5.2.6	后续规划

经历了 CMS 一期、二期迭代之后，在组件覆盖、系统并发及风险控制方面，都得到了保障。但是最终这个事情要想比较圆满，势必要满足如下几点：

1. 开发成本大幅度降低。
2. UI 设计成本大幅度降低。
3. 运营成本大幅度降低。

面对1和2，因为复用了，成本自然降低了；但是由于某些组件配置比较复杂（比如秒杀、货架、代金券等），运营同学在使用起来还是比较吃力，所以我们设了一个感性的目标：

**运营顺畅度提升**

如果运营配置起来，很顺畅，那意味着用时更少了，自然这个问题就不存在了，而提升运营顺畅度，不仅限于组件属性的优化，我们计划：

1. 组件添加、删除、修改基础操作顺畅度。
2. 组件属性简化、汉化、手册同步配备。
3. 能够快速的找到想要的组件，充分发挥自己的想象力。

服务于营销，唯一不变的就是变化；对于营销而言，任重而道远，营销的场景也是多变且复杂的，我们必须时刻保持敏感、如履薄冰，对系统进行优化，才能够更好的服务于用户，赋能营销。



## 5.3	云开发实践

### 5.3.1	背景

- [ ] #### 我们遇到的问题

在云原生的落地过程，serverless 是很重要的一个课题，它能让开发同学免去繁琐的服务器搭建和运维工作，轻松的实现功能的迭代和交付。云开发是目前serverless 架构的经典实践，因此我们基于此进行了探索。

去哪儿网拥有大量的前端工程，数以百记的前端工程代码，对于前端工程师来说如何在当前疫情严重、人力紧张、不熟悉项目的情况下，如何能够随时随地快速着手开发并发布项目？对于团队来说如何保证团队配置化的统一，让开发者按照规范工作？

带着这些问题，参考云原生实践，我们实现了去哪儿网云开发平台，面向业务场景提供了标准的容器化的开发环境，对开发者来说大大提升了便捷性。我们取得的成果主要有：

1. 对于首次启动项目并开发来说提效80%。
2. 对于日常开发维护的项目提效60%。

本文会重点阐述云原生背景下云开发带给开发者的优势，以及去哪儿网云开发整体的技术方案以及产品实践。以下这些场景开发同学应该经常遇到：

1. 当我们要对一个新项目修复bug时，很可能代码改写只需要5min，但安装环境花了两小时。
2. 作为一个新人，或者说接手一个新项目的时候，按文档教程安装环境，项目依旧跑不起来。这个有可能就是某个依赖的版本安装的不匹配或是文档没有及时更新。
3. 前后端联调时，我们前端总是需要发beta版本才能让后端和产品同事看到页面效果。如果有一些需要频繁修改的内容，那么发版就是一个很大的时间成本。
4. 当我们休假的时候，为了保障业务的稳定，必须得带着办公的电脑，那如果现在有一种方案可以不需要那么合适的设备即可办公，是不是就解决了24h 待机的问题。

以上都是我们在传统开发中常见的问题，我们的云开发就是为了解决这些问题出现的。



- [ ] #### 传统开发VS云开发


![传统开发VS云开发](/medias/images/cloudbase/01.jpg)

<center>图5-23 传统开发 VS 云开发</center>

如图5-23所示，左边这一侧是传统开发的流程，可以看到，它的工作区重心是在本地，需要开发者自己去安装  IDE 安装、搭建依赖环境、自己去 gitlab 上拉取代码，然后开发完成后再去到发布平台去发布。

而云开发呢，相对于传统开发，它的工作区重心从本地转移到了 server 端。开发者无需去关心 IDE 安装、代码拉取，更重要的是环境依赖安装，这一步呢其实就解决了我们先前提出的环境安装问题。同时，在 server 端启动服务后，会对外暴露一个服务的地址，这个地址可以在内网环境被所有人访问，这样的话，也就免去了每次都要发 beta 版本联调的问题。那对于开发者来说，本地只需要一个浏览器即可。我们不需要那么合适的设备即可快速上手开发。

从以上可以看出，云开发相对于本地开发有着一些独特的优势，接下来我们详细总结归纳一下。

- [ ] #### 云开发优势


![云开发优势](/medias/images/cloudbase/02.jpg)

<center>图5-24 云开发优势</center>

如图5-24，云开发的优势可以分为4个方面：

1. **解决运行环境问题**

   云开发环境会根据业务场景提前预置需要的依赖，而这另一方面也保证了团队之间的环境一致性，从而避免因为环境导致的一系列问题。

1. **探索云时代团队协作的新模式**

   怎么做到团队配置化的统一，怎么让开发者按照我们的规范去工作。比如当团队协作开发时，我们每个人本地安装的环境版本是否一样，也是一个非常重要的开发问题，即便你的开发环境有很详细的文档，也很难把所有的细节都写清楚。

   举一个例子，像前端同学经常用的 npm 包，发版会很频繁，如果同步只能是群里发公告，然后其他同学再去更新，一来一去增加了非常大的时间沟通成本。那如果用了云开发，就能避免这些问题。

1. **闭合整个研发链路**

   像我们公司团队很多，各个平台层出不穷，开发过程中往往会伴随多个平台的切换，将IDE嵌入浏览器，同时整合各个研发平台，这样的话避免反复切换平台带来的繁琐与不便。

1. **便捷性**

   像在疫情之下，难免会有远程办公的情况，云开发让我们不必需要合适的设备即可写代码，这对便利性也是极大地提升。我们开发同学随时创建随时上手开发。

所以综合来看，不论对个人还是企业来说，云开发都有着非常大的优势，这些很大程度上可以提升开发效率，助力研发效能。基于公司容器化基础设施，再加上业内开源产品的背景下，为了解决本地开发环境等一系列问题，我们去哪儿开启了自己的云开发时代。



### 5.3.2	整体方案

在了解云开发有这么多的优势之后，接下来我们从去哪儿的角度看实践过程中云开发上的一些理念和方案。这个理念总结来说就是并不是纯粹从一个技术和工具的视角去看待，而更多的是从业务需求/业务场景的视角，我们看怎么能够把这个技术给用好，并且以此需要做什么样的技术创新。

- [ ] #### 系统整体架构


![整体方案](/medias/images/cloudbase/03.png)

<center>图5-25 云开发系统架构</center>

如图5-25所示，系统整体架构划分为三部分：开发者本地、远程 server 端以及管理系统。

开发者在本地使用浏览器通过 https 来访问远程 server 端，那在 server 端我们是基于公司 k8s 集群搭建的，会针对不同业务场景设计相应的镜像，从而实例化容器，这个容器的工作区包含 了webIDE、开发环境和代码等等，这一块就是相当于给用户提供了一套标准的开发环境。最后一部分就是管理平台，包含了对webIDE 的管理、文件资源的持久化、开发者的权限和其他平台的整合等。

接下来我们分块讲解下我们在做的过程中的一些方案抉择。

- [ ] #### webIDE选型


实现云开发需要借助一个非常重要的产品— webIDE ， webIDE 简单概括就是只需要一个浏览器，就能编写代码，调试代码，甚至是发布代码。

虽然 webIDE 本身还未流行，但是在各大云厂商，各大国内大厂均已有落地产品，谈不上一个比较新的技术，而且实现方案，也基本明确成熟，这里挑几个业内有代表性的产品进行介绍。

1. **[Theia](https://github.com/eclipse-theia/theia)**

   Eclipse 推出的云端和桌面 IDE 平台，star 数有17.3K，完全开源。Theia 是基于 VS Code 开发的，它的模块化特性非常适合二次开发，比如 gitpod ，华为云 CloudIDE 、阿里系 IDE 便是基于 Theia 开发。

2. **[Code-server](https://github.com/coder/code-server)**

   Code-server 是由 Coder 开发的，它的理念是把 VSCode 搬到了浏览器上。目前 star 数有56.6K，个人版是开源的。

   vscode 是基于 electron 架构，大家都知道 electron 它的 UI 最终是运行在 chrome 浏览器中，这整个和 web 架构在 UI 层是天然相通的。其次 vscode 底层架构源是 typescript ，那 typescript 对于前端同学是相对于比较熟悉的，对于我们后面进行一些源码结构改造提供很好的可拓展性和可定制性。

   vscode 有个最大的优点，它为什么能够发展到现在这个程度，其实和他的插件机制是有关系的。vscode 本身的功能其实不是很多，其他的功能都是以插件化的方式插入到 vscode 里面去。

3. **[stackblitz](https://stackblitz.com/)**

   stackblitz 是一款非常方便写 demo 的 IDE 。提供了非容器化方案的纯前端 node 环境，可以说非常有价值。整体来讲，其技术方案的优点在于不消耗远程资源，但是缺点，一是不开源，二是毕竟是模拟的 node 环境，在系统的一些层面可能会有所缺失。

4. **[JetBrains Projector](https://jetbrains.github.io/projector-client/mkdocs/latest/)**

   JetBrains Projector 是 JetBrains 提出的“远程开发”解决方案，基于 Client + Server 架构。star 数有1k，也是开源的，但最近 JetBrains Projector 作为自己的独立产品的开发已暂停，但 Projector 仍然是 JetBrains Gateway 的重要组成部分，因此官方建议从 Projector 切换到 Gateway。


- [ ] #### **小结**

不管是从 star 数，还是开源，拓展性角度来说，code-server 优点是我们初期阶段选择它作为 webIDE 的原因。当然其实对于 webIDE 这一层，是完全可拓展的和可更换的，这个下面讲镜像架构的时候会提及。



### 5.3.3	本地容器 VS k8s 集群

确定好了 webIDE ，接下来就需要考虑 webIDE 部署在哪，是通过容器的方式部署在用户本地呢还是部署在公司统一管理的 k8s 集群上呢？

那我们就来对比下这两种方案的优缺点：

1. **本地容器**

   docker 启动在开发者本地，对于开发者来讲使用的自由度高，但随之而来的是不利于统一管理。对于电脑的配置也要求比较高，而且 docker 对于我们前端同学来讲也是一个比较大的学习成本。

2. **k8s集群**

   它的优点在于便于团队之间的协作开发，且利于公司统一管理，而且不需要依赖于特定的设备，最主要的是我们开发同学不需要额外学习关于容器化的知识，上手比较快。

所以最终我们选择 K8s 集群管理模式。



### 5.3.4	镜像架构设计

确定好了容器部署，接下来需要设计镜像，因为容器是基于镜像创建的，是由镜像实例化而来的。在设计镜像时，我们是从业务场景出发来梳理的。

起初，我们是先为 cms 低代码场景定制的，原因是它短频快的开发模式和云开发即开即用的特性很契合，cms 业务覆盖面大，像机票、酒店、火车票、市场等各业务线都在使用，而且入手难度低，再加上考虑到其他场景的拓展性，比如说 qrn、node、h5 这些，通过分析这些场景使用的依赖的异同点，我们设计出了下图5-26的镜像架构图。

![镜像架构设计图](/medias/images/cloudbase/04.jpg)

<center>图5-26 云开发镜像架构图</center>

容器镜像按照分层设计分为了4层：

1. **业务场景层**

   目前已经提供 cms qrn node & qzz 等多个场景。这些基本满足了我们公司常用的一些前端场景。那如果后续有想使用云开发的其他业务场景，也可以继续拓展。 

2. **webIDE层**

   嵌入了我们常用的 IDE 。像前面提到的vscode浏览器端的 code-server ，还有 jetbrains 系列，这一层后期可以根据开发同学的习惯使用进行拓展。

3. **语言环境层**

   目前支持 node.js ，后续如果想支持后端，也可以扩展 java 等等的语言支持能力。

4. **操作系统层**

   centos 和 ubuntu 。

分层的好处在于共享资源，比如说有很多的镜像，可以从 base 镜像构建而来。所以最终的镜像其实是由这四层基础镜像交叉组合成的一个镜像，便于多语言环境、多 IDE 类型、多业务场景以及多操作系统的扩展。

- [ ] #### **小结**

有了以上的方案以及实现，我们就可以进行代码编写、运行、debug 、push 等等一系列常规操作。



### 5.3.5	核心难点及解决方案

在使用云开发的过程中，我们也遇到了一些问题。比如：

1. **资源利用率低**

   创建的云环境太多，导致机器资源利用率并不高。对于公司成本节约就造成了一定的压力。

1. **没有打通开发的全流程**

   虽然说现在可以写代码了，但是没和其他平台做一个整合，开发流程还是很分散。

1. **无法实现个性化配置**

   我们想安装自己的插件和配置，在当前云环境内是可更改的，但一旦创建一个新的云环境是不生效的。那这种情况与本地开发相比，在开发体验上还不能完全本地化和可定制化。

于是针对于这些问题，我们也做了以下几个方面的工作去解决。

- [ ] #### 提高资源利用率-弹性伸缩，建立定期清理 webIDE 的机制


1. **超时回收**

   达到不在线的超时时间，比如4小时，即会释放 cpu、内存资源，但 IDE 的配置和已拉取的项目仍持久保存，下次可直接使用。

2. **彻底销毁**

   彻底销毁会将持久化的代码资源和配置都删除。


当 webIDE 容器达到不在线的过期销毁时间（现在设置的是7天），会先给用户发 IM 消息提示，提示中已给出 IDE 的各项信息，如确认不需要使用忽略消息即可，如仍需使用，按照提示操作即可保留 IDE 。

- [ ] #### 打通开发全流程


我们希望除了能实现在浏览器的开发，更希望通过浏览器和公司内其他产品流程结合起来，打通开发的全流程，真正建立研发效能的闭环。对于开发同学来说，下图5-27就是项目开发的完整链路。

![开发全流程](/medias/images/cloudbase/05.jpg)

<center>图5-27 使用云开发进行项目开发完整链路</center>

从 pmo 创建 -> 项目创建 -> 项目开发 -> 代码提交 -> 代码发布 ->  pmo 关闭。其中项目打开 -> 项目开发 -> 代码提交这三个阶段可以交由 webIDE 中操作。其他流程这里举两个例子：

1. 关联 pmo：可以在创建云开发环境关联绑定 pmo 号，而在关闭 pmo 的时候销毁这个环境。
2. 代码发布：也可以打通 CI、CD 。这块已经实现打通了发布平台，我们后面来详细阐述。

- [ ] #### 打通开发全流程-易扩展


对于团队需要的 WebIDE，满足日常的开发是必须的。所以易扩展，一定是排在第一位的。所以基于此，我们在插件中，实现了打通全流程中的一环—打通CI/CD 。下图5-28是我们给云环境定制化的一款插件。插件整体分为四个版块。

![开发全流程](/medias/images/cloudbase/06.png)

<center>图5-28 定制化插件</center>

1. **常用命令**

   这个就是便利了开发同学，不需要大家打开终端执行命令，而是点击即可自动打开终端执行。除了默认的常用命令，我们还推出了自定义按钮执行脚本的模式，让业务线同学定制化运行程序，参与云开发共建。

2. **发布版块**

   也就是我们打通 CI/CD 的一环。这部分会根据打开的文件目录自动识别发布类型，不需要开发同学去发布平台查找以及填写发布表单，即可一键关联发布平台。

3. **个性化定制**

   用户可定制安装属于自己的插件，设置后，在新创建的云环境上都是生效的。

4. **用户个性化配置**

   针对于开发体验这块，我们也是尽量保证在浏览器端大家也能有一个本地化的开发体验，那对于 vscode 来说，插件是必不可少的。我们把插件分为三种类型：

   - 通用插件。比如说中文语言包、深色主题等等

   - 面向业务场景的插件。比如说node场景的eslint语法检查插件、react等等的一些开发提示插件

   - 支持用户个性化的去定制配置和插件。前两类都是在镜像中内置好的，而这类型插件是在创建云环境的时候去动态设置的，能最大程度的接近本地化。

- [ ] #### **小结**

至此，优化完以上问题，webide 本身已经非常完美了，但是在开发体验上还是只能近似接近本地化，还是有一些缺失。比如说在一些快捷键的使用上，因为和 chrome 本身有一些冲突。如果在快捷键体验上要求比较高的同学，那我们也提供了另一种方式。



### 5.3.6	localIDE模式

也称为本地 ide 连接远端容器模式。如下图5-29。
<img src="/medias/images/cloudbase/07.png">

<center>图5-29 定制化插件</center>

可以看到，与先前我们看到的整体架构图相比，在 localOS 处由原来的浏览器通过 https 访问变成了由本地的 IDE 通过 ssh 通道连接到 server 端。

这个过程中，只传输代码、索引等数据，仅将计算匀给服务器，而渲染显示等还是依赖本地的 IDE 客户端，这种情况下，我们就能完全拥有本地化的开发体验。



### 5.3.7	总结规划

- [ ] #### 总结


下图5-30是我们一个核心团队反馈的各个阶段使用云开发前后的时间对比及效率提升。
![localIDE模式](/medias/images/cloudbase/08.jpg)

<center>图5-30 云开发提效数据</center>

整体而言，对于首次启动并开发的项目来说，云开发效率可以提升80%，对于日常开发、查问题这种业务场景，效率能提升60%。可以看出，在云开发结合打通全流程这一块，对于效率的提升起到了较大的作用。

云开发是我们运用云原生理念和技术在开发提效方向的探索，之前可能我们更多的关注于测试运维阶段，因为这些阶段的复杂度相对来说较高，因此更需要自动化、工具化，然而当基本建设完成之后我们再次盘点完整链路和深入挖掘痛点，开发同学这个庞大的用户群里的效率和质量应该是我们关注的重点。

- [ ] #### 规划


对于云开发未来的规划，主要分为三个方向。

1. **横向扩展**

   适配公司内更多的业务场景

1. **继续优化体验、丰富功能**

   - 比如提升不同场景环境管理功能，让该业务场景的管理员能自己改变镜像中设定的的某些依赖。
   - 创建独立的云开发管理平台。
   - 稳定性运维。

1. **推广工作**

   云开发模式上线后，一些业务已经在上面开发。不过更多的开发同学还是持有保守态度，后续会继续提升开发体验，让更多同学接受这种新型的开发模式。











# -----------------

# Part4	质量保障



在云原生的大背景下，公司业务需求的演进以及业务系统架构的迭代速度越来越快，我们在保证交付速度的同时也需要保证质量，因此对我们的质量保障提出了更高的要求。在测试阶段，无论是开发自测阶段还是提测后 QA 测试阶段，对于稳定环境的诉求都越来越高，环境问题对于测试进度的影响越来越严重，进而导致需求延期，业务迭代受到影响。同时，业务的复杂性也给测试环节带来了很多不确定性，人力成本的消耗大不说，测试结果的准确性也难以保证，如何通过更自动化更智能的手段，同时配合代码覆盖率等相关工具，保证测试的可靠性，在业务快速迭代的同时，让业务系统在线上运行的更加平稳安全，也是测试环境面临的一大问题。

本章节我们会对测试质量保障过程当中的一些难点痛点，介绍一下去哪儿网做出的变革和改造。











# 第六章	测试环境治理



## 6.1	测试环境管理平台

### 6.1.1	背景

公司业务的迅猛发展离不开项目的快速迭代，为确保项目有序快速的迭代上线，需要 PM 、DEV 和 QA 在测试阶段不断地优化、验证和调试产品的流程、代码，直至满足上线标准，最终发布上线。在整个迭代过程中，稳定的测试环境为项目顺利交付提供了最基础的保障。

在去哪儿我们采用微服务架构，因此对于一个需求的变更可能会涉及很多应用、DB 等，即使涉及几个，但是我们在验证的过程中也需要搭建一套完整的业务环境保证整个业务逻辑的完全回归；再次业务迭代非常频繁。因此我们常常会面临并行开发测试的问题，而一个需求的测试必须保证该业务流程的完成覆盖，因此对于某一块业务存在同时搭建多套环境的需求，基于以上两点我们需要有快速搭建环境的能力，但是现实却非常残酷，环境成了项目 delay 、测试覆盖不全的主要因素，究其原因主要有以下两点：

1. **环境搭建成本高**

   需要对业务流程比较了解，如应用之间的调用关系，其次需要了解每个应用的服务依赖、启动方式、其他依赖等，通常搭建一套环境至少需要一整天的时间，如果需求并行量小的话大家还能接受，但是经常的需求并行环境抢占就成了问题，即使多建几套环境，但是由于链路比较长，当没有需求的时候，环境资源的占用也是一个不小的浪费。

2. **环境维护成本高**

   搭建固定环境，就需要及时的对环境中的应用版本进行更新，应用的依赖变更、组件升级等也需要及时更新，一旦更新不及时很可能导致测试过程千奇百怪的问题，严重拉长了开发测试的周期，影响交付的速度。而且环境的维护对人员的技术和经验背景要求非常高，一旦人员流动，环境很可能就不可用了。

基于以上问题，我们开始搭建内部的环境治理平台，以提升一线工程师工作效率为最大目标，以环境搭建为切入点，集系统编排、资源分配、环境创建、应用部署、并行测试和资源管理等功能于一身，历时6年时间，主要经历了3个阶段：

1. **支持环境定义并自动化构建**

   主要减少人工搭建/维护环境的成本，通过环境模版快速创建一套完整的测试环境；同时通过环境网络/配置动态隔离，达到多套环境并行存在，多个需求并行测试的目标。

2. **测试环境支持动态路由**

   主要解决多套环境引入的资源成本膨胀及维护多套环境成本据增的问题。通过 SoftRouter 机制减少资源占用及日常维护成本，并增加环境巡检机制，保证环境的高可用性。

3. **支持本地开发联调**

   通过 IDE 插件将本地环境跟测试环境联通，提升开发自测的效率。经过几年的建设，当前的平台架构如下图6-1。

<img src="/medias/images/deployment_testing/framework.png">

<center>图6-1 当前平台架构</center>

平台主要包括四个部分：

1. **业务层** 

   主要提供环境管理/模版管理/服务管理/业务线管理/计划管理/数据分析等功能

2. **编排层** 

   主要包含环境构建任务编排与执行/服务间依赖关系收集。

3. **适配层** 

   主要提供对环境所依赖的网络/存储/资源平台的适配

4. **基础服务** 

   主要包含资源平台 / SaltStack / 七层负载均衡/发布平台/存储相关操作平台

以下我们从演进过程来介绍我们整体的环境治理方案。



### 6.1.2	阶段一：自动化环境交付

第一阶段我们首先要解决的是人工便自动的过程，也就是整个环境治理体系过程，所以我们首先需要对环境进行清洗的定义，其他我们还要明确最终的交付形态，以下从这两方面分别介绍。

#### 6.1.2.1	环境定义和组成

由背景部分介绍，我们的环境需要能够支持某一块业务的测试验证，因此一个环境通常是包含多个应用及其依赖（ DB ，中间件）组合，同时为了保证能够提供多套测试环境，需要进行网络隔离，因此我们的一个环境包含如下图6-2中5个部分。

<img src="/medias/images/deployment_testing/assembly.png">

<center>图6-2 环境构成</center>

1. **AppCode**

   指代一个服务的唯一标识及该服务的基本信息/发布配置/依赖配置/运行时配置，主要开发语言涉及 Java / NodeJs / Python/Go

2. **数据存储**

   主要包含对于主流存储组件的支持，例如 Mysql / Redis 等

3. **中间件**

   主要包含对于主流中间件的支持，例如 ELK 组件/ RabbitMq/ ActiveMq 等，并提供灵活的扩展能力

4. **网络配置**

   主要包含 Openresty / Nginx / Dns 等服务

5. **环境变量**

   主要维护环境内部服务/中间件生成后产生的环境变量信息并支持用户自定义，用于环境内服务的互联互通及环境间服务的隔离

为了保证的快速交付，我们增加了模版功能，即用户可以先将一套环境定义为模块，当需要创建环境是从模版生成，这样避免了并行需求每次重复配置的成本浪费，也解决了个性需求配置修改对所有并行需求的影响问题。

- [ ] #### AppCode画像

我们最终的目的是交付一套可用的测试环境，同时实现环境的日常自动运维，因此需要知道应用的运行时等相关信息以便实现环境的快速交付，我们内部称之为AppCode的画像，主要包含三个部分：资源配置/发布配置/运行依赖配置，资源配置包含kvm/容器类型，以及系统版本/实例个数等配置，详见下图6-3。

![img_5.png](/medias/images/deployment_testing/appcode.png)

<center>图6-3 AppCode的画像</center>

- [ ] #### 数据存储

除应用外，为了保证测试环境的数据隔离，会将应用依赖的数据存储也作为环境的一部分，在去哪儿，我们主要使用的是 Mysql 和 Redis ，以 Mysql 存储为例，在测试环境建立基准库实例，实时同步线上 Mysql 集群只读节点数据至 beta 环境，同时提供基准库（逻辑概念）供用户拉取不同集群的库表信息，在环境创建时，从基准库实例 fork 出测试库实例，并分配 namespace ，供环境内服务使用，如图6-4所示。

![img_2.png](/medias/images/deployment_testing/mysql.png)

<center>图6-4 数据存储实例</center>

- [ ] #### 中间件

某些应用由于特殊的业务属性也会依赖一些中间件，比如 es,ng 等，因此我们也将其纳入环境管理；当前支持的中间件包含多种开源项目，同时支持自定义 SaltStack / Shell 相关脚本，满足日常扩展需要，这些中间件都会在环境创建过程自动创建，如图6-5。

![img_3.png](/medias/images/deployment_testing/middleware.png)

<center>图6-5 中间件自动创建</center>

- [ ] #### 网络配置

网络配置方面，支持域名网关自定义配置（底层为 Dns + Openresty ）,同时支持自定义 nginx ，满足日常扩展需要。

![domain (2)](/medias/images/deployment_testing/domain (2).png)

<center>图6-6 网络配置</center>

- [ ] #### 环境变量


环境变量主要是由环境内各种类型的服务生成的 kv 对，同时支持用户自定义环境变量，供 AppCode 类型服务进行引用。AppCode 类型服务在部署前会将部署目录下的配置文件以环境变量作为数据源进行替换，生成运行时配置，达到环境逻辑隔离效果，如图6-7所示。

![img_7.png](/medias/images/deployment_testing/env.png)



<center>图6-7 环境隔离实例</center>

#### 6.1.2.2	环境交付

在交付环境时并非只需要将环境定义出来即可，我们更需要的是将部署好的可用的环境交付给开发测试同学，因此我们基于上述定义好的环境实现了自动构建，即将环境中的定义实力化，包括应用版本、中间件、数据库依赖性等，下图6-8是整体的构建流程：

<img src="/medias/images/deployment_testing/env_construct.png" >

<center>图6-8 自动构建流程</center>

整个环境的构建流程实际上是一个 DAG (有向无环图)，如下图6-9：

<img src="/medias/images/deployment_testing/dag.png">

<center>图6-9 环境构建流程</center>

整个环境构建流程实际上可以理解为一个分布式任务调度流程，Noah 环境在构建之前根据各个组件的依赖关系进行编排后生成一个 DAG (有向无环图)任务交由分布式任务调度系统 Ceres（公司自研），Ceres 服务启动构建，开始完成图上的所有任务的执行，执行完成后环境即被创建成功。



### 6.1.3	阶段二：动态路由机制

#### 6.1.3.1	背景分析

经过第一阶段的建设，环境管理平台已经覆盖大部分业务线，业务线同学使用业务线模版生成多套环境，环境间相互隔离，依赖多套环境达到并行开发及测试的目的。然而此种使用方式存在以下几个问题：

1. **环境资源成本占用过大**

   我们的环境不仅用于人工测试，也用于各种自动化工具，因为环境获取的成本降低，因此大家在使用量上也大幅提升，虽然我们做了及时销毁等策略，但是日常占用的成本依然非常巨大，不完全统计，内存暂用在10个 T 以上。

2. **多套环境日常维护人工成本比较大**

   日常的版本更新、组件更新等耗费成本，不及时更新等原因也为问题定位的效率等造成了影响。

3. **环境可靠性难以保障**

   我们日常并行使用的有上千套环境，大部分测试环境需要在项目期间持续使用，然而每天有近千条变更在线上环境产生。这些变更若不能及时同步到 beta 环境，那么测试质量显然⽆法得到有效保障，我们急需自动化的检查及自愈机制。

为了解决上述问题，我们调研了行业里相关的解决方案，引入了软路由机制（也就是行业里的甬道环境）、基准环境日常巡检和实时同步、测试环境基础资源巡检与自愈方案。

![img.png](/medias/images/deployment_testing/envs.png)

<center>图6-10  </center>

#### 6.1.3.2	方案设计：软路由机制

如下图6-11所示，多个环境并行测试情况下，黄色部分服务是待测试的服务，然而白色部分的服务都是用来支持整个链路测试的服务，测试链路越长，浪费的资源越多。

![img_1.png](/medias/images/deployment_testing/common.png)

<center> 图6-11 </center>

动态路由设计基于 QTrace (去哪儿网全链路追踪技术)，通过在各端( APP / PC /小程序等)取到 ID 标识，将流量进行染色，然后在 Http / Dubbo / Mq 各个通信中间件层面根据染色标识进行动态路由，这样我们只需要保证一套完整的环境也就是基准环境，其他日常测试环境都采用动态路由的方式，那么原来如果需要7套环境，每个环境10个应用，总计的70台机器变成软路由方式就变成了17台（假设每个环境只测试一个应用），这无疑大大提升了资源的利用率也降低了日常运维的成本，如下图6-12所示。

![img_2.png](/medias/images/deployment_testing/softrouter.png)

<center> 图6-12 动态路由设计 </center>

入口流量如何染色? 测试阶段入口总计包含如下图6-13三种入口，如果我们根据身份标识/设备标识，则可以区分用户的测试流量。

![img_3.png](/medias/images/deployment_testing/logo.png)

<center> 图6-13 测试阶段入口 </center>



接下来我们需要做的是如何将用户测试流量绑定到软路由环境上，从而对用户流量进行动态路由，引导流量进入逻辑测试完全链路上面。

![img_4.png](/medias/images/deployment_testing/qrcode.png)

<center> 图6-14 </center>

如上图6-14 所示，我们提供了扫码工具一键绑定软路由环境，绑定完成后，Noah 会生成路由标识信息，并将路由标识信息推送给 Openresty / Nginx 七层负载均衡组件。当客户端/浏览器/小程序发起测试时，则会经过七层负载均衡组件，负载均衡组件根据设备标识/路由标识映射关系，将路由标识放入 http header中，完成流量染色。

![img_5.png](/medias/images/deployment_testing/draw.png)

<center> 图6-15 </center>

我们是如何做到动态路由的? 以 Http 为例（公司内部主要使用 Openresty / Nginx 作为七层负载均衡组件）

![img_6.png](/medias/images/deployment_testing/domain_locations.png)

<center> 图6-16 </center>

如上图6-16所示，测试平台在软路由环境创建完成时，会针对基准环境内域名完成 location / upstream 组的添加，当测试流量到来的时候，Openresty / Nginx 会取 Header 里面的路由标识信息进行 location 匹配，若完成匹配则会打到对应的软路由环境的 Upstream 上面，若匹配失败则会将流量打到默认的基准环境 Upstream 上面。完成 http 流量的动态路由。

#### 6.1.3.3	方案设计：实时同步及日常巡检

由上述软路由方案介绍可知，日常的测试除了本次变更的应用外其他都会动态路由到基准环境，因此对基准环境的可靠性和新鲜度要求非常高，基准环境必须要及时的将所有应用的变更及时的同步下来，因此为了达到这两个目标我们增加了日常巡检方案和实时同步方案，会对环境进行定期巡检也会对线上变更进行实时监控，及时的将线上服务所产生的代码 / 配置 / 数据 等变更，进行同步，且同步期间，基准环境服务不中断。因此，特别选择三个方案进行选型。

- [ ] #### 基准环境采用双机滚动部署机制



![img.png](/medias/images/deployment_testing/standenv1.png)

<center> 图6-17 </center>

1. **优点：**

   - 双实例保障基准环境日常服务稳定性

   - 双机滚动升级，机器不需要被替换，日志信息可以进行保留

   - 改动成本低

2. **缺点：**

   - 双实例日常日志排查效率较低

   - 基准环境采用双机滚动部署，日常单实例提供服务

   

- [ ] #### 基准环境采用双机滚动部署，日常单实例提供服务

![img_1.png](/medias/images/deployment_testing/standenv2.png)

<center> 图6-18 </center>

1. **优点**

   - 双机滚动升级，机器不需要被替换，日志信息可以进行保留

   - 改动成本低

2. **缺点**

   - 发布期间，日志分散在多台实例，随着发布频次增加，问题会被放大

   - 日常稳定性较差

   

- [ ] #### 基准环境服务流量切换方案

![img_2.png](/medias/images/deployment_testing/standenv3.png)

<center> 图6-19 </center>

1. **优点**

   - 利用软路由环境机制验证日常变更是否可以合并进基准环境，间接提升链路稳定性

2. **缺点**

   - 历史日志随着机器被换新会丢失

   - 日常稳定性较差

   

- [ ] #### 为满足以下目标，我们最终选择方案一

1. **基准环境服务不可中断**
2. **提升软路由链路日常问题排查便捷性，以及日志查询便捷性**
3. **更低的开发成本**

![](/medias/images/deployment_testing/env_check.png)

<center> 图6-20 </center>

环境链路可靠性巡检通过收取线上（代码/配置/数据）变更，自动同步至 beta 基准环境，通过收取线上日志并进行筛选生成 case ，在 beta 基准环境进行回放，完成对核心链路的日常巡检。

#### 6.1.3.4	方案设计：基础资源巡检与自愈

为了高效利用资源，我们的测试环境机器都进行了超售，因此测试环境大量机器资源性能/稳定性等表现都不如线下环境，所以会经常出现资源不可用从而影响测试环境使用，因此为了提高测试环境稳定性，需要提升环境各类资源的健康度可观测性，同时针对异常场景期望做到自动恢复，以下是我们的详细方案：

- [ ] #### 环境检查处理流程

  收集环境内基础资源的各项指标并进行分析生成健康度报告，如图6-21所示。

![img_1.png](/medias/images/deployment_testing/env_base_check.png)

<center> 图6-21 </center>

- [ ] #### 环境自愈处理流程

接收异常事件并进行自动化恢复（以 KVM 磁盘告警处理为例），如图6-22所示。

![img_2.png](/medias/images/deployment_testing/env_base_recover.png)

<center> 图6-22 </center>

通过环境日常基础资源巡检，并搭配针对各种异常场景的自动化处理机制，提升整个环境基础资源的稳定性，从而提升日常业务测试的稳定性。




### 6.1.4	本地化开发测试

#### 6.1.4.1	背景分析

经过前两个阶段的建设我们已经可以快速的交付一套可用的环境而且能够保证环境的可用性和新鲜度，但是回归到我们的环境使用者，大部分用户是开发同学，开发同学在新开发功能时通常首先会在自己本机进行环境的准备，因此他们更希望直接复用本机环境，这样也节省了学习和配置成本，基于此我们展开了对于测试环境本地化开发的探索与实践。

#### 6.1.4.2	方案设计

![](/medias/images/deployment_testing/local_dev.png)

<center> 图6-23 本地化开发测试方案设计 </center>

所谓本地化，即将开发者本地 PC 融合为测试环境的一部分，让开发的目标服务在本地启动且达到与在测试环境中运行的同样效果，方案设计如图6-23。

#### 6.1.4.3	实现原理

去哪儿网的 Java Web 服务使用的服务容器是 Tomcat ，并使用 Maven 组件进行工程管理。因此，我们开发了一套 Maven 插件来实现对本地网络与服务的注册，并在编译期完成对环境变量的拉取与替换；同时，通过 Java Agent 技术在服务本地启动时完成本地网卡的识别与上报，并自动将测试环境中配置的 host 信息加载进本地的 JVM 中。

1. **本地网卡的识别**

   由于现如今大多数的 PC 机都配备了多块网卡，因此，对于使用到本地 ip 的相关绑定操作时(如测试环境域名的 ip 映射、Dubbo 服务使用 ip 注册)，依赖对于使用本地化网卡的正确识别。在本地化的初始阶段，Java Agent 会向 Noah 服务端发起一个请求，服务端会将请求的 ip 作为结果返回，从而获取到启动本地化用到的网卡 ip 。


2. **Host 信息自动加载**

   在某些场景下，测试环境会通过配置 Host 信息来应对代码中写死固定域名的情况，在本地化使用中，Host 信息同样需要加载到本地来完成正确的域名 ip映射。Java Agent 会在本地化启动时，向 Noah 服务端请求对应测试环境中的 Host 配置信息，并加载到本地 JVM 中，从而不需要用户手动对本地 Host 进行修改。


3. **环境变量的拉取与替换**

   诸如数据库 Namespace 用户名密码等配置信息，都是维护在不同 Profile 所对应的配置文件中，代码会在发布过程中识别对应的 Profile 并完成相关环境变量的替换。由于本地化的本质是将本机服务替换掉测试环境中的服务，因此，本地化启动时需要获取对应测试环境中的环境变量配置。本地化 Maven 插件通过在工程编译期请求 Noah 服务端对应测试环境的环境变量信息，以自动完成编译产物中相关配置文件的环境变量替换操作。


4. **服务注册与upstream替换**

   要完成完整的链路测试，测试环境内其他服务的流量也同样需要正确地转发到本地服务中。因此，本地服务需要注册到对应的测试环境中，并将域名的 upstream 进行替换。 Maven 插件在本地化启动阶段，通过向 Noah 服务端发起注册请求，来完成测试环境中对应服务的所有上游转发的替换。

至此，本地启动的服务就完成了对测试环境中对应服务的本地化实现。可以看到，无论是 Host 信息替换还是环境变量的替换，都是在即时产物（ JVM、编译产物）中进行的，因此，本地化对于工程代码是无侵入的。



### 6.1.5	总结和展望

#### 6.1.5.1	总结

以上即是我们环境管理上的三个阶段的探索，回顾这个历程，我们经历了许多波折，也踩过了很多坑，但是最终还是落地了可靠的环境管理方案，总结起来最大的原因是我们在着手之前明确的定义了我们要解决的主要问题，而且所有的设计方案都是基于目标定义，而不是为了搭建一个平台，所以前期的问题分析、目标定义是我们走向成功的关键一步。

#### 6.1.5.2	近期展望

目前我们的测试环境已经能为整个公司的日常测试提供保障，但是我们的环境还是限定于某一个业务领域，并且随着业务的复杂性提升，业务的边界可能越来越模糊，比如机酒打包、订单和售后打包等，基于此我们未来计划打造一套稳定的线下仿真环境( Beta 基准环境)。通过收集线上变更同步至 Beta 基准环境，并根据线上日志自动生成 Case （通过自动化手段提升 Case 覆盖度）并将 Case 在 Beta 基准/子环境进行回放，减少人工维护测试环境的成本，并提升测试效率。通过本地化机制打通本地 PC 与 beta 基准环境连通性，从而提升开发自测效率。

<img src="/medias/images/deployment_testing/future.png">

<center> 图6-24 测试环境管理概览 </center>



## 6.2	分布式编排调度平台

### 6.2.1	背景

在测试环境治理平台中，一键化部署整套可用的测试环境是其核心能力，这一功能的复杂度主要有这几点：

1. **环境庞大且资源种类多**

   整套测试环境中常包含多种资源，包括应用、数据库、中间件、域名、各种配置等。

2. **资源间存在依赖**

   环境中的资源构建不能直接并行，因为它们之间可能存在强依赖，比如 应用启动时所依赖的数据库必须就绪、创建域名时上游的服务实例必须健康，整个依赖关系会很复杂。

3. **底层平台不稳定**

   测试环境治理平台会对接很多更底层的服务，如 OpenStack、OpenResty 等。这些服务由其他服务管理，稳定性一般，因此平台侧需要能弥补这一可用性问题，提供重试、容错等机制，尽可能保障最终的成功。

这一业务其实需要的是基于工作流的编排、调度能力。

介绍下工作流，工作流定义了任务集合及它们之前的依赖关系，举个例子，目前有三个任务 1、2、3，其中 2 和 3 都需要等 1 执行完成后才开始，这就形成了强以来，可以通过一个 DAG （有向无环图）结构来表示，如下图6-25：

<img src="/medias/images/deployment_testing/ceres/1664071930885.png">

<center> 图6-25 工作流 </center>

原来有一套测试环境治理平台专用的调度服务，随着业务发展，该调度服务慢慢暴露出很多问题：

1. **性能瓶颈**

   任务状态非常多，导致调度器和执行器的交互次数过多、更新状态非常频繁；在任务量很大，伴随着很多冗余数据时，导致 MySQL 经常出现大事务、甚至 DB 死锁问题。

2. **无扩展性**

   当初的设计只是给环境构建用的，因此调度服务和使用方服务耦合严重，不可扩展。

3. **无容错**

   调度不稳定，经常出现工作流调度到一半，断了的情况。并且没有超时、重试等容错机制，导致工作流永不结束。

4. **问题难排查**

   调度服务和执行服务各自打到自己的日志中，要排查一个工作流问题，需要看好几个服务的日志，通过同一 trace 才能把请求连起来。

5. **难平台化**

   基于工作流的编排、调度能力其实是通用的，很多系统都需要这个能力，因此下沉到底层通用能力层是合适的。但由于之前的设计，导致当前服务平台化改造成本很高，不好给其他系统使用。

为了解决上述问题，决定重新实现一个通用的分布式编排调度平台，赋能各系统。



### 6.2.2	定位

新系统的核心能力是工作流编排调度，编排为主，调度为辅，因此定位是分布式编排调度平台，想清楚定位后就可以梳理下功能列表了。

- [ ] #### 功能性


以编排为起点，推演下编排相关功能：

1. **编排模块**

   接收任务及依赖关系，进行编排，产出一个完整的工作流。

2. **多种编排工具**

   形成任务及依赖关系，提交给编排模块。因为任务及依赖关系可能很复杂，所以需要提供工具方便用户，比如以页面拖拽方式，或者是 jar 包形式等。

3. **工作流管理**

   编排模块的产物是工作流，工作流会进行调度，是运行时状态资源，需要对其运行时状态进行管理，如启、停、取消等。该模块将编排与调度连接了起来。

4. **模板管理**

   为了让工作流可以执行多次，避免重复编排，故抽取出模板的概念。模板是静态的，由编排模块生成，基于模板可以生成工作流，实现一次编排多次执行。

以调度为起点，推演下调度相关功能：

1. **触发模块**

   调度的发起者是谁，以什么形式触发？因此需要一个触发模块来控制调度的触发。触发方式可以是 API、定时触发、间隔触发等。

2. **调度模块**

   负责工作流的调度，包括执行任务的先后、将任务下发到执行器等，涉及负载均衡、限流、超时控制、容错补偿等，核心模块之一。

3. **注册中心**

   调度器需要知道可调度的主机列表，因此需要一个注册中心，登记能被调度的实例信息，调度器从注册中心中取选项，按照策略完成调度。

4. **通信模块**

   调度器需要将任务下达给执行器，当调度器、执行器跨进程时，需要通信模块完成任务的下发。

5. **执行模块**

   任务被下发到具体实例后，实例会正真执行该任务，实例就是执行器。

6. **日志模块**

   执行器执行任务时会输出日志，为了日志能集中管理，因此执行器需要把日志进行上报，由日志模块进行统一管理。

7. **告警模块**

   工作流执行失败后一般需要通知用户，因此有告警模块。用户配置告警条件和策略，当满足条件时触发告警。

整个功能架构如下图6-26，红色是核心模块，黄色代表必不可少，绿色则为可选。

<img src="/medias/images/deployment_testing/ceres/1664011727032.png">

<center> 图6-26 功能架构图 </center>



- [ ] #### 非功能性


作为公共能力层平台，除了功能性需求外还需要具备以下非功能性目标。

1. **高性能**

   平台能支撑足够的并发工作流数；任务调度延迟应在几毫秒级。

2. **高可用**

   全组件高可用、无单点，支持横向扩展。

3. **可扩展**

   各模块均要一定扩展性以满足后续用户需求，例如 为了支持跨语言而使用新通信协议、新的告警方式等。

4. **易用性**

   接入和使用成本尽可能低，否则会影响后续推广。

   

### 6.2.3	可选产品简介

调研时开源的分布式任务编排调度框架不多，尤其是编排这块很少。这里简单介绍下当时调研的几个开源产品，并进行对比。

- [ ] #### Quartz


Quartz 是 Java 语言实现的轻量级调度框架，以 jar 包形式加入到工程中。调度器和执行器都在一个工程里，因此不需要通信模块，其核心组件就三个：

1. Schedule：调度器。
2. Trigger：触发器，支持四种类型：SimpleTrigger、CronTirgger、DateIntervalTrigger、NthIncludedDayTrigger。
3. Job：被调度的任务。

Quartz 的集群部署方案属于无主模式，多个节点互相独立且互不感知，通过 MySQL 行锁来保证任务不会重复被调度。

<img src="/medias/images/deployment_testing/ceres/1663853426942.png">

<center> 图6-27 Qunartz </center>

可以发现 Quartz 功能比较少，集群模式是无中心化的，没有独立的调度节点，调度与执行耦合在一个服务中。

- [ ] #### ElasticJob


[ElasticJob](https://shardingsphere.apache.org/elasticjob/current/cn/overview/) 是一个分布式调度解决方案，由 2 个相互独立的子项目 ElasticJob-Lite 和 ElasticJob-Cloud 组成，主要调研的是 ElasticJob-Lite，定位为**轻量级无中心化解决方案，使用 jar 形式提供分布式任务的协调服务。**

整体架构如下图6-28（图片来自官网）：

<img src="/medias/images/deployment_testing/ceres/1663915306896.png">

<center> 图6-28 ElasticJob </center>

模块比较多，主要看虚线部分的组件就可以，分别介绍下：

1. App1、App2 就是开发者开发的应用，引入 ElasticJob-Lite jar 后就具备分布式任务调度能力了
2. Elastic-Job-Lite 是以 jar 嵌入到应用程序中的，从图中可以看出特性比较多，自动注册、触发器、选主、任务分片、任务执行、失效转移、监听器
3. Registry：使用 Zookeeper 作为注册和配置中心，同时提供选主功能
4. Console：可选组件，是以服务形式运行的一个 web 控制台，包含静态资源增删改查，和动态资源的控制能力
5. Events、Logs：事件、日志持久化框架组件
6. ELK：一条日志解决方案，其实与 ElasticJob-Lite 无关，可作为 Logs 的实现方案

- [ ] #### XXL-Job


[XXL-Job](https://www.xuxueli.com/xxl-job/) 是一个分布式任务调度平台，其核心设计目标是开发迅速、学习简单、轻量级、易扩展，架构图如下（图片来自官网）：

<img src="/medias/images/deployment_testing/ceres/1663915986675.png" >

<center> 图6-29 XXL-Job </center>

该架构和之前两个框架差别较大，分为中心化的调度中心服务以及执行器服务。

1. 调度中心负责管理调度信息，按照调度配置发出调度请求，自身不承担业务代码。调度系统与任务解耦，提高了系统可用性和稳定性，同时调度系统性能不再受限于任务模块。支持可视化、简单且动态的管理调度信息，包括任务新建、更新、删除、GLUE 开发、任务报警等，所有上述操作都会实时生效，同时支持监控调度结果以及执行日志，支持执行器故障转移。

2. 执行器负责接收调度请求（如执行、终止）、执行任务逻辑。任务模块专注于任务执行相关操作，开发和维护更加简单。


可以发现 XXL-Job 是个功能完备的调度平台，比较符合平台目标。

- [ ] #### 特性对比


Quartz 只是轻量级调度框架而非平台，因此下图6-30这里主要对比 Elastic-Job 和 XXL-Job：

| 功能       | Elastic-Job                                       | XXL-Job                                           |
| ---------- | ------------------------------------------------- | ------------------------------------------------- |
| 触发方式   | Cron、API                                         | Cron、API、父子任务触发、固定间隔（实验中）       |
| 任务分片   | √                                                 | √                                                 |
| 任务类型   | SimpleJob、DataflowJob、脚本、HTTP API            | Java Bean、脚本（Shell、Python、NodeJS 等）       |
| 任务依赖   | ×                                                 | 父子任务（父任务执行完成后自动执行一次子任务）    |
| 路由策略   | ×                                                 | 策略丰富，常见的路由策略都支持                    |
| 工作流     | ×                                                 | ×（规划中 v2.4.0）                                |
| 通信实现   | 无需通信                                          | 基于 Netty 自研的 RPC，HTTP 协议                  |
| 注册中心   | ZooKeeper                                         | MySQL                                             |
| 高可用     | √                                                 | √                                                 |
| 中心化     | ×                                                 | √                                                 |
| 告警       | ×                                                 | √                                                 |
| 日志       | √                                                 | √                                                 |
| Web 控制台 | √（注册中心、事件、任务、执行器管理、执行纪录等） | √（报表、任务、调度日志、执行器管理、用户管理等） |

<center> 图6-30 方案对比 </center>



### 6.2.4	整体方案

对于当前需求，执行器通常是各业务系统服务，因此一定要将调度能力独立出来，作为调度中心运行，因此 ElasticJob 并不合适。这个类似 C/S 的架构有很多好处：

1. 解耦：架构上各组件职责清晰，调度和执行不耦合在一个服务中。
2. 稳定：调度中心的稳定性不受执行器的影响。
3. 易于跨语言：只需要实现各种语言的执行器客户端，不涉及调度层。

XXL-Job 不支持工作流，缺乏任务编排相关能力，也不满足要求，因此选择自研一套分布式编排调度平台。

平台整体架构如下图6-31：

<img src="/medias/images/deployment_testing/ceres/1664034961846.png">

<center> 图6-31 平台整体架构 </center>

从上往下依次介绍主要组件：

1. console：平台 web 控制台，提供各种资源管理能力，包括工作流模板、工作流、在线日志、历史记录等。
2. client：jar 包形式的工具类，提供编排工作流的 API。
3. 调度服务：核心模块，包含编排和调度相关能力，依赖 Redis 做缓存，并存储部分数据，MySQL 做主要存储组件。
4. 日志服务：统一收集和存储任务日志，提供日志查询等能力，底层存储使用 ES。
5. 执行节点：任务的执行器节点，一般是各业务服务，以 java 语言为例，通过引入平台提供的执行器 jar 包，实现执行方法，就可以自动接入平台，完成任务接收执行、结果上报等功能。



### 6.2.5	关键设计

- [ ] #### 编排IR


编排模块的工作是接收任务、任务间依赖关系，形成符合调度格式的工作流结构，本质是一种翻译，这和编译原理有异曲同工之妙，编译原理的核心就是将一门语言转换成另一门语言，因此在架构设计时参考了编译原理，形成符合当前业务的架构，如图6-32：

<img src="/medias/images/deployment_testing/ceres/1664014033649.png">

<center> 图6-32 编排IR </center>

1. **前端层**

   （不是指页面，而是编译原理中的前端）代表一系列不同形式的、生成 IR 的工具；IR 的含义是中间表达形式，是一种中间格式数据，json 格式；后端层就是编排模块，将 IR 转换成预期格式的工作流。

2. **IR 是稳定层，永远不变**

   无论前端工具的形式怎么变化，最终都是输出 IR；对于编排模块，输出的工作流格式可能变化，但输入的 IR 是不变的。这样前后端组件完全解耦，各自发展和变化。

关于 IR 格式的设计，对于一个工作流（DAG）其实只包含两种元素：点和边，图6-33这里给出一种避免数据冗余的 IR 格式供参考，代表了任务 1、2、3 串行的工作流：

```yaml
tasks:
  - id: 1  			# 任务的唯一标识
    ...  			# 其他属性
  - id: 2
  - id: 3
edges: 
  - from: 2			# 需要依赖其他任务的任务 id
    to: 1			# 被依赖的任务的 id
  - from: 3
    to: 2
```

<center> 图6-33 IR格式举例 </center>

- [ ] #### 编排 API

针对工作流编排的 API 设计了两种模式，分别是简单模式和复杂模式，简单模式易于理解但代码量可能比较多，复杂模式 API 比较多，在形成复杂的工作流时需要的代码会比简单模式少很多。

1. **简单模式**

   简单模式很好理解，把全部任务列出来，然后连线即可，因此只需两个方法：add（加点）、link（连接）。

   举个例子，现在要形成如下图6-33的工作流：

   <img src="/medias/images/deployment_testing/ceres/1664026638377.png" div align="center">

   <center> 图6-33 简单模式 </center>

   简单模式实现的伪代码如下图6-34：

   ```java
   Set<TaskParameter> allTasks = Sets.newHashSet(openFridge, putElephant, closeFridge);
   TaskGraph elephantWorkflow = TaskGraphFactory.create()
       .add(allTasks)
       .link(openFridge, putElephant)
       .link(putElephant, closeFridge)
       .buildTaskGraph();
   ```

   <center> 图6-34 简单模式伪代码 </center>

   代码也好理解，不做解释。但这种模式对于复杂的工作流，代码就需要很多，会感觉很啰嗦。

   <img src="/medias/images/deployment_testing/ceres/1664026703156.png">

   <center> 图6-35 复杂工作流 </center>

   要形成上图6-35所示，结构稍复杂的工作流，此时用简单模式需要 link 7 次写 14 个入参，不够简洁。

2. **复杂模式**

   复杂模式提供了一系列 API，主要思想是按顺序逐步构建出整个工作流，像大部分人画一个工作流的步骤一样，先画出头节点，然后是相关节点并进行连接，逐步形成整个工作流。

   仍以前面的复杂工作流为例，复杂模式下的伪代码如下图6-36：

   ```java
   TaskGraph workflow = TaskGraphFactory.create()
       .append(ready)
       .append(boilWater)
       .hook(ready, washCup)
       .append(makeTea)
       .hook(washCup, cleanDesk)
       .append(watchTV)
       .buildTaskGraph();
   ```

   <center> 图6-36 复杂模式伪代码 </center>

   此时只需调用 6 个 API、写 8 个入参，当工作流中连线很多时，复杂模式代码的简洁优势会比简单模式大得多。复杂模式完整的 API 如下图6-27：

| API                  | 图示                                                         |
| :------------------- | :----------------------------------------------------------- |
| append(Task)         | <img src="/medias/images/deployment_testing/ceres/1664027468036.png"> |
| append(TaskGraph)    | <img src="/medias/images/deployment_testing/ceres/1664027331148.png"> |
| hook(Task, Task)     | <img src="/medias/images/deployment_testing/ceres/1664027358348.png" > |
| hook(Task,TaskGraph) | <img src="/medias/images/deployment_testing/ceres/1664027386542.png" > |
| merge(TaskGraph)     | <img src="/medias/images/deployment_testing/ceres/1664027394765.png"> |
| link(Task, Task)     | <img src="/medias/images/deployment_testing/ceres/1664027532308.png"> |

<center> 图6-37 复杂模式完整API </center>



### 6.2.6	通信模块

要完成任务调度就少不了通信，通信的性能与可靠性决定了任务调度的速度和成功率，值得仔细考虑和设计。

- [ ] #### 抽象层


通信模块的具体实现方式有多种，为了可插拔及扩展性，需要根据具体业务做一层抽象，后续如果想更换通信协议只需要增加一个实现类，整体框架保持稳定。

对于调度中心下发任务到执行器业务，可以抽象出下图6-38的两个方法：

<img src="/medias/images/deployment_testing/ceres/1663904738329.png">

<center> 图6-38 方法 </center>

- [ ] #### 实现层

Java 生态通信模块的框架比较丰富，有不同程度的封装及特性，下图6-39介绍三种成熟的方案。

| 方案               | 概述                                                         | 优点                                                         | 缺点                                                         |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 基于 Netty 实现    | Netty 提供异步、事件驱动的网路应用程序框架，可以让开发者快速、简单的开发出实现了某种协议的网络应用<br /><img src="/medias/images/deployment_testing/ceres/1663912722137.png" div align="center"> | 1、高性能<br />2、通信层扩展性好                             | 1、成本高。<br />框架偏底层，因此编码多，如 server、client、listener、handler 等；<br />框架聚焦于通信，因此还需要实现异常处理、负载均衡等功能 |
| 基于 Akka  Toolkit | Akka Toolkit 是 JVM 平台上对 Actoer 模型的实现框架，提供了一系列工具包，这里主要关注这两个模块：<br />1、akka-cluster：集群组件，包括集群成员管理、弹性路由等<br />2、akka-remote(artery-remoting)：更底层的通讯组件，对于分布式应用推荐使用 akka-cluster<br />基于 Akka Toolkit 可以方便的实现通信层 | 1、实现简单，除了通信相关功能（传输、封包拆包）外，还提供了异常处理、失败重试、路由等功能<br />2、对 Actor 模型支持友好 | 1、上手难。不同于传统编程思维，开发者可能比较难转变到基于 Actor 的编程模型上 |
| MQ 模式            | 使用成熟的 MQ 进行通信，调度中心投递消息、执行器消费消息<br /><img src="/medias/images/deployment_testing/ceres/1663911902247.png"> | 1、完全解耦<br />2、实现非常简单，MQ 不仅解决了通信问题，还提供了负载均衡、异常处理、注册中心、健康检查等功能 | 1、强依赖 MQ，对 MQ 的可靠性、功能性等要求高<br />2、系统负载和 MQ 挂钩，MQ 可能成为系统瓶颈，需要对 MQ 非常熟悉 |

<center> 图6-39 方案 </center>

因为公司内部已经有一个非常强大、功能完善的 MQ 组件 [QMQ](https://github.com/qunarcorp/qmq)（已开源），为了更快开发出第一版，所以最终选择了 MQ 模式，MQ 使用 QMQ，这样省去很多开发工作，QMQ 把注册中心、健康检查、负载均衡、容错等机制都包揽了，调度基本全部完成。因为有抽象层，提前预留了扩展点，即使以后 QMQ 成为瓶颈，想换通信层实现也是比较方便的。

使用 MQ 后要考虑一个问题，MQ broker 端会有 Topic 级别的 QPS 限流，如果通信都是用一个 Topic 性能会大打折扣；如果每个任务/工作流使用一个 Topic，会导致 Topic 无限多，不方便管理且影响 MQ 性能。因此需要找一个业务维度进行 Topic 的公用，比如业务线，此时我们将业务线拼接到 Topic 中，这样 Topic 数量会是有限集，并且能限制业务线维度的并发量，一举两得。



### 6.2.7	存储设计

之前的调度服务在存储这块有明显性能问题，工作流在调度任务时，任务的状态很多，QPS 有放大效应，到 MySQL 这层容易扛不住。例如正在运行 100 个工作流，可能到 MySQL 这层，任务状态更新的 QPS 是 2000，此时 MySQL 容易成为性能瓶颈。

在新的设计中，根据数据的业务特点，决定使用 Redis + MySQL 配合做存储，对于状态类频繁变更的数据尽可能存在 Redis 里。先后设计了两版方案，这里主要看最终落地的方案。

- [ ] #### **存储方案**

核心思想是将很少变更的数据存 MySQL，频繁变更的数据存 Redis。通常存 Redis 后会损失一定的事务性，通过分析业务数据特点、设计存储结构和操作，能解决掉事务问题。

方案的具体细节如下：

1. 将工作流模板信息存 MySQL，工作流运行时（存在频繁更新状态）数据存 Redis。
2. 工作流运行完成后就不需要更新了，此时将数据转移到 MySQL 中。
3. 还会使用一套 Redis 充当缓存，将热点模板、最近访问的已完成工作流等数据进行缓存，提高访问性能。

对于事务可以分为这三种情况：

1. **只操作 MySQL**

   MySQL 天然具备事务性。

2. **只操作 Redis**

   通用的方法是利用 Lua 脚本保证原子性，且 Redis 不需要回滚机制，详情可阅读官方文档。但 Lua 脚本维护成本比较高容易乱，没有使用该方案，而是根据业务特点设计了一套：

   - 针对运行时工作流，使用 Set 结构存放当前正在运行的、全部工作流 id。

   - 针对每个工作流，使用一组 kv 存放工作流 id 和全部任务 id 集。


   - 再使用三个 kv 分别存放边信息、任务信息、上下文信息。


   - 在新增时先操作 B 和 C，最后操作 A，只有 A 中有了 id 才认为工作流保存成功。如果中间产生异常，Redis 里有了 B/C 脏数据，因为带 TTL 所以会自动过期。读的时候也是先读 A 中的 id，根据 id 查 B/C，因此读不到脏数据。


   - 在更新时，只存在更新 C 类型某一个 kv 的情况，不存在多次操作 Redis 的情况。


   - 在删除时，先删除 A 中 id，B/C 是否删除成功无所谓，和新增时类似

3. **既操作 MySQL，又操作 Redis**

   在当前场景中，只有一个业务需要同时操作 MySQL 和 Redis，就是在工作流运行完成后移动到 MySQL 的业务。只需要先操作 MySQL，数据存好后，Redis 删除数据的操作失败后也无所谓，因为有过期时间，到期自动删除。

<img src="/medias/images/deployment_testing/ceres/1664075898096.png">

<center> 图6-40 业务流程 </center>

因为整个业务不算复杂，所以可根据不同数据特征，进行精细化设计和控制。这不是一个通用的方案，如果是复杂业务不推荐如此精细，复杂度会很高。



### 6.2.8	日志系统

通过调度系统可以获取到工作流的运行情况，比如工作流中已完成的任务、当前正在运行的任务信息，不过也仅限于任务状态，如果要看任务执行的具体情况就没办法了。更麻烦的是当某个任务执行失败后，需要登录到执行器所在机器上查看日志，很不方便，预期是在平台上就能看到执行的错误信息。

这就需要引入日志系统，执行器在执行的方法中能获取到平台的 logger 对象，通过该 logger 记录执行日志，并自动尽可能实时地上报到统一的日志系统，平台通过日志系统就能查出工作流中每个任务的实时执行日志了，架构如下图6-41：

<img src="/medias/images/deployment_testing/ceres/1664029662030.png">

<center> 图6-41 架构图 </center>

1. worker 输出日志时，需要控制住日志量，避免海量日志的情况，否则会严重影响日志服务的性能和机器带宽。当日志量过大时进行丢弃，并将丢弃动作进行上报，让用户能知晓因数据量太大而被丢弃。

2. 日志中需要自动带上工作流、任务相关标识，否则日志服务无法知道是哪个任务的日志；还需要携带时间戳，因为消费可能存在延迟。

3. 日志先在 worker 本地缓存一会，当日志量积累到一个阈值，或等待时间过长时，将日志以消息的形式批量发出，充分让每条消息包含尽可能多的日志。

4. 通过 MQ 实现削峰，日志服务按照自己能力慢慢消费，避免打崩日志服务。

5. 日志服务消费消息，将日志存储到 ES 中。

6. 平台提供日志实时展示能力，如果每次看日志，日志服务都从 ES 查一遍，性能会很差，机器带宽扛不住。

   因此日志服务需要为查询生成本地缓存文件，该缓存能覆盖一定时间范围的日志。当后续再有查询时，如果在缓存所覆盖时间范围内则直接从缓存取，超过该范围的进行增量获取，这样能大大减少查询 ES 的次数和网络带宽。

   另一个优化方式是前端做一些处理，第一次查出所有日志，后续查询的起点时间范围是已展示日志中最新的时间。



### 6.2.9	落地效果

首先在测试环境治理平台中用新平台替换了原先的调度组件，提升明显：

1. **性能**

   一个 30 个组件的测试环境，整体环境构建时长缩短 4 分钟以上，提升超过 30%。

2. **成功率**

   新平台有很高容错，促使环境构建成功率有约 15% 提升，避免了原先工作流调度不结束的问题。

3. **排障成本**

   异常工作流排查成本下降超 50%。

后面接入了更多的系统，包括测试环境资源巡检、业务检查系统等，效果和口碑都很好。











# 第七章	自动化测试实践



## 7.1	背景

### 7.1.1	旅游业务概览

![image](/medias/images/autotest/1.1-1.png )

<center>图7-1 旅游业务版图</center>

去哪儿是典型的互联网旅游电商平台，那么它就有着旅游业业务复杂的特点。首先，它的版图比较大，涉及到的业务种类繁多。举几个例子，比如出行服务，有的用户会选择坐飞机，那么就涉及机票业务；有的会选择坐火车或者汽车，那么就涉及火车票汽车票的业务；还有选择自驾出行的同学，那么就涉及租车用车业务；再比如住宿服务，大部分同学会选择酒店，那么就涉及酒店业务；有一部分同学为了体验风土民情，可能会选择当地的客栈；还有一部分同学考虑到资金成本，尤其是一些学生，可能会选择民宿。

除了这些大类之外，在每个类别内部，业务也是突飞猛进的发展，我们以机票业务举例，以前我们可能只是卖票，现在更多的可能是关注用户体验，为用户提供完整的商品包的概念，定向的为用户提供一些服务，提高用户的购票流程体验。与此同时，还有不断探索的旅游新玩法，比如机酒服务，甚至门票服务。机票和酒店虽说是旅游行业中很重要的两个元素，但是在去哪儿内部，这是两个独立的业务部门，也被当做独立的业务线单独运营。

但是现在，我们逐渐让这两个业务产生交集，用户可以在去哪儿的app上感受一条龙服务，比如如果用户想来北京环球影城玩，那么使用去哪儿的产品可以一条龙买完机票订完酒店并搞到门票，体验方便简洁的购票流程，如下图7-2所示。



![image](/medias/images/autotest/1.1-3.png )

<center>图7-2 去哪儿app多样化服务</center>

那我们具体看下，在5年甚至10年间，我们的业务有哪些变化，我们以机票业务举例。

几年前用户买张机票那就是买张机票，很纯粹的需求，付钱买票，按时出行。但是现在，随着服务意识的增强和提高，在用户体验提升为目标的鞭策下，各平台提供了更多的贴合用户的服务，比如包装产品，自助值机，附加服务提供和附加商品搭售等等，可以说只有你想不到的没有平台没法提供的。用户在购买界面上能看到更多元化也多样化的产品形态。

我们对比一下以前和现在业务的关注点，几年前，用户更关注的是如下3个维度的基础信息，如图7-3所示：

1. 比如起降地、出发日期、仓位信息。
2. 航程信息、比如是直飞还是中转、航司信息，因为有的同学可能在固定的航司办理了会员，所以就会有积分和里程服务。
3. 航站楼信息，经停点信息，以及起降时间点，这和用户的行程安排息息相关。

![image](/medias/images/autotest/1.1-4.png )

<center>图7-3 去哪儿app部分基础信息</center>

那么现在的机票业务，如我们刚才提到的，更关注用户的体验，我们将机票当做一个商品来看待，形成定制化的打包服务。目前qunar已有的打包服务很多，这里我们只列举几个典型的套餐，让大家感受一下它的具体含义和价值体现。图7-4我们列举了四种典型的套餐，分别是商旅套餐、退改套餐、极速出票和机酒套餐。

![image](/medias/images/autotest/1.1-5.png )

<center>图7-4 去哪儿套餐产品举例</center>



1. **商旅套餐**

   是专门为商旅人士打造的套餐，他们的特点就是不差钱，但是对于整个旅行过程当中的体验很看重，所以对于这样的用户我们主要为他们提供了一些增值服务，提高旅行体验。

2. **退改套餐**

   主要是为旅程灵活度较大的用户准备的，他们有很大的倾向或者可能退票或者改签，退改套餐能够给这类用户最大的出行自由度，同时能够以较低的价格进行退票或者改期行为。

3. **极速出票套餐**

   是给那些临时起意出行的用户准备的，比如老板突然派你明天去出差，那么你可能希望你的机票很快出票完成，因为临近出行，能尽早拿到票就能避免一些意外情况发生。

4. **机酒套餐**

   是一个组合套餐，用户在qunar上购买机票的时候可以获得酒店的优惠券，可以在当前购买机票的页面直接用券订酒店，不止操作流程简单，也更加划算省钱。

### 7.1.2	系统和业务迭代

为了承载上述的业务逻辑，我们系统也变得越加的复杂，我们可以从图7-5对比看下我们业务系统的迭代变化。

![image](/medias/images/autotest/1.2-1.png )

<center>图7-5 业务系统迭代变化</center>



左图是几年前的一个近似大单体的服务，叫做 tts ，翻译过来说 the total solution ，顾名思义，所有的解决方案都在这里完成。再看右图，现在的业务系统是分层结构的，我们可以看到按照功能聚合划分之后，整个系统分为6层。当然 tts 的功能还是存在的，只是沉在了现在系统的最下面一层，提供基础报价信息，在它的上面，我们可以做各种花样玩法，包括刚才提到的业务上的演进，还包括一些技术上的优化，比如缓存，报价计算优化等等。

除了系统的维度，承载这复杂业务的后台数据也同样变得复杂起来，比如上面提到的套餐，为了承载套餐提到的功能，那么我们就需要这么些个数据节点去包含这些业务信息。所以用户看到的报价信息对应的背后的数据结构也愈加的复杂，如图7-6所示。

![image](/medias/images/autotest/1.2-2.png )

<center>图7-6 报价数据结构举例</center>

与此同时，公司的研发流程其实也越来越规范，比如说敏捷开发， OKR 管理，MVP 模型的使用，milestone 里程碑的使用等等，如图7-8所示，导致项目的交付速度加快，交付的周期大大缩短，做到了真正的持续交付。那么交付频率的提升带来的是测试频率的提升，按照之前人工的测试的回归过程，不仅要分析代码的改动范围，还要去决策回归的范围，然后得出回归的一个成本。同时，在那些人为决策之后，决定不需要回归的部分，其实经常发生漏测导致线上问题的状况。

![image](/medias/images/autotest/1.2-3.png )

![image](/medias/images/autotest/1.2-4.png )

<center>图7-8 研发流程举例</center>

真正进入到测试过程中之后其实问题更多，比如说你要测试的系统的依赖很繁杂，它的上下游也不稳定，数据时有时无，构造 case 也会消耗大量的精力，但是覆盖度却无法得以保证，如图7-9。

![image](/medias/images/autotest/1.2-5.png )

<center>图7-9 测试过程中的问题举例</center>

除了上下游依赖之外，测试对于流程的依赖程度也很高，举个例子，对于机票购票流程来说，它其实包含了从搜索到 booking 到生单再到支付，如果有后续退票改期的话，还需要走售后流程，整个流程环环相扣，很难只从其中的某一个环节单独测试，因为比如说booking依赖于搜索得到的数据，生单又依赖于booking 得到的数据，这里的数据不只是接口返回的数据，还有一些其他存储介质中暂存和缓存的数据，导致的结果就是比如说我想测试生单，那么势必要把搜索和 booking 先走一遍，如图7-10所示。

![image](/medias/images/autotest/1.2-6.png )

<center>图7-10 测试对于流程的依赖举例</center>



最后我们再通过数据，更直观的感受下，图7-11显示的是我们去哪儿某事业群一个季度的故障数据统计，漏测占比达到了66%。而公司线上高级别故障因为漏测导致的占比也在21%。可以看到这个数据是比较高的。

![image](/medias/images/autotest/1.2-7.png )

<center>图7-11 故障数据统计</center>

### 7.1.3	概述

经过分析我们可以看到，测试和发布的工作量比较大且重复。并且在 dev 负责质量的背景下，问题更加凸显，主要有3个痛点：回归型测试占比大、人工验证成本高和环境管理困难。图7-12所示可以大概分为五大类：

![image](/medias/images/autotest/1.3-1.png )

<center>图7-12 测试和发布的五类困难点</center>

1. **测试维度**

   我们如何能够将测试维度，也就是我们所谓的 checklist 沉淀下来，能够代代相传，同时随着业务变化能够不断更新。

2. **测试的有效用例**

   我们如何能够用较低的成本去生成实时性较高的测试用例，并尽可能的保证 case 的覆盖度。

3. **测试环境**

   我们如何能够降低测试环境的维护成本，尽可能保证测试环境的可用性和线上的一致性。

4. **测试分析**

   我们如何能够降低用户排查问题的难度，缩短排查问题的流程，能够让用户精准定位问题所在。

5. **持续集成**

   我们如何能够将自动化测试融入到线上的 CICD 流水线当中，用户只需要在几个关键步骤上感知自动化测试的执行和结果即可。

除了痛点之外，我们再看下 qunar 的测试工具现状。总结下来，可以有如下几种：手工测试，、qunit接口测试、querydiff接口测试，下面分别介绍一下。

1. **手工测试（以机票售卖场景为例）**

   - **编写checklist**

     5种航程类型*70个产品tag*乘机⼈*……

   - **构造用例**

     从订单数据等来源找到用例，并修改测试配置。

   - **部署环境**

   - **执行测试**

   - **分析结果**

   ![image](/medias/images/autotest/1.3-2.png )

   <center>图7-13 </center>

   整个过程枯燥且让人焦头烂额。

2. **qunit接口测试**

   - 维护用例

     把入参和断⾔维护到qunit中

   - 部署环境

   - 执行测试

     判断结果是否符合断言

   - 分析结果

3. **querydiff测试工具**

   - 线上流量录制

     按照⼀定的采样率录制参数和结果

   - 部署环境

   - 执行测试

     把前面录制的参数和结果回放

   - 分析结果

针对这些情况，有必要实现一个自动化平台来辅助 dev / qa 做到更好的交付结果，降低因为回归漏测导致的bug 甚至故障，从根本上解放人力，让 dev 和 qa 能够投身于更重要的事情上，而不是机械的去做一些重复性的枯燥的，完全能够被代码取代的工作。

根据上面的痛点可以知道，自动化平台至少需要满足4点要求：

1. checklist和case管理
2. 接口对比测试和结果分析
3. 环境管理
4. 应用元信息管理

至此，灭霸应运而生。

“ 灭霸 ” 这个名字的含义，按照《复仇者联盟》灭霸如图7-14带上原石手套之后的能力：随机消灭一半bug！

![image](/medias/images/autotest/1.3-3.png )

<center>图7-14 灭霸原型 </center>

在下面的篇幅当中我们逐一的阐述我们所做的探索，包括踩过的坑，有过的创新，总结的教训以及积累的经验等等。



## 7.2	case智选

![image](/medias/images/autotest/2-1.png )

<center>图7-15 灭霸系统结构 </center>

我们首先来看图7-15中灭霸的系统结构 。灭霸的整体架构比较复杂，这里只列出几个核心模块，包括灭霸本身的四大模块，以及和外部交互的模块。

最上层的黄色部分的模块是用户能够比较能直观看到的模块，即 UI 部分，它包含了配置管理部分，用户可以在这里配置需要进行测试的应用以及接口信息，还有测试管理部分，用户可以根据自己的需要去进行测试的调度。还有项目报告部分，用户可以看到自己执行的测试的结果信息和各个不同维度的分析，还有测试历史，用户可以看到某一个应用在某一段时间内执行的情况和趋势。

第二部分中间靠左边蓝色的部分是测试管理控制模块，这一部分对于用户是黑盒的，这里定义了测试执行流程中的各个步骤，从测试的诞生到测试的编排，从 case 的生成到应用的部署，再到 case 执行和结果 diff ，最后到测试报告的生成和结果分析展示。

第三部分是中间靠右边蓝色的部分，是数据控制模块，里面包含了 ELK 数据，用户文件上传的数据，API 对接接口数据等，这些数据都是用来生成 case 的。后面还有 mock 数据，主要是用来进行一种更特殊更复杂的测试场景，就是录制回放模式的测试，需要 mock 数据的支撑。

第四部分是绿色部分，包括了录制回放模块需要的各个组件，主要是以阿里开源的 jvm-sandbox-repeater 组件为核心，在上面进行定制和二次封装，包括了入口的录制回放，子调用的录制回放，入口的白名单和采样配置，子调用的黑白名单配置，以及最重要的各个不同组件的录制回放插件，比如 redis，db，rpc（ http/dubbo ）等等。

第五部分红色部分是去哪儿内部的其他组件，包括了 QSSO 单点登陆，邮件，IM 系统 qtalk ，ELK 日志数据，qtrace 全链路追踪系统，QMQ 消息中间件等等。

看完了整体系统架构，我们接下来看一下细节。首先是 case 智选，这里主要解决的是之前提到的两个痛点，测试维度的固化，和 case 生成问题。

### 7.2.1	case 生成方式和测试维度自动更新

- [ ] #### 测试维度

对于测试维度来说，比较重要的两个概念就是 checklist 和 case 了。对于 checklist 来说，诉求是能把各系统重要的点都固化下来，支持增删查改，能够方便的检索。

1. **多维度：**可灵活的自由组合，界面上可勾选。

   - 航程类型

   - 是否带儿童

   - 报价源：政策/旗舰店

   - 政策类型

   - ...

2. **初始化**：默认初始化部分维度叉乘的checklist点列表。

- [ ] #### Case 维度

对于 case 来说，要尽可能减少维护的成本，并且保证实时性/有效性。

1. **至少覆盖无线和www**
2. **覆盖足够多的产品形式**

- [ ] #### 业务系统维度

业务系统的测试维度是相对稳定，维度中的取值范围可能随业务迭代不断变化

1. 配置到⾃动化测试平台中
2. 多维度：航程类型、终端、乘机⼈…

- [ ] #### 项目checklist是笛卡尔积结果的子集

1. app端单程的低价特惠的儿童报价

![image](/medias/images/autotest/2.1-1.png )	

<center>图7-16 笛卡尔积 </center>

测试维度固化之后，通过笛卡尔积生成 checklist。

![2e0118d0-3b36-11ed-986a-a5f39c4740b8](/medias/images/autotest/2e0118d0-3b36-11ed-986a-a5f39c4740b8.png)

<center>图7-17 </center>

![4dbe66f0-3b36-11ed-986a-a5f39c4740b8](/medias/images/autotest/4dbe66f0-3b36-11ed-986a-a5f39c4740b8.png)

<center>图7-18</center>	

这份 checklist 就是一个全集，比如一个1成人0儿童，直飞航程的搜索条件就是这个 checklist 全集中的一条记录。我们将直飞航程换成往返航程，其他约束条件不变，那么它就是checklist全集中的另一条记录。case生成来源包括如下几种：

1. **根据测试维度生成用例**

2. **数据来源**
   - ELK 业务日志

   - gent 标准化记录请求入参打印日志

3. **时效性**

![image](/medias/images/autotest/2.1-5.png )	

<center>图7-19</center>	

有了 checklist 之后，我们就可以用它来生成 case 了。我们首先看一下数据来源，这里我们使用的是 ELK 数据作为原始数据，为什么用 ELK 呢？在之前提到痛点的时候，我们提到了，我们希望用相对较低的成本去获取case ，而我们的业务同学为了排查线上问题，已经在线上打印了接口的请求和响应数据，并通过 ELK 收集起来，那么我们也使用这部分数据，做到自动化测试和业务线排查问题共享数据，减少数据冗余带来的成本消耗。

然后我们这部分 ELK 数据和 checklist 一起去生成 case ，这里我们可以把 hecklist 想象成一个漏斗，ELK 数据流经这个漏斗，和 ecklist 中的各个维度进行匹配，匹配上之后就生成了属于该 checklist 某一条记录对应的case。除此之外还要另一种生成 case 方式，就是无规则采样，相比之下，这种方式更加的简单粗暴，因为它省略了 checklist 匹配的流程，这样会导致生成的 case 对于 checklist 集的覆盖度不可控，因为它完全取决于采样的结果。当然也不是说这种方式就一无是处，后面我们会提到使用这种case生成方式的场景。

最后是 case 的实效性，因为 ELK 里的数据是用来排查实时线上问题的，所以它的实时性得到了保证，我们复用了同一份数据，那么 case 的实效性也就得到了保障。

case 维护的一个重要问题是测试维度的更新问题。有的时候，业务迭代会新增某个维度的内容，就是说扩大了这个维度的取值范围，比如售卖终端又多了一种渠道，叫分销渠道。这种业务分支新增的情况如果都需要人为处理的话，会有一定的人力损耗。我们会在做 ES 和 checklist 匹配的过程当中，自动识别出 checklist 维度取值范围之外的值，同时补充到 checklist 已有的取值范围中去，实现维度自动扩充的一个效果。

![2.1-4](/medias/images/autotest/2.1-4.png)

<center>图7-20</center>		

![image](/medias/images/autotest/2.1-7.png )	

<center>图7-21 </center>		

反过来，如果一个业务维度已经很久没 case 覆盖，说明这个业务维度已经不存在了。比如售卖终端中的分销类型，某个代理商只和去哪儿签订了半年的合约，那么势必半年之后这个渠道就不再有报价了。那么评估一个业务维度的消亡，它不是一锤子买卖，这里我们引入了一个叫做周期覆盖度的概念，如果长时间一直没有覆盖的维度，我们才认为是是消亡的维度。同时，配合上面提到的自动补充逻辑，当一个维度周期的取值没有被覆盖，导致被删掉之后，过了一段时间又重新出现，那么也会重新被补充到 checklist 当中去。	

![image](/medias/images/autotest/2.1-8.png )

<center>图7-22 </center>			

![image](/medias/images/autotest/2.1-9.png )

<center>图7-23 </center>			

下面说下 case 的生成策略，也有几种，对应不同的场景和优先级。比如说，用户测试的时候，实时触发生成的 case，这部分它的时效性最高，优先级也最高，是生成 case 的首选。但是因为线上实时产生的日志量非常的大，尤其是搜索场景，所以日志的筛选是在一个时间范围内的，比如说过去的三个小时之内，这三个小时的场景覆盖可能会有遗漏，所以还有一个逻辑是定时补充，线上会有定时任务每隔N小时去补充case，这里的补充当然是定向补充，会选取那些没有覆盖的 checklist 进行补充。

那么经过线上长期的运行，已经证明，以上这两种策略可以获得足够高的覆盖了。不过有的系统因为有些特性，可能存在长尾的情况，比如一些冷门的产品，用户购买的频次就是很低，那么这些 case 一旦出现，我们就不能放过，因为后面很长一段时间都可能很难出现了，这种 case 我们要固化下来，在后面的测试中直接使用，进一步的提高覆盖度。

### 7.2.2	case 生成策略时机以及 case 有效性

生成的这些 case 也是需要维护的，这里主要指的是 case 时效性的检验，举个例子比如说机票搜索场景，用户在今天搜索了一个2月1号的航线，这个搜索条件被我们的系统加工成一个 case，那么在2月1号之后，我们测试的时候如果使用到了这条 case，它搜索的就是一个过去时间的航线，自然就过期了。

 其实对于那些通过 ELK 实时生成的 case，我们不需要太过担心，因为它的实时性能够得到保证。但是对于定时补充或者固化产生的那些 case，它们的新鲜度可能会比较低，尤其是固化下来的稀疏 case，可能是几天、几周甚至几个月前的数据，这种 case 是要被剔除的，因为它验证的代码流程是不符合预期的。

还有一些场景，比如某条航线的机票售完了，那么结果可能会提示用户该航线已售罄，但是我们关心的核心逻辑是报价的计算和交易流程，这个时候可以配置一些关心节点，比如报价信息节点，如果没有这个节点，我们就会认为这个 case 也不再有效。 

对于稀疏的 case 其实，它的产生非常不容易，如果只是因为日期过期而舍弃掉这个 case 的话，代价有些大，这个时候我们会使用类似于数据偏移的策略，修改它的触发日期，来起到续命的效果。这是一种在全链路压测领域中也经常采用的策略。

![image](/medias/images/autotest/2.2-1.png )

<center>图7-24 </center>				

### 7.2.3	其他特殊场景

最后我们补充说一个相对来说比较特殊的场景，叫做入参单一问题。有的场景入参条件包含的信息量非常有限，比如说我们根据订单号去查询订单详情，订单号本身它只是一串数字和字母的组合，不具备丰富的业务语义，所以我们无法根据订单号这个维度来保证业务维度覆盖的程度。但是其实订单本身是有业务语义的，只是这个维度在结果中而不是参数中，对于这种场景我们就需要将结果和入参绑定起来，通过结果来匹配checklist。

 还有一种更特殊的场景，它的业务维度既不在参数中也不在结果中，而是在中间过程中，比如说系统根据参数先去查询出一个中间结果，然后根据中间结果再去做其它操作，那么这种就需要特殊对待，同理，我们需要将这部分信息和入参绑定起来作为 case，完成 checklist 的匹配。

![image](/medias/images/autotest/2.3-1.png )	

<center>图7-25 </center>						



## 7.3	测试环境治理

case 的问题解决了，我们来到测试环境问题这个 topic 。 测试环境的管理，尤其是资源的消耗，其实一直是一个很令人头疼的问题，这里我们主要围绕两个方面去展开，首先会看下我们现阶段的测试执行是如何同环境挂钩的，然后会看下我们的测试平台如何借助公司内的新产物“软路由环境”，来帮助我们解决之前测试环境存在的问题。

最开始先说一下我们的这个测试执行流程，当代码发生变更或者 CICD 流水线中的某个环节触发了自动化测试，平台会得到通知进行环境的准备，代码的部署等等一系列事项。去哪儿内部的项目，是通过 PMO（也就是所说的 jira ）来管理的，如果只有一套测试环境的时候，同一个应用有多个 PMO 同时进行的时候，就涉及到环境的分时复用和资源抢占问题。

![image](/medias/images/autotest/3-1.png )

<center>图7-26 </center>		

所以环境池这个最简单的方案就被用来解决这个问题，测试平台会为我们的用户，预先准备好 N 套环境，这个 N 的取值大小，会根据该应用项目的热度来决定，也就是说，项目并行度高的应用，就会给它更大的环境池来解决排队问题。但是往往一个应用的测试和它的上下游脱不开干系，所以我们不止要为该应用准备环境，它的上下游都需要准备好，而且能够串联起来，对于机票酒店这种复杂业务场景，一整套环境会涉及上百个应用，那么环境池对于资源的损耗会大大提高。

![image](/medias/images/autotest/3-2.png )

<center>图7-27 </center>		

对于为了应对上述问题，“软路由”闪亮登场。我们首先介绍一下，什么是软路由。 图7-28中有三个环境。

第一个环境 base env 我们称之为基准环境，这里我们先假设这套环境中有6个应用。按照普通环境的玩法，只要有一个 PMO 中包含了这其中6个应用中的1个或多个，我们就需要单独去搭建以这6个应用为一组的测试环境，来做测试使用，N 个 PMO ，就需要 N 套环境。但是在软路由情况下，我们的测试环境其实不需要是这样六位一体的一整套环境，而是需要测哪个应用就单独搭建该应用对应的环境，而它的上下游会因为软路由的功能统一路由到 base 环境上。

举个例子 PMO1 的项目涉及应用 A 和 D ，我们就单独搭建一套 test env1 的环境，这个环境中只包含 A1 和D1 两个应用。同时 PMO2 涉及应用 A 和 D ，当然也可以是其他应用，那么我们就单独搭建另一套 test env2 环境，这个环境只包含 A2 和 D2 两个应用，test env2 与 test env1 这两个环境完全隔离。假设我们的业务调用链路就是图中的 A->B->C->D->E->F ，那么软路由的路由能力，能够使 MO1 的测试流量从入口路由到 test env1 的 A1，然后回到基准环境 B，再到基准环境的 C，再到 test env1 的 D1，然后继续回到基准环境的 E 和 F。

对于 test env2 同理，会走一条 A2->B->C->D2->E→F 的这样一个路径。

在这种架构下，对于环境中包含上百个应用的大环境，但是测试又只测几个应用的场景，软路由占用的资源肉眼可见的少，对于资源的节省是非常非常可观的。

![image](/medias/images/autotest/3-3.png )

<center>图7-28 软路由</center>		



## 7.4	录制回放落地

接下来说一下相对复杂的一个测试场景，录制回放。录制回放落地这个章节主要会从两个方面展开，首先是录制回放模式所应用的场景，以及实现的选型，然后是录制回放模式下排查问题困难这个痛点的一个细化的解决方案。

说到录制回放，我们先聊一下自动化测试的两个维度。第一个维度就是接口的 diff ，主要应对查询类型的测试场景，也就是“读”场景下的接口自动化 diff 测试。

第二个维度，就是所谓的录制回放，它应用的场景比较多，比如下游接口不稳定导致不幂等，同样的请求有时候能返回结果有时候却超时或者因为其他原因，没有办法返回正确结果；再比如下游没有数据，有些测试环境中，数据残缺不全，导致一些case测试的时候没有数据没有结果；还有的下游接口的调用可能会消耗资金，一些下游接口对接了一些外部公司的应用，有的可能是按次数收费，有的按流量收费，而这些应用有的没有对应的测试环境，走的都是同一套线上；还有一些写场景，比如下单等，同时该场景可能使用了仿真环境，而且隔离做的不够彻底，下游可能存在连接了某个应用的灰度或线上库，那么就可能对仿真或者线上库造成污染。

![image](/medias/images/autotest/4-1.png )

<center>图7-29</center>		

去哪儿录制回放的选型，其实经过了时间的推移也有过迭代。第一个版本大概是三四年前，那个时候还没有 jvm-sandbox-repeater 这个开源组件，只有 jvm-sandbox ，所以去哪儿基于 jvm-sandbox ，在上面自己定制的封装了录制和回放的能力，实现测试当中子调用和其他组件 mock 的功能。18年阿里新开源了 jvm-sandbox-repeater，就是在 jvm-sandbox 上面将录制回放的能力抽象实现了一把。去哪儿为了利用开源社区的红利，我们迅速的使用了 repeater 去替换掉了去哪儿r自己内部的实现，同时在上面做了一些小小的定制。

![image](/medias/images/autotest/4-2.png )

<center>图7-30</center>		

接下来我们看一下，整个 jvm-sandbox-repeater 的结构，它大体可以分为两大部分，一块是数据面，一块是控制面，这个分法其实并不是官方的分法，而是参考了同样为阿里 jvm-sandbox 系的混沌工程利器 chaosblade的分法，chaosblade 在最后一个章节也会提到。 这里控制面，主要是进行录制回放请求的下发和和路由的分配，数据面则包含了沙箱核心功能的实现，包括录制回放的各种插件，数据上报以及存储的数据中心等等。

![image](/medias/images/autotest/4-3.png )

<center>图7-31  jvm-sandbox-repeater的结构</center>		



接下来再说一下去哪儿在上面的一些定制处理，整个控制面由去哪儿自己的自动化测试平台来接管的，包括了中控，case 录制、回放以及筛选等逻辑。数据面，自研了一些新的插件，包括 dubbo 的异步回调，ning 的异步httpclient 等一些异步 rpc 场景，还包括去哪儿内部的一些 db 和 redis 的组件。

因为去哪儿的 dbclient 和 redis client 也是自研的，所以这部分组件的插件需要自己去搞。 那么数据面里面，case 存储管理这块，我们按功能划分拆分成了两个不同的类型，我们使用 ES 管理索引用来做 case 的筛选，使用 Hbase 去存储原始的数据，包括入口和所有子调用录制的信息，用来做 case 执行回放使用。ES 和 Hbase 的数据通过唯一Id实现关联绑定。

![image](/medias/images/autotest/4-4.png )

<center>图7-32</center>		

录制回放模式落地之后，在推广使用的过程中其实会发现，当我们的测试出现预期差异的时候，相对于普通模式，录制回放模式下，问题定位的难度大大增加，因为引入录制回放组件之后，本身整个测试的复杂度就上升了，因为有 mock 数据这个依赖，用户在排查问题的时候不像之前纯接口 diff 那样的简单纯粹，比如说这里的数据不同，产生的 diff ，它就不单单是结果的 diff ，还包括了子调用入参的 diff 。而 mock 的结果和代码逻辑的变动，可能会共同导致代码流程的不同，也就是方法调用链路不同，这都带来了问题排查的复杂度。

![image](/medias/images/autotest/4-5.png )

<center>图7-33</center>		

那么我们为了去解这最后一公里的问题，我们先看看之前一般有哪些排查问题的手段，总结一下，一般有三大类：

第一种是最简单的，就是通过异常栈，因为异常栈信息当中，包含了方法的调用路径，所以通过研究这个路径可以定位到异常位置，调用关系以及引发它的问题；

第二种是 debug ，这是开发最喜欢的一种方式，除了能够看到方法调用链路之外还能看到上下文信息，入参结果取值等等；

第三种是 arthas ，因为有些时候 debug 不能随心所欲的使用，比如在仿真环境甚至线上，所以借助 arthas将 debug 功能可视化。

![image](/medias/images/autotest/4-6.png )

<center>图7-34</center>		

以上这三种都是 JVM 中链路排查的一些手段。那么对于跨 JVM 的链路排查，公司也有 qtrace 这种全链路追踪系统，能够在更宏观的层面，也就是微服务层面，去看到一次请求在哪个应用会出现问题。 

那么我们抽象一下，我们的目标是为了在出问题的时候将 JVM 的内存方法调用链路去梳理出来，那么将上述宏观的这种应用维度降维成 JVM 的方法，在微服务内这个微观层面下，如果我们也能梳理出一个 JVM 方法链路调用拓扑图的话，我们将回放时基准环境的拓扑和测试环境的拓扑进行 diff ，就基本能够找到从哪个方法起，开始出现的问题，这比改造录制回放组件，在 diff 处去抛出一个异常，或者是用户 debug ，包括 arthas ，都更易用也更直观，大大简化了用户排查问题的路径，也可以降低时间和人力的成本。

![image](/medias/images/autotest/4-7.png )

<center>图7-35</center>		

因为一个 JVM 内链路上涉及到的方法可能有很多，除了我们自己写的应用代码之外，还包括了 jdk 的，还有一些公共包比如 guava ， Apache 的公共组件，还有一些公司技术部封装的内部组件，如果把所有的方法链路都串起来，整个链路拓扑可能会非常大，信息量也非常大。而其实真正涉及业务流程的大部分都是我们自己的写的那一部分代码，所以我们提供了一个配置功能，用户可以根据需要，按照包路径，类名，方法名等维度去配置，哪些链路是需要关心的，同时支持了黑白名单，在关心之外的调用链路不会被拉出来，这样可以进一步降低用户的排查成本。 

![image](/medias/images/autotest/4-8.png )

<center>图7-36 配置路径</center>		



## 7.5	其他探索和实践

之前的三个大章节说的都是自动化测试的一些本职工作，主要是为了业务去保驾护航。那么除了进行业务回归之外，它还能发挥什么其他的作用，在其他方面去为公司赋能？这里就到了脑洞环节了。那么整个2021年，我们结合公司内的一些其他工具和平台，进行了一些所谓的“杂交”实验，在某些方面产生了一些1+1>2的效果，这里就以三个例子来展开阐述，分别是“自动化测试与混沌工程”，“自动化测试与全链路压测”以及“自动化测试与基础组件升级”这三个方面。

### 7.5.1	结合混沌工程的实践

混沌工程是去哪儿在2021年投入很大精力去落地实践的一个领域。其中一个和自动化测试结合的比较好的案例就是“强弱依赖”演练。首先先介绍一下强弱依赖的概念，图中 A 依赖了B、C、D这三个服务和一个数据库。那么假设，我们的服务D挂了，A还能正常对外提供服务，那么D就是弱依赖，反过来B、C和数据库这其中任何一个挂了，A都无法返回正常结果，那么B、C和数据库就是强依赖。

![image](/medias/images/autotest/5.1-1.png )

<center>图7-37 强弱依赖</center>		

举个实际的例子，如图7-38，我们将上述的 A、B、C、D 和数据映射到去哪儿机票的预定场景，当然这里是一个简化版的预定流程，主要是为了解释清楚这个问题。 A对应机票预定的入口，B对应风控校验系统，数据库对应库存，C对应生单数据处理，D对应大数据平台的分析服务，那么这里看下来，只有 D 是弱依赖，因为即使我们的订单数据无法进入大数据平台，影响的也只是去哪儿自身的一个数据分析和统计，对于 C 端的用户来说，机票还是可以正常购买的，反过来，无论是风控，库存还是生单处理，哪一个环节有问题，用户都会受影响。 

上面的例子比较简单，我们人工就能比较清晰的去判断这个例子中的强弱依赖关系。但是真实业务中的依赖关系多而且复杂，一个生单的流程可能涉及上百个应用。 那么强弱依赖演练就是为了发现这些繁杂的应用中，哪些是强依赖，哪些是弱依赖，进而推动业务线去根据数据做系统优化，当然我们最终的目的都是减少强依赖，然后对剩下的为数不多的强依赖进行HA处理。

![image](/medias/images/autotest/5.1-2.png )

<center>图7-38 机票预订场景下的强弱依赖</center>		

下面我们看一下图中7-39整个强弱依赖演练的流程，先看下和自动化测试平台结合之前的一个旧流程： 

在准备阶段，先启动强弱依赖治理，选择对应的应用，然后进行依赖分析，先手动标记，这整个过程主要是人去介入，我们会去借助 qtrace 产生的拓扑图，加上对业务的一个了解程度，完成手工的操作。

 接下来在演练阶段，根据标记结果执行强弱依赖演练，也就是根据这些标记的强弱依赖去注入一些常见的故障和错误场景，然后在这些场景下去观测整个演练的结果——大多数是接口的返回值和业务的流程，来生成强弱依赖报告。

最后，我们可以根据出来的报告，去修正之前在准备阶段的手动标记的强弱依赖的内容。可以看到整个过程需要人工参与的部分还是比较多的，主要在依赖分析，手动比较，根据断言生成报告和依赖标记的修正。 那么在这里我们加入自动化测试平台的帮助，就可以让整个流程更加简单，也就是我们所谓的强弱依赖自动演练和标注。那具体怎么做的？

![image](/medias/images/autotest/5.1-3.png )

<center>图7-39 强弱依赖演练的旧流程</center>	

这里，我们不需要进行依赖分析和手动标记，我们直接使用自动化测试作为流量的入口，将同样的请求分别打到一个正常的基准环境和一个准备好强弱依赖演练的一个测试环境，然后我们借助自动化测试的断言能力对接口的返回结果直接进行智能分析，当断言不通过的时候，直接将本次演练的下游接口标记为强依赖，反之标记为弱依赖。这个环节不单大大提高了自动化的程度，减少了人为交互，提速效果明显，同时，通过自动化测试平台本身case 覆盖的保证，让整个比较过程的范围面更广，结果也更加可信了。流程详见图7-40所示。

![image](/medias/images/autotest/5.1-4.png )

<center>图7-40 当前去哪儿的强弱依赖演练流程</center>	

### 7.5.2 与全链路压测的结合

这里的全链路压测主要借助了自动化测试平台的 case 获取能力，前面抖了个包袱，在说 case 生成的时候，提到了 checklist 过滤和线上随机采样的对比，那么在压测场景下，我们看重的更多是量，而不是业务逻辑覆盖，所以这里我们就可以使用线上随机采样甚至全采，来获得 case，用于压测这种需要大量 case 来探测系统性能的这样一个场景。 同时在一些复杂场景下的 mock 数据功能使用了刚才我们提到的自动化测试平台的录制回放能力，就是图7-41中所有的这些非灰色标注的部分。

因为整个链路在压测的过程中有时候有些服务是不能被压的，比如一些外部依赖，像机票行业，涉及到航司或者代理商，除非你能够说服对方和你一起进行压测，否则这部分数据你只能 mock 。那么 mock 数据的准备，当然还是用录制回放的模式会更节省人力，虽然复杂度会提升。

尤其像我们公司内部，压测的影子库相关功能的 agent 和录制回放的 agent 是两个不同的 agent ，所以还要去解决 agent 插桩兼容性的问题，比如同一个字节码修改的切点，如果因为提前返回，导致后生效切点无法工作，那么功能就不能正常完成了。我们在这方面也是做了很多工作，这里就不详细去展开说了，后面在全链路压测topic 中会展开去讲。



![image](/medias/images/autotest/5.2-1.png )

<center>图7-41 全链路压测流程</center>	

### 7.5.3	基建变更和升级

然后再说一下基础组件自动升级这块。基础组件和中间件的升级其实一直是各大公司比较头疼的话题，这也直接体现了技术部门和业务部门之间的最大矛盾。业务部门更关心的是业务架构是否合理、是否满足业务需求、技术部的一些迭代：比如 dubbo 跟随社区升级、比如一些 apache 或者 guava 包的版本、比如 jdk 的升级，业务其实不太care，反而如果一旦出现了不兼容的升级，可能会增加业务线开发的成本。比如因为 API 的改动需要改代码，再比如maven版本管理的变化导致依赖冲突等等。更严重的，如果这些升级因为一些场景没有考虑的很周到，导致业务线升级之后发生了线上的故障，这会大大降低业务线对于公共组件升级的热情和信心，让技术部的同学的工作无法去开展和推进，OKR 也没法完成。

同时对于技术部同学自己来说，怎么样才算回归完全，我可能即使通宵达旦累死累活的，结果好像也并是很让人满意，反而被业务同学去吐槽。 在这样一个背景下，去借助自动化测试平台的能力，做基础组件自动升级这样一个方案走上了舞台。

因为自动化测试的覆盖度和成熟度相对来说都比较高，执行效率也比较让人满意，所以技术部的同学只需要将新的公共版本的分支进行自动化回归，断言不过的时候再看下问题所在。一个晚上不需要熬夜即可轻轻松松覆盖所有应用，第二天早上来看测试结果，根据结果做修正，没有几次回归解决不了的，如果有，那么再来几次。最终，达成的效果，就是关系已经很恶劣的技术部和业务线的同学皆大欢喜，大家各自所做的事情能够互相借力，事情做成了，信赖也建立了，是一个 happy ending 。

然后有了基础组件升级的这样一个成功的经验之后，我们将这一套流程继续复刻在其他类似的场景上，包括在2021年去哪儿内部重点落地的从 KVM 迁移容器的项目，也进行了使用。容器的迁移涉及到的应用范围很大，环境上既包含了测试环境，还包含线上环境。如果单纯靠人去搞，会投入很大的成本。同时，容器成熟之后，后续的一些镜像相关的更新和升级也同理，可以用上这整个回归流程。 这就是我们自动化测试平台和基建升级所做的一个有比较好效果的结合案例。



![image](/medias/images/autotest/5.3-1.png )

<center>图7-42 基建变更和升级</center>	

## 7.6	总结展望

最后总结和展望一下，整个案例我们介绍了去哪儿自动化测试平台在落地实践过程中的一些创新、亮点，以及趟过的坑和积累的经验教训。最后还囊括了一些和其他工具的结合，包括和质量保证息息相关的混沌工程和全链路压测，让整个自动化测试不再局限于它自己的圈子，不再着眼于自己的一亩三分地。因为自动化测试就那么些东西，如果我们固步自封的话，这个测试平台最后只会变成一潭死水。

包括以后的展望，我们的方向也是在不断优化测试平台本身的同时，去开更大的脑洞，和前沿新鲜技术多去结合，擦出更多的火花，产生化学效果，衍生出更多的这种1+1>2的案例。 











# 第八章	测试覆盖率



## 8.1	背景与收益

伴随业务的不断完善，企业已经跳过快速扩张的阶段，更加重视服务质量与用户体验，因此我们做了很多质量建设，包括接口自动化、UI测试、单元测试、静态代码扫描等，同时也落地了质量门禁与发布集成，进行检查拦截，但是即便如此仍然故障频发。

对故障原因进行分析，我们发现近两年有超过三分之一是因为漏测造成，这无疑会被大家质疑我们质量建设的效果，总结分析根本原因是质量保障要保证验证覆盖度，而像去哪儿这样体量的公司经历了长久的发展演变，业务非常复杂，因此我们做质量建设的时候通常是优先保障核心应用链路的，这就导致虽然辛苦建设还是会有遗漏的问题。

基于此我们在想历史包袱治理的成本太高，而且在业务快速发展的大环境下我们也不可能完全停滞进行全量的治理，因此我们最主要的还是控制新增变更的质量，前边已经说到手段已经建设完成，现在要做的就是保障手段在新增变更的执行，这时候测试覆盖率就变成了一种非常好的手段，虽然覆盖到了不一定有问题，但是没覆盖到更容易出问题。

因此，从21年开始我们进行了测试覆盖率的建设，当前覆盖率平台已经被我们纳入提测上线前必须的质量保障步骤，同时也被广泛应用在更多的场景：如接口自动化 case 补充的参考、代码 review 的范围参考等，成为开发过程质量建设保障不可缺少的重要组成服务，而且统计数据，当前发布故障中，代码漏测得到了大幅的改善，占比由37%降低至8.3%。

## 8.2	实践框架

覆盖率的建设经历了两个阶段，开发落地与降本提效。

- [ ] #### **开发落地**

包括了调研、开发、集成 CICD 与落体过程，全面落地需要考虑落地对用户无干扰，平台性能，存储磁盘问题，更需要考虑覆盖率agent对业应用是否产生哪些影响、踩过的坑以及如何优雅的解决。

截止2021年初，覆盖率平台也作为基础能力覆盖所有业务线，默认全部应用开启覆盖率，应用接入数占比98.96%。

- [ ] #### **降本提效**

推广落地后面要做的是用户体验提升，提升覆盖平台数据的精准度与效率成为了首要任务。经过大半年的优化，到2021年底，发布质检平台中，覆盖率拦截数值比例由30%左右提升至70+%。

以下我们从这两个阶段分别介绍。

### 8.2.1	阶段1—实现落地

开发阶段我们结合了公司内部的技术栈以及当前市面上解决方案，避免重复造轮子最终采用开源工具作为基础，集成到 CICD 中，并实现代码分支覆盖数据合并，与多维度查询功能。落地过程也出现多种特殊场景，如启动失败，业务报错等一系列问题。面对复杂的业务应用，我们踩过了很多坑，通过升级改造得到了很好的解决，以下对实现过程进行详细的介绍。

#### 8.2.1.1	覆盖率简介

- [ ] **覆盖率的基本原理如下：**

1. Java 文件编译后的文件由一条条指令组成。

2. 覆盖率工具在每一条指令后插入一个标识，常见boolean类型，默认 false。

3. 请求进入系统后，该标识会设置为 true 。

4. 将标识数据保存到下来，可以保存本地文件或 TCP DUMP 方式获取。

5. 结合class文件与插装标识生成覆盖率数据；再结合源码可以生成可观察的报告文件。


​	插桩指令示意如图8-1：

![image](/medias/images/coverage/code-Instruction.png )

<center>图8-1 插桩指令示意图</center>

#### 8.2.1.2	工具选型

公司技术栈以 java 为主，市场上主要的覆盖率工具有：Emma 、Jacoco 、Cobertura。功能对比如下图8-2所示。

| 类别       | Jacoco                       | Emma                         | Cobertura                             |
| ---------- | ---------------------------- | ---------------------------- | ------------------------------------- |
| 原理       | 使用ASM修改字节码            | 可以修改Jar、Class文件字节码 | 基于Jcoverage。基于ASM框架对class插装 |
| 覆盖颗粒度 | 方法、类、行、分支、指令、圈 | 行、块、方法、类             | 行、分支                              |
| 插装模式   | on-the-fly 与 offline        | on-the-fly 与 offline        | offline                               |
| 缺点       |                              | 不支持JDK8，不再运维         | 关闭服务器才可以获取覆盖率报告        |
| 性能       | 快                           | 较快                         | 较快                                  |

<center>图8-2 插桩指令示意图</center>

- [ ] #### 两种插桩模式

1. **onthefly**

   javaahent 参数指定特定的jar文件，启动 instrumentation 的代理程序。

   - 更方便，不用提前进行字节码插桩，无需考虑 classpath 的设置。
   - 以下情况不适用：部署环境不支持 javaagent ，部署环境不允许设置 jvm 参数，动态修改字节码过程中和其它 agent 有冲突，无法自定义用户加载类。

2. **offline**

   在测试前先对文件进行插桩，然后生成插过桩的 class 或 jar 包。

结合我们的场景、最后我们选择 Jacoco 作为覆盖率收集工具，采用 java agent动态插装模式，降低插装对发布耗时的影响，如期地完成了开发与落地。

#### 8.2.1.3	平台建设

- [ ] #### **分支模式**

  由于分支模式对于我们覆盖率的实现方案有很大影响，因此我们先简单介绍下去哪儿的分支方案，我们主要采用分支开发，主干发布的方式，即 master 跟线上保持一致，当有新需求时就需要从 master 拉出来一条 feacher 分支，每次测试验证部署都会生成一个 btag ，测试验证是在 btag 上进行，最终上线前将分支的变更（所有 btag 内容）merge 到 master 上。

  ![image](/medias/images/coverage/dev-deploy.jpg )

  <center>图8-3 分支开发</center>

- [ ] #### **平台设计**

  当我们对一个需求进行提测或者上线时，我们需要观测的是本次分支所有变更的覆盖率总和，因此需要将支持多次测试（所有 btag ）的覆盖率数据汇总，因此整体的架构流程图如下：


![image](/medias/images/coverage/qcoverage.png )

<center>图8-4 平台设计</center>

这里我们需要说明的是由覆盖率原理的介绍可以得知，生成报告需要三份数据。

1. class 数据在发布平台获取 war 包解压；触发时机：收到发布成功的 mq 消息开始获取，一次性。
2. 源码数据在 git 平台获取；触发时机：收到发布成功 mq，开始获取，一次性。
3. 覆盖率数据通过 ip 与端口 dump 存储；触发时机：用户查看数据时，实时触发，可多次获取，数据追加。

下面简单介绍部分功能的细节：

1. 业务迭代过程存在大量代码不易测试，业务开发过程只关心修改后的方法测试，因此获取 git 源码时，需要做 gitdiff，数据入库。

2. 代码覆盖率只关心项目中的源码，因此需要根据 pom 解析出源码模块，将第三方 jar 包去除。

3. 覆盖率报告支持全量报告与增量报告，因此 class 拉取后保存全量同时，再根据 diff 数据生成变更的 class 文件。

4. 容器或 KVM 销毁时，避免覆盖率数据丢失，需要主动获取数据。

5. Jacoco 源码需要拉取改造，支持 diff 数据以及报告展示。


#### 8.2.1.4	落地坑点

由于业务应用复杂，嵌入 agent 启动过程会出现各种问题，下面介绍两个大坑。

- [ ] **反射天坑**

1. **背景**

   - Jacoco覆盖率实现原理是字节码插装方式，在业务 Class 中插入 $jacocoData 字段、$jacocoInit方法。


   - 参考代码：org.jacoco.core.internal.instr.InstrSupport，如图8-5。


   ```java
/**
 * Name of the field that stores coverage information of a class.
 */
public static final String DATAFIELD_NAME = "$jacocoData";
/**
 * Name of the initialization method.
 */
public static final String INITMETHOD_NAME = "$jacocoInit";
   ```

   <center>图8-5</center>

2. **问题**

   业务线代码存在较多反射逻辑，误将覆盖率插装字段获取出来，出现应用启动失败以及对外接口错误造成故障等问题。

3. **解决方案**

   - 修改 jvm 反射类中的反射方法，在通过反射获取属性以及方法时，过滤 $jacoco 成员。

   - 反射获取 JDK 反射类 sun.reflect.Reflection ，通过 asm 字节码修改，达到修改反射方法的目的。

4. **注意点**

   - JDK11与JDK8中的反射类路径不一样，需要再次处理，JDK11中路径jdk.internal.reflect.Reflection#filter。

   - jacoco 自身也使用反射类，过滤时需要保障 jacoco 使用正常。

- [ ] **端口相关问题**

1. **端口占用**

   历史应用使用 KVM 部署，存在端口使用不规范，需要检查端口占用情况，避免启动失败。

2. **多应用部署**

   KVM 的 beta 环境，业务线为了使用方便出现多个应用部署在一个实例上面，需要支持同 ip 多端口。

3. **访问权限**

   多端口访问，注意访问权限问题。

- [ ] **性能与存储**

1. 无脑 git clone 既耗时，又占用大量磁盘，注意设置仅 clone 一层。

2. TCP dump 耗时较长，特别容器化后 ip 众多，注意标识失效的 ip ，可以节约大量时间，当然并发拉取是必须操作。


#### 8.2.1.5	应用场景

代码覆盖率我们有很多应用场景，主要包含以下几个场景：

1. **发布流程强卡点**

   我们的开发上线过程存在多个阶段，需求提测前开发需要自测；上线前，QA 需要验证测试，在不同的阶段我们把测试覆盖率数据作为了流程传递的强卡点，同时根据测试人员的专业技能和投入成本，我们为不同的阶段设置了不同的拦截标准，下图8-6是我们的卡点阶段和标准值：

   ![image](/medias/images/coverage/deploy-process.png)

   <center>图8-6 发布流程强卡点</center>

   | 阶段     | 拦截指标与阈值                       |
   | -------- | ------------------------------------ |
   | 提测发布 | 1. 行覆盖度：50%   2.分支覆盖度：30% |
   | 线上发布 | 1. 行覆盖度：75%   2.分支覆盖度：50% |

   <center>图8-7</center>

2. **接口自动化测试覆盖度参考**

3. **接口自动化回归覆盖度参考**

4. **codereview测试覆盖度参考**

5. **线上应用覆盖率数据展示，业务线参考进行系统瘦身**

6. **本地化覆盖率数据上报**

#### 8.2.1.6	效果展示

- [ ] **执行数据**

| 指标           | 触发       | 耗时   |
| -------------- | ---------- | ------ |
| 质检触发       | 30万次/月  | < 0.7s |
| 项目管理触发   | 20万次/月  | < 0.8s |
| 自动化触发     | 15万次/月  | < 0.5s |
| 覆盖率数据收集 | 100万次/月 | < 0.1s |

<center>图8-8</center>

- [ ] **报告展示**

![image-20220914193602718](/Users/chunfang.zhang/Documents/workspace/github/qtwb/medias/images/coverage/report3.png)

![image](/medias/images/coverage/report1.png)

<center>图8-10 报告展示-2</center>

如图8-10展示，报告中记录变更代码行与对应的方法，未变更的方法不统计。提升一定量的精准测试，降低测试难度。



### 8.2.2	阶段2—降本提效

由于覆盖率被作为研发流程的强卡点，因此测试达标标准的覆盖率数据就成为开发与 QA 必须要保障的步骤，但是我们要做的事效率和质量的平衡，因此除了完善自动化测试平台，我们还需要尽可能给开发测试提供更精准与便捷的测试方式，以下介绍三个主要的尝试。

#### 8.2.2.1	重复测试成本优化

- [ ] **业务痛点**

开发与 QA 作为工具的使用者，抱怨测试过的方法，发布几次后覆盖率消失了，需要平台识别并保留没有变更的方法覆盖率数据。

开发在发布测试过程中，历史测试过的方法，期望覆盖率保留，不进行重复测试；这要求覆盖率保留到方法级别。

- [ ] **痛点原因**

分析根本原因是 jacoco 原生覆盖率数据存储结构过于粗糙，数据中以整个文件为单元存储覆盖率数据。文件任何文本变更造成整个文件覆盖率均不可复用，反之则可以合并复用。

覆盖率数据结构：id 对应文件文件，probes 为插装数据，如图8-11：

```java
[
{"id":-5594133704031447426,"name":"com/google/common/util/concurrent/MoreExecutors$DirectExecutor","probes":[false,false,true,true,true]},{"id":8095165752988655296,"name":"org/springframework/context/annotation/BeanAnnotationHelper","probes":[false,true,true,true,true,false,true,true,true,true,true,true,true,true,true,false,false,true,true,true,true,true,true]},{"id":3793724476497282488,"name":"org/apache/tomcat/util/digester/ObjectCreateRule","probes":[true,true,true,true,true,true,true,false,false,false,false,true,true,true,true,true,false,false,false,false,true,false,false,false,false,false,false,false]}
]
```

<center>图8-11 </center>

- [ ] **业务期望**

| 版本&覆盖率      | 方法1               | 方法2                          | 方法3                                 |
| :--------------- | :------------------ | :----------------------------- | :------------------------------------ |
| 发布版本V0 btag0 | m1 【V0】           | m2 【V0】                      | m3 【V0】                             |
| 发布版本V1 btag1 | m1 【V1】           | m2 【V1】                      | m3 【V0】                             |
| 发布版本V2 btag2 | m1 【V2】           | m2 【V1】                      | m3 【V0】                             |
| 覆盖率结果       | 仅btag2覆盖率可使用 | btag1，btag2覆盖率都可追加使用 | btag0，btag1，btag2覆盖率都可追加使用 |

<center>图8-12 </center>

- [ ] **修复方案**

1. 将最后一次发布作为基准，与改分支的每次发布对应的代码做 diff，将变更类中未变更的方法记录下来。

   - 一份代码重复使用，减少多次 clone 磁盘与时间浪费。

   - 此处需要拉取 btag 的代码。

2. 将该分支历史发布分别生成覆盖率，获取变更类中未变更方法（1中记录）的覆盖率数据，并持久化到数据库。

3. 分支覆盖率获取变更方法的覆盖率数据。

   - 初始方法插装数据录制，与后续多 tag 覆盖率插装数据汇总，均在相同方法。

   - 可将变更方法类加载到 jacoco 类中，避免串行更改一连串方法，费时费力，还易错。

代码参考：

```java
org.jacoco.core.internal.analysis.MethodAnalyzer#accept
org.jacoco.core.internal.analysis.InstructionsBuilder
```

<center>图8-13 </center>

#### 8.2.2.2	精准测试

- [ ] **业务痛点**

在酒店、机票、服务等大业务线，业务逻辑复杂的部门，历史遗留下来的逻辑较多，case 很难测试所有行。使用者共同的夙愿就是测试可以精确到变更行，不要以方法作为维度。并且可以识别日志、监控与注释等无业务逻辑的变更。

- [ ] **痛点原因**

1. 变更一行代码需要测试整个方法的所有行，按比例计算覆盖率，测试成本高。
2. 添加日志、监控、注释与空格等无业务逻辑代码也被算作变更行。
3. 覆盖率精准度不高，造成覆盖率偏低，且拦截意义不大。

- [ ] **优化方案**

![image](/medias/images/coverage/exact-process.png)

<center>图8-14 </center>

由于给覆盖率添加一种新的统计数据，修改模块较多，此处列举核心修改点。

```java
//添加统计类型：
org.jacoco.core.analysis.CoverageNodeImpl
//变更行
org.jacoco.core.internal.analysis.SourceNodeImpl#incrementLine
//报告：
org.jacoco.report.html.HTMLFormatter#createTable
```

- [ ] **展示效果**
- [ ] ![image](/medias/images/coverage/report2.png)

<center>图8-15 </center>

1. 方法行：是统计变更方法的所有行，仅供业务线参考。

2. 变更行：是统计变更行的总数，与测试走到的实际变更行，计算出变更行覆盖率。


#### 8.2.2.3	本地覆盖率数据收集

- [ ] **业务痛点**

 作为一名开发，自测完就想上线，但是本地测试的数据又不能被覆盖率平台收集，发布 beta 环境后还需再次测试；不仅降低了开发效率，还增加了开发人员的困扰，影响美丽的心情。

- [ ] **痛点原因**

1. 本地测试的 case 无法作为有效数据被收集。
2. 新需求开发覆盖率很难满足，发布进度被拖长。

- [ ] **解决方案**

1. 覆盖率平台添加本地覆盖率。

   - 记录 git 信息、sha 值、用户信息、时间。
   - 记录 appCode 、分支、来源等做数据分析。

2. 添加本地覆盖率收集上报能力。

   - 单测执行生成覆盖率数据并自动上报。
   - idea 开发插件，支持测试数据手动上报。

- [ ] **使用效果**

1. **调用数据**

   5k次/月

2. **单测触发**

   单侧实现较简单，pom 引入 jacoco 的 agent ，单测执行完调用 url 接口即可，这里不做介绍。

3. **idea插件简介**

   插件开发有一定的工作量，由于 idea 版本不一样提供的底层 api 有所变更，因此需要支持多个 idea 版本。idea 插件如图8-16和8-17：

​	自定义 tomcat 插件，绑定环境，VM 参数自动填充。

![image](/medias/images/coverage/local1.png)

  <center>图8-16 idea 插件-1</center>

  启动后添加右键功能，上报覆盖率数据。![image](/medias/images/coverage/local2.png)

  <center>图8-17 idea 插件-2</center>

- [ ] **效果数据**

| 指标         | 优化前阈值 | 优化后阈值 |
| ------------ | :--------- | :--------- |
| 变更行拦截   | 50%        | 70%        |
| 变更分支拦截 | 30%        | 50%        |
| 跳过拦截申请 | 1.3k/周    | 100次/周   |



## 8.3	总结展望

#### 8.3.1	总结

复盘覆盖率平台，一个工具的发会展经过三个阶段：

1. 做出来：根据业务需求做出工具，提供基础能力。
2. 推广开：落地到业务线，落地前先在本组试用。
3. 有价值：工具为公司带来实际价值，做好数据统计。

达到第三个阶段才是企业的真实目的，心怀谦卑之心，倾听用户的心声，将工作做到有价值至关重要。

#### 8.3.2	展望

做工具我们希望它稳定运行的同时，能够应用到更多的场景，当前我们主要的应用场景还是在测试阶段，后续我们会逐渐在灰度、仿真、线上场景做出探索，比如说系统腐化分析、新业务需求评估等。











# -----------------

# Part5	CICD

CICD 是保障我们持续交付的基础能力，它的核心能力是将开发同学的劳动成本转化为可执行的二进制文件并部署到基础资源上进行运行，从而为用户提供服务。去哪儿网的 CICD 系统是完全自研的，最终部署到我们的私用云上，云原生之前我们的资源只有 KVM ，KVM 最大的特点是提前申请型，一旦申请 IP 固定，我们提前对各种需要的权限、资源申请好，之后每次部署只是版本更新即可。而容器是 k8s 统一管理，发布过程动态创建，所以机器 IP 和 hostname 都是随机分配的，这就需要我们动态的做各种周边建设；而且容器更多的管理是在 k8s 层，我们无法像 kvm 那样更精确的管控，这些都需要我们 CICD 系统进行适配。

本章将分两个章节对我们的 CICD 进行介绍，流水线介绍我们开发过程的测试验证效率保障方案，发布驾驶仓介绍我们基于云原生落地中发布过程可观测性的适配。











# 第九章	发布驾驶舱



## 9.1	背景介绍

### 9.1.1	去哪儿网 CICD 建设

在竞争激烈的互联网行业，高效持续的交付能力是我们抢占市场先机的重要基础，由此 CICD 应用而生。所谓 CICD 即 Continuous Integration、Continuous Delivery，是指通过自动化的手段将代码持续的集成打包并持续的部署验证，它主要的功能就是保障技术人员的产出能够快速被验证、被交付，从而保证业务价值、产品需求能够快速、持续可靠的交付到用户手上。

去哪儿网作为典型的互联网旅游电商平台，有着旅游行业业务复杂、业务需求多变的特点。为了更好的应对业务的快速变化所带来的挑战，去哪儿网结合自身的业务特征，开发了自己的 CICD 系统，确保应用高质量且快速的持续集成和交付。

在去哪儿，我们采用微服务架构，每个微服务我们称之为应用，每个应用都有唯一的标识即 appcode ，所有的研发过程都是围绕 appcode 进行，同样 CICD 也不例外，下图9-1是我们的基本架构图。

![image](/medias/images/deploy_dashboard/deploy_dashboard_6.png)

<center>图9-1 CICD基本架构图</center>

如上图9-1所示，去哪儿网 CICD 体系主要由应用画像、代码/仓库管理，质量控制、流水线、编译/部署以及k8s/kvm 六个模块构成。其中：

1. **应用画像**

   我们的 CICD 是以单个应用为维度建设的，而应用画像便是保存了该应用的配置信息以便交付运维使用，包括运行时配置(环境、主机数、CPU、内存等)，发布配置(代码地址、自定义发布脚本、发布参数等)，依赖配置(负载均衡、应用监控、DB白名单等)。

2. **代码/仓库管理**

   主要管理应用代码、基础镜像、依赖源，同时也保存持续集成生成的镜像、二进制产物包等等。

3. **质量控制**

   在 CICD 过程我们集成了质量控制体系用于 CICD 过程质量保障，包括丰富的质量检查手段：code review 检查、静态代码扫描、安全扫描、单元测试、自动化测试、覆盖率检查等，这些检查手段是通过代码 push 自动触发的，同时会有及时的消息反馈，减少了人力成本。；还包括质量门禁模块，用于将检查手段集成到发布阶段，保障检查手段的真实有效执行及被解决。

4. **编译部署&流水线**

   我们的发布系统是一个独立的系统，编排调度模块完全自研，调度 Jenkins 集群执行具体的构建打包和部署逻辑，公司内的开发测试同学可以通过输入分支信息进行部署发布，这样保证了线上发布的人工控制。

   同时我们依托于 argo 自研了流水线服务，将研发过程的构建打包验证串联起来，包括质量检查、代码校验、构建打包、环境创建更新、部署发布、业务验证等来，并实现 code-push 自动触发，执行结果自动IM 消息反馈，保障了开发过程的快速集成、持续验证、无人工干扰。

5. **k8s/openstack**

   最终我们的服务根据业务属性不同部署我们自己的私有云上，包括 k8s 和 openstack。

### 9.1.2	容器化落地

 随着云原生技术的兴起，去哪儿网从2021年初也开启了自己的容器化落地之路，通过一些列的改造适配迁移，目前我们已经实现了80%左右的应用容器化，在基础设施容器化章节中我们已经介绍了在落地过程中 CICD 系统的改造，但是当时的改造主要还是在交付逻辑上的改造，用户交互流程上只是适配，而且我们的交付模式采用了IP动态分配的方式，这种方式跟 kvm IP 固定方式完全相悖，因此我们之前基于 kvm 建设的 CICD 交付流程对于开发过程的无感知流水线冲击不大，但是对于线上发布这种需要全方位信息观测的动作就造成了较大的冲击。经过一段时间的运行，问题逐渐暴露，先来看下图9-2中我们以前的交付界面。

![image](/medias/images/deploy_dashboard/deploy_dashboard_8.png)

<center>图9-2 交付界面（旧）</center>

由上图9-2我们不难发现主要存在以下4个问题：

1. **整体进度不直观**

   kvm 发布我们可以很直观的查看到发布的进度，但是对于容器发布（每次发布 pod 重建，同时采用双deployment 部署，一增一减、先增后减）却无法很直观的了解当前的整体进度、新老版本的比例、单pod 的进度等。

2. **发布异常定位慢**

   当前模式对于 kvm 来说，发布过程开发同学是提前通过跳板机远程登陆到主机后，单机查看日志信息。但是当容器发布时，容器的名称是动态生成的，开发无法提前登陆，这样就会导致开发无法实时的监控异常，出现问题发现就会滞后很可能造成故障。

3. **信息分散**

   其实由上图可以看出我们的发布过程只能看到机器的部署进度，然后发布是一个非常重的动作，是将我们的成果交付用户的最后一道防线，也是很容易产生问题的环节，统计我们历年的故障，将近50%以上的故障都是发布引起的，所以一定要做好质量保障，主要是发布前的验证测试以及发布过程的及时观测。

   在去哪儿我们是要求发布之后持续观察监控30分钟的，但是从上图中我们并不能看到监控信息，开发同学每次需要到监控系统打开关注的面板，然后再进行发布，无形中又为发布增加了负担。

4. **关联问题无法发现**

   发布某个应用有时候可能自己没有问题，但是会造成上游服务异常，比如说我们接口向下不兼容，但是别人却依赖了我们的老版本而没有同步修改，因此怎么能让本次发布影响的所有应用都被可观测，这又是我们需要考虑的重点.

基于以上问题，我们跟项目流程管理同学、业务同学一起确定了发布流程并把流程固化在了发布系统中，新增了仪表盘、发布过程异常信息汇总，并依托公司的应用拓扑图，将当前应用及其上下游应用的异常日志、监控报警有机整合起来，让整个发布过程更加透明，用户能够实时观测到发布进度、异常日志和异常报警，从而快速做出决策，减少故障，及时止损。

## 9.2	实践框架

由上述的分析可知，我们主要解决的是线上发布的可观测性问题，那么就需要知道这个过程需要观测哪些内容，其实由云原生的可观测性我们也可以知道，主要包含：

1. **报警**

   通常变更是导致问题的最直接原因，因此当我们发布的时候需要实时关注报警信息，这样才能最快的发现变更导致的问题并快速的决策恢复。

2. **监控**

   有的同学可能会说报警不就够了吗，为啥还要看监控，其实报警是一个相对来说比较严格的策略，为了提升报警的有效性，降低日常报警的干扰率和开发同学的运维成本，我们并不希望大家将所有的指标波动都设置为报警。

   而且一个应用的变更可能影响的链路非常长，比如说有一条链路 A->B->C->D->E，当A变更时，可能是E的指标发生了波动，因此在去哪儿业务同学会将关心的指标聚合成几个重要的面板，同时对这几个面板进行核心标记，这样当这块业务中的任何一个应用变更时，只需要关注这些核心面板即可，大幅提升了定位问题的速度；

3. **日志**

   由于指标报警的滞后性（在去哪儿指标是1分钟收集一次，报警自定义，通常是5分钟报出来）和灵敏度（指标更多的是业务的反应，单台机器的异常可能无法在业务指标上快速体现），在发布过程开发同学需要实时查看变更 pod 的日志。然而有些变更可能不体现在单台上，因此我们也需要关注应用整体的异常日志变化量。

4. **链路**

   正如上文分析，微服务架构虽然大家都可以独立运行，但是又相互依赖，而且变更通常可能导致的是上下游、甚至多层依赖的异常，因此我们需要关注变更应用以及整个链路的报警、监控、日志信息。

5. **进度**

   线上发布是将老版本下线新版本上线的更新过程，而通常一个应用为了实现高可用都会部署在多集群，同时也会根据线上的 QPS 进行多 POD 部署，而且前文也提到我们是采用双 deployment 模式，先上线再下线，因此整个发布的过程是需要一定的时间的，那么我们就要对当前的进度可观测，因此我们需要了解当前整体的进度：新老版本的个数，单 Pod 的进度：在初始化或者 OR 更新等。

基于以上分析我们对发布过程进行了改造，目标是将发布过程应该关注的信息进行聚合，以便用户可以快速的做出决策，避免问题的扩大化，我们将其命名为发布驾驶舱，以下是我们的架构图：

![image](/medias/images/CICD/dashboard1.png)

<center>图9-3 发布驾驶舱架构图</center>

如上图9-3所示，发布驾驶舱最终落地是在发布系统中，主要包含数据层、分析层、展示层。

### 9.2.1	数据层

1. **应用拓扑**

   基于负载均衡日志、rpc 信息等提炼分析，生成应用间的依赖拓扑图，可以获取 appcode 的上下游信息，发布驾驶仓从此处获取本次发布影响的变更链路，确定影响的应用范围。

2. **监控控告警**

   基于 grafana、graphite、icinga 开发的公司级监控报警系统，发布驾驶仓从此处获取本次发布的核心面面板及影响应用的监控告警信息。

3. **异常日志**

   公司自研的基于 kafka、hbase 等的异常日志实时收集、分析系统，发布驾驶仓从此处获取单pod异常堆栈及影响应用的异常日志汇总信息。

4. **资源管理**

   k8s 集群管理，发布驾驶仓跟其交互获取容器部署过程中的整体进度信息(批次信息、部署日志)、整体pod 数变化以及每个 pod 的状态。

### 9.2.2	分析层

分析层主要是将数据收集之后进行分析汇总。

1. **拓扑分析**

   在发布过程我们会从应用拓扑中获取当前应用的上下游，并将这些应用及发布应用整合作为本次发布的变更影响范围。

2. **报警、监控、异常日志**

![image](/medias/images/deploy_dashboard/deploy_dashboard_2.png)

<center>图9-4报警、监控和异常日志</center>

如上图9-4所示，在容器发布进行到部署阶段后，系统会首先获取当前 appcode 的上下游信息，之后会轮询监控告警系统和异常日志系统，获取发布时间段内当前应用以及上下游应用的报警信息和异常日志信息，同时会计算报警增量、异常日志环比、增量等；后端分析汇总后传回前端，分别展示总体信息（报警总数、异常日志新增&环比）以及详细信息，并且在有报警和异常日志时，我们会自动跳转到对应 tab 页以进行更及时的提醒。

### 9.2.3	进度层

![image](/medias/images/deploy_dashboard/deploy_dashboard_5.png)

<center>图9-5</center>

我们当前的容器发布采用的是双 deployment 的形式，发布过程中会新建一个 deployment ，分批次增加新版本的 pod 数目，在当前批次发布完成且通过检测后，会对应缩减原 deployment 的 pod 数；由此一增一减，先增后减，最终新 deployment 达到目标容器数，原 deployment 缩减到0并被新 deployment 替换。

双 deployment 带来了很多优势，比如易于控制，能够分批发布，能够及时暂停，安全性较高、扩展性强等；但同时也带来了一些问题，比如代码版本管理困难、中间状态不好控制等。由此在观测性上也带来了一些问题，比如用户反馈无法及时观测到新旧代码版本的pod数变化、整体发布进度(比如批次信息、发布状态等)。因此我们引入了仪表盘，实时展示新旧代码对应的 pod 数，以及整体发布进度信息。

![image](/medias/images/deploy_dashboard/deploy_dashboard_4.png)

<center>图9-6</center>

如上图9-6所示，在发布过程中，发布系统会实时获取当前发布整体进度，包括发布批次、发布状态等，并实时 watch pod 变更，在有 pod 变更时，会按 deployment 、新旧代码版本重新计算 pod 数目，并回传给前端并展示。

### 9.2.4	展示层

最终我们会将以上获取到的信息及分析汇总结果展示给用户，包括：

1. pod新老版本整体进度仪表盘，可以实时查看当前发布批次、发布状态、新旧版本pod数据等。
2. 监控告警、异常日志的汇总信息，包括当前报警总数、异常日志类型总数、异常日志新增数、环比数等。
3. 报警、异常日志详情tab:详细查看报警个数、报警指标、报警状态、异常日志类型、异常日志堆栈、环比增幅等。

通过以上这些，用户可以实时掌控整个发布过程，提升发布安全性，降低故障率，及时止损。

![image](/medias/images/CICD/dashboard2.png)

<center>图9-7 示例</center>

![image](/medias/images/deploy_dashboard/deploy_dashboard_10.png)

<center>图9-8 示例</center>



## 9.3	难点坑点

### 9.3.1	怎样精准识别关键信息

- [ ] #### 问题描述

  发布驾驶仓的目的是建立发布过程的可观测性，按照上述实现路径我们将发布过程的信息进行了聚合，理想情况下可以解决用户的痛点，但是如果我们仅仅是把信息堆砌在一起，那么用户看到的就是一堆杂乱无绪的信息，不仅解决不了问题，严重甚至会对定位问题造成干扰，那么就失去了精准提醒、快速定位的意义。

- [ ] #### 解决思路

1. **信息统计而不是明细罗列**

   由上述分析可知我们变更期间需要观测指标、报警、异常日志等多种信息，而且通常情况如果此次发布变更产生问题，很可能会有级联报警、异常日志等，单独明细去看费时费力，因此我们提供了信息汇总，通过统计发布期间的异常日志、报警量总量变化、发布阶段和平时的环比对比等，让用户可以很直接的观测到发布带来的影响并做评估。

   同时我们也提供了报警和异常日志明细 tab，每分钟定时刷新，而且当有增量变化是动态跳转 tab，在交互上做到更佳智能。

2. **链路诊断而不是单点观测**

   俗话说牵一发动全身，经过这么多年的积累成淀，业务系统的关联关系是非常庞杂的，因此当一次发布变更时，不仅自己的信息非常多，影响的范围也可能会非常大，系统的雪崩也可能是这么来的，因此我们通过应用的拓扑依赖获取了应用上下游的依赖链路及应用自己的相关信息进行汇总，这样能够更大范围的观测变更的影响，更快速的定位变更的影响。

3. **业务观测而不是资源分析**

   其实在做发布驾驶仓时我们也调研了一些开源的 devops 产品，发现大部分的产品在容器发布过程的可观测性集成的是资源层面的监控，但是通常情况下业务变更对资源的影响较小，对业务的影响较大，因此我们获取了发布应用对应的核心监控面板（面板可能是该应用涉及的业务模块，也可能是该应用归属团队的业务大盘），并将该面板集成到发布过程，这样发布的同学可以高效的了解发布应用的影响，同时也解决了新同学学习核心面板、部分同学只关注自己应用变更的问题。

### 9.3.2	怎样获取有效的异常日志

- [ ] #### 问题定位

  技术同学应该都知道，为了问题定位或者处理特定 case，系统一般都会打印许多日志，同样异常日志也会有很多，拿我们比较大的业务系统，10分钟就会产生上万条异常日志，但是这些日志有时候是系统运行所能容忍的，并不需要被关注，那么我们在发布阶段怎样获取发布的异常日志帮助业务同学定位问题呢？

- [ ] #### 解决思路

  定义异常日志的优先级：我们根据日常收集到的异常日志进行了分组，比如代码异常（空指针、数组越界）、DB 异常（慢查询）、redis 异常（获取链接失败）、dubbo 异常（限流异常）、http 异常等，同时每组异常类型进行了优先级定义，包括 P1/P2/P3/P4 ，如空指针、数组越界等为 P1，那么发布驾驶仓只会获取 P1 级别的异常日志，这样当该异常日志有变化时，发布人员就可以及时的终止发布进行定位了。

  

## 9.4	总结展望

在去哪儿网云原生的途中，容器化被作为其中极其重要的一步，是去哪儿网云原生的前提和基石。通过容器发布驾驶舱项目的完成，将发布进度、应用及其上下游的异常日志和异常报警有机统合在一起，帮助业务线提升容器发布过程中的可观测性，发布人员可以实时掌握整个发布进程并及时决策，增强了容器发布的透明度，提升了发布的安全性。后续我们将在以下两方面进行探索：

1. **继续整合更多的决策信息**

   包括如负载均衡up_stream变更信息，并尝试引入决策算法，比如在出现特定异常日志、报警或者某些异常日志报警数目到达一定量后，自动挂起发布进度，并提示用户及时决策。

2. **在发布驾驶仓进行埋点数据统计**

   比如发布过程是否查看监控面板、观测时长是否符合公司发布要求等，通过这种方式真正将规范落地到系统中。











# 第十章	持续交付流水线



## 10.1	背景

### 10.1.1	效率质量的平衡

互联网背景下，如何更快也更高质量的交付我们的产品已经成为了一个越来越重要的课题；。

一般看来快速和高质量有互斥的地方，需要做均衡和取舍。在技术发展的今天，各个模块功能愈加完备，提升效率的工具也越来越多，如何提高质量的方式和手段也愈加的花样繁多。

但一些提效的手段提高的效率也被越来越多的流程或者检查所拖累，各种功能模块之间的数据同步、信息流转甚至审查也越来越难以协调，需要花费大把的时间在此上，且每一次迭代都需要重复以上流程；

此种状态之下我们急需一种能够协调效率和质量的工具，降低每一个工程师及其它参与者的心智负担和学习成本，实现效率和质量的双赢。

### 10.1.2	去哪儿面临的效率困境

在去哪儿，我们为了保证交付效率和质量做了各种工具建设，以下是我们的一个需求从产生到最终的上线要经历的全部过程：

![image](/medias/images/CICD/pipeline1.png)

<center>图10-1 需求生命周期</center>

从上图10-1可以看出这个流程是非常长的，然后又是不可或缺的，因此我们需要做的就是将这个流程尽可能的自动化，过程对用户无感知，结果将关键信息暴漏给用户即可，因此我们实现了 codereview 的自动创建、code-push 静态代码自动扫描、软路由环境、一键部署等。

即我们做了很多单点的建设，而且为了最大程度的发挥大家的主管能动性，这些单点建设分布在不同的团队里，不同团队的开发习惯、服务方式都不一样，这种情况在基础设施不完善的情况下，效率提升非常明显，但是基础设施已经建设完全，问题就逐渐暴漏：

1. **集成成本变高**

   上述所说不同点的效率、质量保障可能是不同团队提供的，不同团队的开发风格差异非常大，可能是一个http接口、也可能是一个消息推送、还可能的是一条广播事件，但是最终都要集成到统一的流程中，比如安全同学提供了漏洞扫描组件、业务团队需要在部署完成进行业务检查等，由于事先没有统一的规范，那么最终只能对单独功能进行适配，开发维护成本都非常高；

2. **上线风险变大**

   流程迭代越来越长，尤其是为了保证交付的质量，我们会做各种各样的自动化检查，比如部署完成的接口自动化，代码变更后的配置修改等，研发过程的流程驱动全靠人工，人工就难免遗漏，经常会听到测试人员抱怨开发不自测不提前做检查的声音，但是开发同学同样为写代码之外的工具流程学习成本叫苦不迭。

   再说上线，虽然我们会在提前对齐发布步骤，但是这些都是整个过程的回放，尤其是对于横跨几周甚至几个月的大项目，很可能造成步骤的遗漏，最终产生线上故障，我们已经遇到过好多次上线后忘记修改配置、忘记开启定时任务的故障。

3. **学习成本变高**

   对应着流程的复杂度提升，开发测试同学的学习成本也逐渐提升，曾经我们做过统计，一个需求的交付过程需求切换6个以上的系统，这些系统即使你不需要彻底了解，但是起码也需要知道入口在哪，主要功能是啥，遇到问题怎们定位等，也许大家可能说习惯了就好了，但是互联网人员流动是常态，这种新入职员工的成本也不容小觑。

   

## 10.2	业务方案设计

### 10.2.1	方案选型

由上述的背景分析可知，我们急需将研发过程的流程进行自动化串联，解决以上面临的问题，因此我们调研了业界的相关方案，发现主要是两种解法：

1. **all in one云平台**

   这种方式的好处是所以逻辑都由一个系统承接，所有的数据和信息也都是存在于一个系统，保证了数据源的统一性和用户体验的一致性，对于新的方案接入也可以保持方法统一；但是这种方法对于我们去哪儿这种有较长发展历史周期的公司不太适用，完全重头搭建成本太高，系统对接适配就会带来比较差的用户体验，同时对接的系统需要将各种数据对齐，比如说是以 git 为准、还是以一个应用为准等，同样需要一个数据适配层；

2. **流水线调度**

  调研业界研发效能平台的流水线，大部分是比较简单的构建部署（可能也有复杂的是笔者没有调研到），但是这种模式是我们可以借鉴的，因为在单点上我们已经做到足够好，需要做的就是利用流水线将所有的流程串联起来，同时将上游阶段（开发阶段）产生的数据信息，比如DB变更、配置变更等信息传递到下游，这样就避免了上线前人工编排的遗漏。

  而且通过流水线串联可以让用户很直观的了解流程节点信息但是又屏蔽掉了单点的具体信息，同时流程断点也可以很直观的定位，大大降低了用户的学习和使用成本，基于此我们选择了该方案。在真实的落地实现过程中我们主要需要解决以下几个问题：

  - 可持续发展，设计整体需要向后考虑，留下扩展和更改的可能性，因为我们可能后续会接入更多的步骤，提前考虑避免日后的运维成本。

  - 使用时候的学习成本要低， 避免用户接入 or 使用过程中需要通过大把时间来了解流水线的背景知识。

  - 模式要尽量统一， 流水线整体执行等要采用统一的执行流程或执行规范，避免架构的过多适配。

  - 可观测，不能完全是一个纯粹的黑盒，要能够观测到从开始到结束的过程及各个阶段耗时及数据和日志的变化。

### 10.2.2	流水线类型拆分

在整个交付过程，我们通常的划分是设计阶段（产品关注）、实现阶段（开发关注）、测试阶段（开发、测试关注）、发布运维（开发、测试关注），在每个阶段关注的角色和执行的流程都不相同。

1. **设计阶段**

   主要是业务同学和产品同学关注，做的事情更多的是思考设计，所用的工具也比较统一，而且跟后续的流程连通性要求也没那么高，因此我们先不关注设计阶段。

2. **开发阶段**

   我们期望的是质量左移，即问题尽可能早的在开发阶段暴漏，因此我们要求开发同学及早的集成及早的解决问题；同时开发自测一般关注的是单应用维度，因此我们基于每次push进行全流程执行的效率复用度更高。

3. **测试阶段**

   需要开发同学快速修复、QA 同学快速验证，因此也需要频繁的部署、测试、验证，而且在去哪儿我们目前正在推行开发自测自发，但是像测试环境的搭建、自动化测试之星这些测试同学是更擅长的，因此这些流程的学习执行成本降低的需求更强烈。

4. **上线和运维阶段**

   则需要非常慎重，一般情况每步都需要人工确认验证；而且一次发布对应的是一组应用，因此一般是对多个应用的组织编排，因此这个阶段更关注的是提前的组织编排能力和开发测试阶段的变更：包括 DB、代码、配置的自动生成能力，而不是自动触发能力。

因此我们根据以上阶段拆分成几条流水线：

1. **开发流水线**

   覆盖用户提交代码，到完整的自测过程，交付开发人员完整的在运行中的自测环境。

2. **提测流水线** 

   覆盖开发自测完成，代码提交给 QA同学进行测试的过程，解决之前的数据扭转等问题，及自动化的执行更多的如回归测试，性能测试等。

3. **项目流水线**

   覆盖一个项目的周期，涉及多应用多模块的开发及测试，编排其执行顺序及统一的联调测试。

4. **运维流水线**

   固化的运维场景及一些常用的流程性动作串联。

5. **其他** 

   用户自定义。

那么由上述的流水线拆分我们还发现一个问题，在不同的流水线中关注的应用个数是不同的，开发测试可能是单应用，而在需求上线阶段关注的却是多应用的组织编排，而且我们公司内部更希望将流程固化，避免流水线灵活自定义太大的运维成本，因此我们选择了固定应用流水线+灵活编排项目流水线的方式，如图10-2所示：

![pipeline2](/medias/images/CICD/pipeline2.png)

<center>图10-2 流水线</center>

我们优先做了开发流水线，因为效率的提高及覆盖的场景无需一次性做到大而全，可以采用分阶段和渐进式的方式来完成，每完成一阶段提高一部分效率并降低部分成本即是好的，避免大而全带来的推广难度及不可控的问题,以下的设计和实现以开发流水线为例进行介绍，其他都可以服用此模式。



## 10.3	整体框架和实现

基于以上存在的问题及整体的设计思路来考虑，整体的流水线执行逻辑可以简化为以下3个层面。

### 10.3.1	单次任务执行逻辑

1. yaml（配置文件这里以 yaml 为例）提供了完整的执行流程配置，提交到执行层，后续由执行层完全接管执行。

2. 执行层识别 yaml 中的最早执行的 step，执行并收集其执行结果。
3. 执行层根据初始 step 的状态及 yaml 配置相关的的串并行逻辑及条件判断和循环执行逻辑继续推进 step执行，直到最终完成。

![执行逻辑.png](/medias/images/deploy_dashboard/pipeline1.png)

<center>图10-3 yaml 执行流程</center>

### 10.3.2	系统模型设计

采用分层设计，总体分为三层，隔离和简化用户输入使用的 UI层，动作编排和执行及记录的适配层。

1. **UI 和 API 层**

   提供用户配置和查看流水线的界面，将复杂配置逻辑简化仅留下用户使用的必填项，降低使用成本。

2. **编排层**

   提供模板化和固化功能，实时根据参数及执行过程的编排填充完整的执行配置，最终通过一个个任务执行，且通过回调等方式留存执行过程及结果的记录。

3. **执行层** 

   驱动执行， 收集执行状态和数据，回调并记录结果。

**![整体执行架构.png](/medias/images/deploy_dashboard/pipeline2.png)**

<center>图10-4 分层设计</center>

### 10.3.3	落地实现

#### 10.3.3.1	执行层

- [ ] #### **选型对比**

|              | Jenkins X | argo-workflows | 自研 |
| :----------- | :-------- | :------------- | :--- |
| 部署/维护    | 一般      | 简单           | -    |
| 简单使用     | 简单      | 简单           | -    |
| 复杂用法     | 难        | 一般           | -    |
| 配置友好程度 | 难        | 简单           | -    |
| 社区活跃     | 好        | 好             | 无   |
| 开发成本     | 一般      | 一般           | 高   |
| UI           | 一般      | 差             | -    |

<center>图10-5 选型对比</center>

作为平台方我们会对外提供服务注定会有很多复杂度很高的使用场景，这个时候 Jenkins X 就不太符合我们的需求了。

其次，我们也比较关注与整个开发成本，完整的自己实现一遍需要关注的细节太多，成本较高也不符合预期。

另一个比较重要的选型依据是我们正在容器化的进程中，而 argo-workflows 是 CNCF 项目，天然的更契合云原生，所以我们的最终执行层选型为 **argo-workflows**。它主要有2个特点：

1. 运行在 Kubernetes 上的自定义资源（CR）定义工作流，工作流中的每个步骤都是一个容器
2. 可将多步骤工作流建模为一系列任务，也可以使用有向无环图（DAG）描述任务之间的依赖关系

- [ ] #### **考量**


1. 考虑独立的 k8s 集群，避免意外大量资源占用影响线上业务。
2. argo-workflow 执行 pod 时候除了任务本身还会有初始化和最终日志等清理镜像，部署时候应该注意将该镜像拉取到本地的仓库并改为本次仓库部署，减轻可能的每次执行拉取外部镜像的耗时。

- [ ] #### **实现**


部署方式参考各种搜索结果直接通过 yaml 安装即可。

#### 10.3.3.2	编排层

- [ ] #### 考量


1. 尽量提供统一且通用的抽象的可复用的模块来降低管理难度及减少执行层拉取新镜像的开销。
2. 整体执行过程要留痕，可观测，不能完全作为黑盒，便于问题排查和统计分析。
3. 提供通用的模板存储和复用逻辑，减少用户使用成本。

- [ ] #### 实现


编排层主要抽象四服务：

1. 通用镜像 “httppod”，提供所有的支持http访问形式的模版接入和使用
2. 通用的异步请求结果封装 pipeline_gateway；
3. 交互接口及适配层 pipeline-x；
4. 其他定制即兼容适配 pipeline-adapters；

- [ ] #### **httppod**

一个通用的镜像 “httppod”，提供所有的支持 http 访问形式的模板接入和使用

1. **httppod 实现逻辑**

   - 通过环境变量的形式来决定请求方法，请求的目标地址、超时时间及是否同步请求还是异步请求。

   - 通过命令行参数来决定请求的参数，命令行参数使用类似 get 请求的方式来输入，最终的请求发出去时候会根据请求的方法等判断是否转为字典形式发送。

   - 访问部分服务的时候可能仅仅是个通知或者是弱依赖的服务，其返回状态不应该阻止后续步骤的执行。

   - 不同服务返回状态的时候参数 key 不固定，或者 value 类型不同。

2. **环境变量必选**

   - METHOD， 可选值 “GET” “POST”，用于区别发出去的 http 请求采用哪种方法。


   - REQUEST_URL ，请求目标 url，需要以 http:// or https://  开头。


   - TIMEOUT ，超时时间，单位为 s，int 类型的值，需要大于0 。

3. **环境变量可选**

   -   IS_ASYNC，是否是异步执行，默认是同步，该 env 取对应的值的第一个字母，如果第一个字母为 y or Y，则设置为异步。

4. **命令行参数可选**

   用于设置 http 请求的相关参数， 只支持 "xxx=xxx&x=aaa&xxxxx=bbb" 的写法， 且不含空格。

5. **难点分析**

   - 采用何种异步的形式

   ”一般的 RPC 调用“存在的问题在于不是每一个用户都能够驾驭，学习及使用成本较高，且比较黑盒隐藏了较多网络调用等等信息，不利于问题排查。而 HTTP 虽然会要求我们发起请求的同时要起一个 HTTP 的 server 来供服务端回调，但 HTTP 学习成本较低，且传输数据信息比较灵活所以这里采用 HTTP 的形式来做。

   - 唯一性标识应该放到哪

   由于是 HTTP 异步调用那么需要能够精确的将结果回调发起端，需要有唯一标识；最初的阶段我们将唯一的标识放到发起请求的返回体中，但是实践过程中发现如果我们将唯一标识放到返回的请求体中，如果其他已经存在的服务要作为模块提供流水线使用，会对之前已经实现的服务等产生较大侵入性，会要求其修改返回的body；所以我们将唯一标识放到 http header 请求头来解决该问题，既有服务增加一个 http 头返回的成本相对较少，也不容易产生副作用。

   - 执行状态识别


   同样的既有的服务采用了不同的形式的返回状态标记，我们需要一个比较灵活且能覆盖比较大范围的识别模板，且能够可配置；见下方配置示例。

   - 执行状态后续动作


   不同的服务提供的服务等级及服务本身的重要性其实是不同的，甚至部分服务可能只是个简单的通知类服务，成功与失败的状态不太重要需要有对失败情况的忽略；见下方配置示例10-6。

```
状态识别代码样例, 每一个代理的服务获取其对应的状态校验函数
func (t *TransportConfig) getStatusCheckFunc(_type string) func(string, string) bool {
    switch _type {
    case StatusCheckEqual:
        return func(s1, s2 string) bool { return s1 == s2 }
    case StatusCheckHasPrefix:
        return strings.HasPrefix
    case StatusCheckHasSuffix:
        return strings.HasSuffix
    case StatusCheckContains:
        return strings.Contains
    default:
    }
    return func(s1, s2 string) bool { return false }
}
```

```
某些服务的返回结果判断格式及是否忽略退出状态等配置
提供目标服务的统一的配置中心配置，无需在执行 httppod 时候在要求用户输入
"xxxxxxx": {
    "name": "xxxxxx",
    "response_status_key": "status",
    "response_status_type": "Int",
    "response_status_check_type": "Equal",
    "response_status_legal": [
        "0"
    ],
    "is_ingroe_step": true
},
```

<center>图10-6 配置示例</center>



- [ ] #### pipeline_gateway

通过一个通用的模块 "pipeline_gateway"，提供所有已经实现了异步接口（但请求格式不是简单的字典等形式有其他格式要求）或通过 IC 消息（内部的一种异步消息队列，非 http callback）来提供执行结果的服务抽象。

1. **pipeline_gateway 实现逻辑**

   - 通过抽象请求的目标、方法、参数及返回结果的数据验证方法将不同目标配置化。

   - 通过统一的配置中心配置，实现整体的配置动态热加载。

   - 为防止误操作及避免不必要的操作通过版本号等控制片段级别的更新。

   - 部分服务的认证方式不同，也需要适配和支持。

   - 不同服务的对于失败或者跳过定义不同。

   - 该服务是代理的直接通过 httppod 无法请求的已经存在的服务，无可避免的也需要适配返回 key的不同及 value 格式不同等问题。

   - 异步通过 IC 等返回状态的服务必须提供一种识别对应请求的唯一 ID ，该 ID 需要统一存储到固定的地方，使该服务无状态可水平扩展。

2. **难点分析**

   - 如何识别代理的服务返回的状态，如同 httppod 也需要识别状态等，我们也统一采用将对应格式的数据转为 String 后来判断状态。

   - 不同的服务有 header 认证，有采用用户认证等形式，我们需要适配不同的认证方式。

   - 部分服务的消息通知等模块是后实现的，其实现过程中甚至存在消息使用的成功与失败与直接请求返回的成功与失败的字段及类型不同，有部分服务改动成本较高也需要我们来实现。

   - 不同的服务调用的方式不同，由于调用参数及格式等变化不多我们统一抽象为模板的形式来固化。

   - 会接入很多的服务，需要尽量的降低每个服务的接入成本。

配置和服务模板等示例如图10-7。

```
主配置文件片段样例, 控制接入哪些服务
{   
    "version": 1,
    "name": "xxxxxxxx",
    "request_url": "http://xxxxxxxxx/api/xxxxxxx",
    "method": "POST",
    "is_need_auth": false,
    "auth_style": "",
    "add_request_header": [],
    "add_request_header_params": [],
    "response_uuid_key": "appTaskId",
    "response_uuid_type": "String",
    "response_status_key": "status",
    "response_status_type": "Int",
    "response_status_check_type": "Equal",
    "response_status_legal": [
        "0"
    ],
    "ic_topic": "aaaaaaaaaaaaaaaaa",
    "ic_uuid_key": "appTaskId",
    "ic_uuid_type": "String",
    "ic_status_key": "status",
    "ic_status_type": "Bool",
    "ic_status_check_type": "Equal",
    "ic_status_legal": [
        "true"
    ],
    "template_file": "xxxxxxxxx.tpl"
}
```

```
模板配置文件样例   xxxxxxxxx.tpl， 对应服务的请求模板
{
    "build_params": {
        "app_params": [
            {
                "app_code": "{{ .xxxxxxxx0 }}",
                "xxxx_forced": {{ .xxxxxx1 }},
                "xxxx_stage": {{ .xxxxxxx2 }},
                "env_list": [
                    {
                        "env_id": "{{ .env_id }}",
                        "servers": [{{ .servers }}]
                    }
                ],
                "input_tag": "{{ .input_tag }}",
                "pmo_id": "{{ .pmo_id }}",
                "restart_healthcheck": {{ .restart_healthcheck }}
            }
        ],
        "xxxxx_order": "",
        "xxxxx_order": "",
        "xxxxx_type": "{{ .deploy_type }}"
    },
    "channel_id": "{{ .channel_id }}",
    "xxxxxxxxx": "{{ .xxxxxxxxxxx }}"
}
```

<center>图10-7 配置示例</center>



- [ ] #### pipeline-x

1. **实现逻辑**

   - 定位于交互接口及适配层。

   - 提供执行过程及统计等信息的外部存储和查看能力。

   - 根据模板生成待执行任务的完整配置并交付执行。

   - 防腐设计，后来发展的系统不可避免的需要有各种的兼容和定制逻辑，这些功能尽量做到这里。

2. **问题和迭代**

   - 流水线开始和结束如何通知用户？需要在开始的时候获取执行的流水线 name，留存记录，并在结束的时候带着该 name 回调 pipeline-x 。

   - 减少多数用户配置和使用的时间？提供标准的固定的流水线模板，如开发流水线、提测流水线等固定的执行动作。

   - 尽量覆盖多数用户且用户接入成本足够的低且减少整体时间？ 用户代码 push 之后即开始触发流水线。

   - 需要减少无用流水线等执行？ 同项目同分支等每次触发前终止之前未执行完成的任务，减少不必要开销。

   - 用户明确已知不触发流水线可在代码 push 的时候 message 中添加排除信息。

   - 接入哪些服务需要通过配置文件在配置中心配置，防止大范围的无意义信息骚扰用户。

- [ ] #### d.pipeline-adapters其他定制即兼容问题

1. 由于发布策略的问题，我们要求每次最新的发布都是从 master 拉取最新代码改动后，然后分支发布后合并回 master，但用户往往在push代码的时候没有合并 master ，这时候我们提供一个通用的解法来帮助用户自动合并（无冲突的情况）master 并提交。
2. 我们的自动化测试系统在执行过程中，是以环境的 ID 维度来进行的，但整体执行过程中信息的填充会是环境的名称，统一缺失了 Name 到 ID 的一个过程，则在整个执行过程中封装个特点的 pod ，将环境Name 转为 ID 供后续流程使用。

#### 10.3.3.3	UI层

- [ ] #### 考量


1. 对于配置层面，我们希望公司内采用相同的流程规范，因此开发流水线不支持用户自定义配置 stage 和step；执行过程，用户只需要关心执行的步骤及失败的节点即可，因此 argo 本身的界面已经满足我们的需求，不做二次开发。
1. 回到流水线的最终目标是提效，根据我们的流水线定义可以知道执行的周期还是相对较长，那么我们并不希望用户等待反馈，因此增加了 IM 消息结果反馈的方式，只是执行开始和结束关键节点通知，而且只是暴漏关键失败信息，提升问题排查定位的效率。

- [ ] #### 实现


1. 前端直接采用 argo 的界面。

2. 消息通知集成内部的 IM 消息在开始结束阶段推送消息通知，并包含关键信息。

   ![失败通知.png](/medias/images/deploy_dashboard/pipeline4.png)![执行过程样例.png](/medias/images/deploy_dashboard/pipeline6.png)

<center>图10-8 实现界面</center>



## 10.4	总结规划

### 10.4.1	总结

由流水线的落地实践我们有两点感悟：

1. 流水线能实现很好的编排组合，但是灵活性也意味着复杂度，所以当我们要落地的时候需要考虑主要解决的问题什么，如果是效率那么我们可以支持灵活定义，如果是标准那么最好就要流程统一，因此做决策的时候一定要参考自己的规模体量和主要诉求；
2. 建模好的流水线可以编排一切，但是实现的时候还是要考虑投入产出，比如线上的编排虽然能提升效率降低风险，但是它带来的效率相对于其他领域可能效益并不明显，所以我们就将其延后了，因此投入的时候还是要衡量好收益。

### 10.4.2	规划

目前我们已经完整的落地了开发流水线，而且公司的活跃应用都已经接入，而其他的流水线还没有开始，主要原因对于这种流程整合类型的方案效果较难量化，后续我们确定了量化指标后会继续进入。

其他方面我们已经搭建了较好的流水线编排引擎，因此后续我们可以识别具有流程和数据传递要求的服务进行流水线适配，探索更多的使用场景，比如 AIOPS 等。

----











# -----------------

# Part6	可观测性建设

目前大部分的企业都是采用微服务架构模式，它带来了一系列的便捷，同时也存在一些问题，因为大部分的功能系统都会拆分成多个微服务，随着业务的复杂，服务会被拆分的越来越多，像我们的机票订单系统服务依赖可以说是百级别的，不同的服务基本是不同的人、甚至是不同的部门维护，因此一旦出现问题，传统的监控技术和工具就很难跟踪这些分布式架构中的通信路径和相互依赖关系，更别提排查问题定位根本原因了，这对业务的损失也是无法忍受的，因此监控技术和工具的改革就势在必行了。

可观测性是云原生的一大原则，是云原生时代的必备能力，它的目的就是为了解决链路追踪问题定位的，它一般指的是metrics、logging、tracing的组合，本章我们将对这三个部分分别介绍，包括演进过程，难点坑点等。











# 第十一章	分布式链路追踪系统实践



## 11.1	背景

随着分布式系统架构的普及，系统越来越复杂，常常被切分为多个独立子系统并以集群方式部署在数十甚至成百上千的机器上。为掌握系统运行状态，确保系统健康，我们需要一些手段去监控系统，以了解系统行为，分析系统的性能，或在系统出现故障时，能发现问题、记录问题并发出告警，从而达到先于运营人员发现问题、定位问题。也可以根据监控数据发现系统瓶颈，提前感知故障，预判系统负载能力等。

在去哪儿旅行内部，拥有 Watcher 监控体系（ Watcher 包括业务线自定义监控指标，通用中间件指标 Knell 体系 )、报警体系（ Watcher 自带报警）、雷达（根据智能预测算法进行报警）以及日志体系(基于 ELK 的实时业务日志体系以及离线日志，和错误日志分析系统 Heimdall ），但是缺乏一个串联整体的分布式链路追踪系统。在众多开源的 APM 系统里面我们选择了自主研发，主要基于去哪儿网历史技术框架以及 JavaAgent 技术的实现，在整个实施过程中解决了系统大数据量高并发的性能问题以及 Trace 中断，和整个调用拓扑连通性的问题。

### 11.1.1	技术选型

在云原生的可观测性定义中包含了 Monitoring、Logging 以及 Tracing，如下图11-1所示，这三部分构成了云原生可观测性的三大基石。

![图片1](/medias/images/qtrace/图片1.png)

<center>图11-1 云原生可观测性的三大基石</center>

从图11-1中可以看出，APM 体系是贯穿于整个云原生开发运行过程中的，是对系统最直观的感受，那么我们要建设一套APM系统应该如何选型呢？

![WechatIMG132](/medias/images/qtrace/WechatIMG132.png)

<center>图11-2 </center>

1. **框架选型**

   目前有很多开源框架可以选择，监控我们可以选择 Prometheus 和 Grafana 的组合，这个组合已经在业界非常流行，只需要做一些内部的适配，比如和内部的系统关联，指标关联等等。

2. **日志选型**

   在日志这部分也是有很多选择的，从 ELK 体系到 Loki 有很多可以选择的组件，这里更多的问题可能是数据量的问题，因为每天海量日志的传输存储需要耗费大量的资源，包括存储、传输以及处理分析资源。这块的技术选型需要根据公司产生日志大小以及重要性进行分级处理，比如实时日志可以存储到 ES 或者Clickhouse ，一些非重要的日志可以压缩存储到 HDFS 。

3. **技术选型**

   最后就是 Trace 这部分的技术选型，国内比较火的框架有 Skywalking，是华为开源的一款 APM 工具，架构简单界面优雅，采用 Agent 插装的模式可以不改动现有框架代码自动增加 Trace 相关功能，非常适合一些体量中小的公司，因为这部分公司大量的采用了开源框架，而且数据量不是特别大，存储也可以有很多选择，Skywalking 适配了非常多的第三方存储。

   Jaeger 是国外比较火的一款 Tracing 工具，可以展示相关的调用链路，有一些简单分析，也是采用了 Agent 插桩的模式，不需要改动代码就可以实现 Tracing 的功能，整体来说各大 APM 工具的基本功能都是很全面的，更多的需要结合公司的一些技术栈来选择，在去哪儿旅行这边， Java 是使用最多的语言，其他还有 Python 和 Go，另外在历史的某个时段，也做过很多中间件的人工插桩，自研的分布式 RPC 调用框架，以及 QMQ，分布式配置系统 Qconfig 以及分布式任务调度系统 Qshcedule 都是进行过改造。基于这种情况，我们需要做的事覆盖一些常用的开源插件，就可以完成整个插桩工作。



### 11.1.2	架构设计

![image2021-8-26_17-49-51](/medias/images/qtrace/image2021-8-26_17-49-51.png)

<center>图11-3 架构设计 </center>

基于技术选型的分析，我们采用中间件插桩人工手写代码，其他开源中间件使用 Agent 插桩模式，这样可以快速构建 Traceing 的记录能力，数据传输层我们采用了 Apache Flume 组件以及 Kafka 作为我们的数据传输中介，使用 Flink 作为整体接受以及处理分析的框架，最终将数据存储到 Hbase 中。展示界面 WEB UI自研，前端采用了 React 框架，展示调用拓扑，调用链路，异常日志以及分析结果。

### 11.1.3	数据流程图

![image2021-3-12_18-13-55](/medias/images/qtrace/image2021-3-12_18-13-55.png)

<center>图11-4 数据流程图 </center>

1. **日志打印和收集上报** 

   Agent 部分主要是日志的打印和收集上报，这部分分为两个中间件，一个是本身产生 Trace 的中间件，另外一个是上报的中间件，目前产生 Trace 的中间件是包装现有的开源组件以及部分采用 Agent 的动态插桩实现。上报的 Agent 采用了 Aapache 的 Flume ，对于 Flume 进行部分改造，支持日志轮转不丢日志，针对行级别收集，以及在应用中心下发配置，配置收集不同的日志，不仅包括 Trace 的日志任何的实时收集日志都可以通过这个组件进行上报。

2. **日志上传**

   日志上传主要通过 Kafka 中间件，目前公司的所有的机器上报日志都是通过公共的 kafka 中间件上传，最终存储会有所不同，Trace 日志经过 Flink 进行聚合得到拓扑、和聚合的结果,拓扑和聚合结果都是在 FLink 分析得到的，聚合的结果主要包括失败的 Trace，超时的 Trace 等等，拓扑的数据主要是整个调用拓扑结构。

3. **监控上报**

   监控部分的上报主要是通过 Watcher 系统进行上报，在 Trace 系统里面进行统一的埋点，这样在 Trace 支持的中间件里面可以统一的打印监控日志，监控日志会推送到 Watcher 系统进行展示，报警配置也在 Watcher 系统配置。

4. **UI展示部分**

   通过存储到 Hbase 和 Mysql 的数据可以查询出来具体的 Trace 信息以及相关的拓扑结构，通过聚合结果可以查询到具体的错误量失败率，耗时最大的Span 等等信息。

5. **关联日志展示部分**

   这部分数据主要存储到 Clog，目前已经转移到数据组统一的ELK平台进行存储和查询关联。



## 11.2	实施遇到的问题&解决方案

### 11.2.1	中间件自编码 + 插桩JavaAgent

- [ ] ####      常用中间件的支持硬编码方式


支持内部中间件体系（ Dubbo QMQ QunarAsyncHttpClient QunarHttpClient Qshcedule Qconfig ）常见的开源通信中间件（ Apache http client 3 &4  版本  okhttpclient ），tcdev 4.0 一下版本采用硬编码方式支持如下列表中的中间件支持方案。

| 组件类型     | 组件名称                  | 组件版本       |
| :----------- | :------------------------ | :------------- |
| HTTP         | async-http-client         | 1.9.x 版本以上 |
| HTTP         | apache httpcomponents     | 4.0.x 版本以上 |
| HTTP         | okhttp3                   | 3.0 版本以上   |
| HTTP         | apachecommons-http client | 3.x 版本       |
| HTTP         | qmob-commons 内部版本     | 任何版本       |
| HTTP         | qm-common内部版本         | 任何版本       |
| HTTP         | meerkat-http 内部版本     | 任何版本       |
| Dubbo        | QunarDubbo版本支持        | 任何版本       |
| QMQ          | Qunar QMQ 版本支持        | 任何版本       |
| Qconfig      | Qunar 版本支持            | 任何版本       |
| Qschdule     | Qunar版本支持             | 任何版本       |
| Hystrix      | 开源版本支持              | 任何版本       |
| Mybatis      | 开源版本支持              | 3.2.3 版本以上 |
| JdbcTemplate | 开源版本支持              | 任何版本       |
| Webflux      | 开源版本支持              | 任何版本       |
| Node         | 开源版本支持              | node6 以下版本 |

<center>图11-5</center>

- [ ] ####       常用中间件支持 Java Agent 方式

​    tcdev 4.0 以后支持中间件的版本不仅包含上述列举的中间件，而且不需要通过硬编码的方式来进行支持，采用 Java Agent 探针的模式来支持，内部跨线程问题是导致中断的主要原因，在 Qtrace Agent 里面已经全面解决，包括 Jdk 提供的 Runnable Callable ExecuteService Future Rxjava Reactor java 等跨线程问题都已经解决。

| 跨线程场景      | 支持   | 备注                            |
| :-------------- | :----- | :------------------------------ |
| Runnable        | 支持   |                                 |
| Callable        | 支持   |                                 |
| Lambda表达式    | 不支持 | 如果lambda表达式，会导致断掉    |
| ExecutorService | 支持   |                                 |
| Rxjava          | 支持   | 指定scheular 会默认包装执行线程 |
| reacotr java    | 支持   | 会默认包装执行线程              |

<center>图11-6</center>

如果没有 Qtracer.wrap 的话，即使有 QtraceAgent 都会中断。

```Java
CompletableFuture<Integer> future = CompletableFuture.supplyAsync(new QTraceSupplier<>(()->{

    LOG.info("supplyAsync------"+QTraceClientGetter.getClient().getCurrentTraceId());
    return 1;
}));
  Integer i = future.get();
  LOG.info(String.valueOf(i));
  CompletableFuture<Void> future1= CompletableFuture.runAsync(QTracer.wrap(()->{

      LOG.info("runAsync------"+QTraceClientGetter.getClient().getCurrentTraceId());

  }));
  future1.get();

  executor.submit(QTracer.wrap(() -> {

      LOG.info("in lambda------"+QTraceClientGetter.getClient().getCurrentTraceId());
  }));

  executor.submit(new Runnable() {
      @Override
      public void run() {
          LOG.info("in lambda------"+"in runnable"+QTraceClientGetter.getClient().getCurrentTraceId());
      }
  });
```

<center>图11-7</center>

- [ ] ####        Java Agent 方式性能问题

​    性能问题主要体现在请求耗时、吞吐量是否收到影响，先说结论对于这两方面的影响大概在3%-4%之间。首先明确几个概念：

1. **裸请求**

   只未使用 agent ，未使用 Qtracer 组件包装，直接发送请求，不支持 qtrace 功能。

2. **使用 qtracer 工具包装 http 或者线程池**

   对于 http 拦截使用 Qtracer 组件给httpclient增加拦截器，对于跨线程使用 qtracer 组件包装线程池。这两种都是通过编码方式提供 qtrace 支持。

3. **使用 agent 拦截请求**

4. 未使用编码方式而是使用 agent 字节码修改拦截的方式去支持 qtrace ，用户不需要做代码修改。

### 11.2.2	实验一：对于Http请求的影响

|          停顿时间（ms）           | 10 ms  | 50 ms | 100 ms | 200 ms | 400 ms | 500 ms |
| :-------------------------------: | :----- | :---- | :----- | :----- | :----- | :----- |
|              裸请求               | 2013   | 1041  | 640    | 367    | 190    | 153    |
|              拦截器               | 1985   | 954   | 635    | 362    | 190    | 153    |
|               agent               | 1628   | 943   | 623    | 356    | 186    | 148    |
|           拦截器-agent            | 357    | 11    | 12     | 6      | 4      | 5      |
| agent吞吐量减少百分比（比拦截器） | 17.98% | 1.15% | 1.89%  | 1.66%  | 2.11%  | 3.27%  |
|           裸请求-agent            | 385    | 98    | 17     | 11     | 4      | 5      |
| agent吞吐量减少百分比（比裸请求） | 19.13% | 9.41% | 2.66%  | 3.00%  | 2.11%  | 3.27%  |

<center>图11-8 实验一</center>

1. **说明：**横轴 模拟业务请求耗时 10 50 100 200 400 500 ms 纵轴吞吐量
2. **结论：**请求耗时在50ms 以上，吞吐量影响在4%

| 停顿时间（ms）                      | 10 ms  | 50 ms  | 100 ms | 200 ms | 400 ms | 500 ms |
| :---------------------------------- | :----: | :----: | :----: | :----: | :----: | :----: |
| 裸请求                              |   39   |   75   |  123   |  214   |  414   |  514   |
| 拦截器                              |   39   |   82   |  124   |  217   |  414   |  512   |
| agent                               |   48   |   84   |  126   |  221   |  422   |  534   |
| agent-拦截器                        |   9    |   2    |   2    |   4    |   8    |   22   |
| agent平均耗时增加百分比（比拦截器） | 23.08% | 2.44%  | 1.61%  | 1.84%  | 1.93%  | 4.30%  |
| agent-裸请求                        |   9    |   9    |   3    |   7    |   8    |   20   |
| agent平均耗时增加百分比（比裸请求） | 23.08% | 12.00% | 2.44%  | 3.27%  | 1.93%  | 3.89%  |

<center>图11-9 实验一</center>

1. **说明：** 横轴模拟业务请求耗时   纵轴整体耗时 
2. **结论：**超过50ms 对于请求的耗时影响在4%以内 



### 11.2.3	实验二：对于跨线程的请求影响

-  **吞吐量比较**


| 停顿时间                               | 10 ms     | 50 ms     | 100 ms     | 200 ms    | 400 ms    | 500 ms     |
| :------------------------------------- | :-------- | :-------- | :--------- | :-------- | :-------- | :--------- |
| 裸请求 - agent                         | 111       | 14        | 3          | 21        | 3         | -4         |
| 裸请求                                 | 1424      | 980       | 538        | 340       | 187       | 146        |
| wrapper - agent                        | 42        | 6         | -4         | 16        | 3         | -6         |
| wrapper                                | 1355      | 972       | 531        | 335       | 187       | 144        |
| **agent吞吐量减少百分比（比裸请求）**  | **7.79%** | **1.43%** | **0.56%**  | **6.18%** | **1.60%** | **-2.74%** |
| **agent吞吐量减少百分比（比wrapper）** | **3.10%** | **0.62%** | **-0.75%** | **4.78%** | **1.60%** | **-4.17%** |
| agent                                  | 1313      | 966       | 535        | 319       | 184       | 150        |

<center>图11-10 实验二</center>

- **耗时比较**

| 停顿时间                         | 10        | 50        | 100       | 200       | 400       | 500        |
| :------------------------------- | :-------- | :-------- | :-------- | :-------- | :-------- | :--------- |
| 裸请求                           | 55        | 80        | 146       | 231       | 421       | 536        |
| wrapper                          | 58        | 81        | 139       | 235       | 421       | 545        |
| agent                            | 59        | 81        | 146       | 246       | 428       | 524        |
| agent - wrapper                  | 1         | 0         | 7         | 11        | 7         | -21        |
| agent耗时减少百分比（比wrapper） | **1.72%** | **0.00%** | **5.04%** | **4.68%** | **1.66%** | **-3.85%** |
| agent - 裸请求                   | 4         | 1         | 0         | 15        | 7         | -12        |
| agent耗时减少百分比（比裸请求）  | **7.27%** | **1.25%** | **0.00%** | **6.49%** | **1.66%** | **-2.24%** |

<center>图11-11 实验二</center>

- **结论：** 对于跨线程的场景Qtrace Agent 对其性能的影响非常低低于5%



## 11.3	Trace 的全链路治理

### 11.3.1	背景

1. 日志收集端性能不足，丢数据 。
2. 后台任务处理性能（3百万 QPS ）不足，数据延迟大。
3. 跨线程Trace 链路断开 。

### 11.3.2	日志组件的优化

![图片2](/medias/images/qtrace/图片2.png)

<center>图11-12 日志</center>

在 Agent 性能以及达到要求后，接下来我们对日志收集组件做了一次监控，发现整体Trace的完整率很低，有两个概念，一个事 unkownSpan 这个是我们构建出来整个 Trace 拓扑后缺失的 span，totalSpan 即所有的 Span，这两个的比值就是失败率，或者说不完整率，这个指标能说明在一个链路中，缺失了那些Span，这些 Span 是真实存在的。经过分析发现：

<img src="/medias/images/qtrace/WechatIMG134.jpeg" />

<center>图11-13 组件问题占比</center>

这个组件的问题占了50%以上的问题，继续分析每个 case 后发现核心问题是 Flume 性能跟不上日志的写入速度。

![WechatIMG135](/medias/images/qtrace/WechatIMG135.png)

<center>图11-14 组件问题占比</center>

通过分析发现丢数据的问题主要发生在异步写数据这块，因为后续 Sink 过程中发送的速率跟不上写入的数据量，导致了内存队列日志的堆积，堆积到一定程度就需要将数据抛弃，这样导致了数据无故消失，造成了最开始的现象。从图11-14中可以看到，只要我们对内存队列的大小做扩容，以及将 Sink 改为异步发送以及扩展 TailRead 的并发就可以将性能提升上来。但是这样做虽然性能提升上来了，这对 Flume 本身产生了巨大的压力。

![图片3](/medias/images/qtrace/图片3.png)

<center>图11-15 日志监控</center>

大量的日志需要从 Flume 组件上传，但是我们不能无限制增加其内存，日志收集组件的内存限制是通过 JVM 参数配置的，在这种内存受限的情况下如果大量的日志传输就会造成频繁的 OOM，可以看到图11-15中右下角的 OOM 监控显示，整个 OOM 是非常严重的，这也导致了大量的数据传输丢失，如何解决内存不够用的情况呢？增加内存显然不现实，不能占用过多系统内存，保证业务进程的使用。还要让日志稳定的上传上来--限流。



![图片4](/medias/images/qtrace/图片4.png)

<center>图11-16</center>

通过对整个传输过程做动态限流，将数据占用的内存容量控制在合理的范围即可，图11-16中通过 SlidingWindow 限制一个批次的 Size 大小，以及条数大小，以及单条日志的 Size 大小，这里为啥要限制单条，已经有全部的大小限制了。因为有些日志就一条就可以打爆你的内存，必须将这些日志进行截断，要不然会频繁的 OOM。通过这些限制就将传输组件的流量进行了精准控制，保证不出现 OOM 的情况。

但是事情往往不是想的那么简单，突发情况！

![WechatIMG136](/medias/images/qtrace/WechatIMG136.png)

<center>图11-17</center>

在部署整个日志收集组件的过程中触发了一次故障，业务系统的进程突然被Kill，这种情况迅速波及了其他服务，导致了一次“雪崩”，为什么我们部署了这个服务会影响到业务应用呢？而且我们明明做了内存限制？

在触发问题的系统中我们发现了奇特的现象。

![WechatIMG137](/medias/images/qtrace/WechatIMG137.jpeg)

<center>图11-18 案例</center>

可以看到17800这个日志收集进程竟然占用了197%的 CPU 资源，这台机器已经是下线了业务应用的机器，CPU 使用率本身不高，经过查看其他机器的 Flume资源使用情况，发现占用了大量的堆外内存资源，原因是需要发送大量 kafka 消息，这些消息会缓存在堆外内存中去，大量的堆积会导致整个的系统内存不足，最终 Linux 系统会 Kill 掉占用内存最大的进程，毫无例外都是业务的应用进程，至此算是了解其中原因，如何解决呢？

就是要对 Flume 进程做资源控制，CGroup 技术就是为此而生，通过设定进程的 CGroup 资源组，来控制日志收集进程的资源使用情况，来限制过度使用内存和 CPU 资源。最终整体失败率从80%降低到20%左右达到了预想的效果。

![图片5](/medias/images/qtrace/图片5.png)

<center>图11-19 案例</center>



### 11.3.3	海量日志处理能力

在处理完成日志上传的各种类型问题后，数据传输和处理的性能瓶颈逐步展现出来，首先是Kafka的性能瓶颈。出现的现象是在日常运行过程中 kafka 集群会出现剧烈的抖动，如图11-20所示。

![图片6](/medias/images/qtrace/图片6.png)

<center>图11-20</center>

通过监控发现了整个集群的网络空闲链接急剧下降，客户端连接数也急剧下降，造成大量的客户端连接超时。通过分析整个Kafka的处理流程发现，当网络空闲进程急剧下降的同时，Kafka 的写入进程耗时上涨严重。

![WechatIMG138](/medias/images/qtrace/WechatIMG138.png)

<center>图11-21</center>

从图11-21中可以看到红色标记的部分是 Processor 的数量大量降低，原因是在 RequestChannel 中处理任务的 KafkaRequestHandler 性能下降导致，这个Handler 主要负责写入数据和索引，当磁盘 IO 达到机器性能瓶颈的时候就会导致这种情况。那么我们就需要优化写入性能，分散更多的 Partition 以及升级相关写入慢的磁盘从机械硬盘到 SSD。

解决 Kafka 的问题后任务处理又是一块非常重要的模块，这个模块主要是处理所有的数据，包括解析，分析、与根据各种维度的聚合，需要聚合相关的拓扑以及各种维度的索引。

![WechatIMG139](/medias/images/qtrace/WechatIMG139.jpeg)

<center>图11-22</center>

平均 QPS 200W 左右 峰值300w，在巨大的流量涌入的同时，最常见的问题就是背压。

背压出现的原因是下游任务处理能力不足，如何优化背压？

1.subTask 是否消费均匀，yarn集群分配资源是静态分配，导致运行期资源不足。为什么需要看子任务是否处理数据均匀？

如果在分配任务的过程中数据存在倾斜，数据不均匀，会导致整体任务处理缓慢，部分算子背压严重，严重影响整个集群的处理速度。

2.算子的 input 和 output 是否一致 内存是否充足

在发布 Flink 任务的过程中需要设置内存大小，此处就需要评估上下游的算子数量以及传输数据量大小，如果配置内存过小就会导致堆积。

3.使用内存 map 替代 window 。

部分情况下，只需要缓存部分数据的情况可以采用内存 map ，性能会大大超过 window 的性能。但是存在丢失数据的风险需要根据情况评估是否可用。

4.filter 一定小心下游算子的拥堵导致全面的拥堵，压缩算子传递数据，使用 shargeGroup 共享 JVM。上下游的算子如果配置 shareGroup 则可以共享 JVM，这样避免多余的网络传输，提升整体的执行效率。

![WechatIMG140](/medias/images/qtrace/WechatIMG140.png)

<center>图11-23</center>



## 11.4	总结

整个分布式链路追踪系统，从技术选型、架构设计以及落地的过程中，遇到了很多结构性问题和性能问题，从问题分析到定义指标最终解决问题，积累了很多经验，不通类型的问题都是可以通过定义指标、数字化指标、分析问题、逐步拆解、最终分治解决，希望对大家在建设 APM 系统的过程中有所帮助。











# 第十二章	日志诊断

## 异常统计分析服务—Heimdall



## 12.1	背景

随着业务发展和微服务架构的普及，企业内微服务拆分粒度越来越细，服务间调用关系错综复杂。对于一些复杂的，比如机票和酒店售卖业务场景，可能动辄涉及上百个应用，当某个系统发生异常时会导致多个服务受到影响。此时 APM 系统就派上了用场，监控（Metrics）、调用链（Tracing）、日志（Logging）帮助业务同学快速定位问题。普通的业务监控报警能起到快速发现问题的作用，但具体case的排查还需要研发人员通过异常栈信息来分析，比如数据库连接异常、空指针等等。

去哪儿网很早就有了监控系统 Watcher，能够起到快速提醒业务响应异常的作用，然后开发同学排查是接到报警的系统本身的问题还是下游依赖的系统的问题，如果是下游系统的问题，就要这样一层层地找下去，有时候定位问题时间会比较长。当某个系统出现问题时最根本的表现就是产生异常，如果能直接提示开发同学系统产生了新的异常，或者异常量上涨了，就能够大大缩短开发同学排查问题的时间，做到快速恢复故障。

去哪儿网有一套完整的日志收集和查看体系，首先应用通过日志打印框架将日志打印到本地 log 文件，机器上默认安装日志收集的 agent，将日志内容通过kafka上报，再通过 ELK 提供日志存储和查询的能力。

如果能够自动地、快速地识别异常，并将日志堆栈内容直接提醒给研发同学，将会大大提高解决问题的效率，甚至防患于未然。因此，异常统计分析系统——Heimdall 应运而生，主要目标如下：

1. 分钟级别的异常统计
2. 发布过程中展示同比环比
3. 支持添加监控报警
4. 支持用户自定义时间范围查询
5. 能够展示异常栈
6. 支持应用和机器级别的异常统计

整体的演进包含两个阶段：

1. 基于实时日志收集的建设

2. 基于基础组件的改造，以在业务服务端直接拦截并上报异常的方式进行了改进

以下分阶段进行阐述。



## 12.2	实践框架

### 12.2.1	阶段一：基于实时日志收集的建设

- [ ] #### 技术栈


实时日志收集 kafka+大众点评开源工具 CAT+FLINK+Heimdall 平台

- [ ] #### 架构图


![image](/medias/images/heimdall/heimdall_1.png )

<center>图12-1 架构图</center>

- [ ] #### 核心模块介绍

1. **clog 模块**
   clog 模块主要负责异常栈的接收、存储和查询。消费 kafka 消息接收日志，解析出 ERROR 级别的日志，并将其关联的应用、trace、日志详情、时间戳等相关信息一并存储起来，再将转换成统一格式的日志以 kafka 消息的形式发出，待 flink 任务消费并做进一步解析。clog 的存储结构分为三层：本地磁盘（临时存储）+ES（做文件索引）+HDFS（持久存储）。这样的存储结构保证了热数据的快速查询和冷数据的持久存储。

   - **存储：**首先全部的实时日志都会按 bu 发送到 kafka，clog 以二进制流的方式消费此 kafka 消息，然后经过 MessagePackDecoder 进行解析，解析出日志级别、AppCode、traceId等信息，组装成固定格式的日志内容。再由 BlockProcessor 为每个 DataBlock 构造出索引用于查询，EsIndexManager 将索引保存到 ES 中，数据部分 DataBlock 保存到本地文件，定时转存到 HDFS 做持久存储。

   - **block position格式定义：** ip-保留天数-时间（ yyMMddHHmmss ）- offset，例如：10.xx.xx.xx-7-20211110185535-4625901837。

     每个应用 5s一个block，一个 block 最大8M，当本地磁盘空间利用率达到75%，就上传到 HDFS。

   <img src="/medias/images/heimdall/heimdall_es.png" >

   <center>图12-2 存储架构图</center>

   -  **查询：**查询异常详情时先查询 ES，得到索引，根据索引判断本机是否存在相应的 blockPosition，如果是本机ip 并且本地磁盘中存在，直接从磁盘读取数据返回，若是本机 ip 但磁盘中不存在，根据文件名、position、seconds 等信息查询 HDFS 。如果不是本机 ip ，则向索引中的 ip 发起远程 HTTP 请求，转化成对应 ip 的查询。

   ![heimdall_flow](/medias/images/heimdall/heimdall_flow.png)

   <center>图12-3 查询流程图</center>

2. **flink 任务模块**
   flink 任务主要用来进行异常信息的解析计算处理，将异常类型、应用、机器等相关信息提取出来，按分钟级别做次数统计，并打印异常指标到监控系统。

- [ ] #### 难点分析


1. **实时日志收集的日志量巨大—消耗资源大**

  由于实时收集的日志本身是不过滤日志级别的，大量的非 ERROR 日志也会被收集。从使用角度上，这些非 ERROR 的日志并不是用户关心或者期望看到的数据，纳入异常日志统计并没有什么用反而会造成干扰，所以需要从大量日志中过滤掉非ERROR日志，这会耗费大量的计算资源。还有一部分是多个系统间传递数据，消耗在了跨系统传递无用信息的宽带上。

2. **实时日志存在延迟—flink数据统计不准确**

  由于日志收集属于非核心流程，当应用的日志量较大的时候，实时日志收集存在延迟的情况，有些日志的延迟甚至超过了1个小时。在异常日志统计时使用了 flink 的滚动窗口来进行计算，由于日志的乱序和部分日志延迟，导致这些日志被丢弃，造成统计数据不准确，误差将近10%。

3. **未考虑环境隔离—掺杂仿真环境数据**

  公司内部根据不同使用目的和途径，存在多种不同的环境，包括 beta、仿真、灰度和线上，实时日志收集没有对环境进行区分，仿真、灰度和线上的日志都会被统计，而事实上仿真环境属于测试范畴，会对统计结果造成干扰，尤其是当短时间内进行大量自动化测试且引发异常的情况发生时，干扰会更加显著。

4. **非全量应用都有实时日志收集—有些应用不能使用此功能**

  由于公司内整套实时日志收集是 ELK，成本比较高，所以只有部分核心应用开通了实时日志收集，未开通的应用就没办进行异常日志统计和监控。

5. **容器化后日志收集方式改变—容器化后的应用统计不到数据**

  近两年去哪儿在进行 KVM 到容器化的迁移，两者技术差异还是比较大的，日志收集的方式也进行了彻底的改变，包括 kafka 消息的形式和格式都变化较大，原有异常日志统计架构已经完全不能满足。也正因此，我们做了一次系统架构调整，从源头异常日志收集到统计逻辑都做了重大调整。

### 12.2.2	阶段二：基于基础组件的改造

- [ ] #### 改进目标


1. 支持容器的异常日志统计。
2. 解决统计不准确问题。
3. 降低资源成本。
4. 应用范围要扩展到全司 java 应用。
5. 可以按照环境类型维度进行过滤。

- [ ] #### 改进策略


1. 将数据源从实时日志收集改成在业务服务端的基础组件进行拦截和上报异常。
2. 将从全量级别的日志中筛选异常日志改成直接在源头过滤，只上报异常的日志。
3. 在基础组件在业务服务端直接做好结构化，并做初步聚合（按异常类型做聚合，同种异常次数聚合，异常栈详情采样），减少冗余数据传输的资源消耗，kafka 集群 partition 从60个降低到14个，异常日志每秒消息量从486K 降低到 53K。
4. 废弃 flink 任务（之前使用 flink 主要是做日志文本解析，数据源变更后，就不再需要了），开发新的统计服务。

![image](/medias/images/heimdall/heimdall_2.png)

<center>图12-4 改进后的架构图</center>

- [ ] #### 核心模块介绍

- **logger-spi**

  负责在客户端进行日志采集、过滤、聚合、采样、上报。通过 agent 对 logger 进行插桩，并过滤出带异常栈的日志，然后将异常日志按照异常类型进行初步聚合，将1min 内同一种类型的异常进行累加计数，并且对异常日志详情进行采样，最终将数据通过 kafka 消息上报。同时为了避免对服务造成过多损耗，当占用的内存达到限额时会直接上报到 kafka。

  我们将异常日志分为了业务异常（ BusinessError ）和系统异常。业务异常是指没有异常栈的，和业务流程相关的异常，比如："没有该目的地的航班"等；系统异常是指没有系统业务含义的，带堆栈信息的异常。目前我们只关心系统异常，业务异常是直接过滤掉的。

  上报的数据结构如下：

```java
{
"ip": "xx.xx.xx.xx",
  "sendTime": 1634802724460,
  "host": "l-xxxxxxx",
  "appCode": "xxx",
  "envName": "proda",
  "exType": "com.xx.xx.xx.xx.xxException",//无异常栈的直接写BusinessError
  "count": 100,
  "details": [
    {
      "level": "ERROR",
      "fileName": "/home/xxx/logs/xx.log",
      "content": "2021-10-21.15:52:04.040 INFO  [Dubbo-thread-57] ProxyUtil.proxyConnection:38 [proxyConnection] QTraceId[xxx_211021.155203.xx.xx.xx.xx.6786.xxx_1]-QSpanId[1.7.1.5.1.1.3.1.13.1.3.1] db proxy error",
      "timestamp": 1634802724458,
      "traceId": "xxx_211021.155203.xx.xx.xx.xx.6786.xxx_1",
      "stackTrace": "com.qunar.xxx.QProcessException: api|BookingRule Error: [预定规则错误, 最大可预订间数<=0, maxRoomNumber=0]
        at com.qunar.xxx.xxx(xxx.java:197)
        at com.qunar.xxx.xxx(xxx.java:117)
        ...
        at org.apache.dubbo.remoting.transport.dispatcher.ChannelEventRunnable.run(ChannelEventRunnable.java)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)"
    }
  ]
}
```

<center>图12-5 数据结构</center>

模块详细架构图：

![image](/medias/images/heimdall/heimdall_3.png)

<center>图12-6 模块详细架构图</center>

- **heimdall-statistic**

  接收客户端上报上来的异常日志初步聚合结果，在内存中按分钟进行统计并暂存储统计结果，并定时更新到hbase中，更新时先从 hbase 中查询出该应用、该分钟原有的异常数据，再与内存中的数据叠加，最后更新到 hbase 中。这种计算、统计方式与 kafka 消息到达的顺序无关，不管消息有没有延迟，只要消息没丢，就都能统计进去，从而保证统计数据和实际不会有偏差。统计数据包含以下四个维度：

  - 每分钟的异常个数

  - 每分钟每种类型的异常个数

  - 每分钟每个机器的异常个数

  - 每分钟每个机器每种异常类型的异常个数



## 12.3	应用场景

基于我们提供的异常统计和详情查看等基本能力，公司内部还产生了帮助业务系统定期进行服务治理的工具。

### 12.3.1	服务治理

将 AppCode 和 owner 对应起来，每周发邮件提示系统异常量，并制定规范，比如哪些类型异常必须修复，便于owner持续关注自身系统的健康状况。

### 12.3.2	与发布系统集成

在发布过程中，发布系统会调用Heimdall接口获取该应用及其上下游系统的异常量变化情况，便于在发布过程中有问题及时发现、及时终止。

### 12.3.3	异常检测报警

不仅是在发布过程中要关注异常量，在平时也会出现各种各样的突发情况，比如硬件故障、中间件故障、数据库故障、攻防演练等。因此基于 Heimdall 的基础数据，开发了实时地根据异常统计数据自动识别新增异常类型和异常数量环比上涨，并及时提醒给研发人员的工具，想要开通的应用可以自定义配置报警规则，比如环比上涨多少要报警等。能够进一步提升系统稳定性和研发人员排查问题的效率。



## 12.4	总结展望

  目前 heimdall 系统已经接入1300+应用，成为了研发质量中的一个重要指标，研发人员也养成了关注系统异常情况的习惯，为公司业务稳定发展提供技术保障。

  一个系统在诞生的时候基本上都会有一些没考虑到的点，并且随着周边环境的变化，原有的设计也会不满足，优秀的系统不是一成不变的，而是慢慢打磨、优化、改进、完善才形成的，每个时期和阶段都有它的价值。同时也要敢于突破原有设计的束缚，取其精华去其糟粕。

  异常量统计数据和异常堆栈的作用远不止于此，未来我们可以将异常日志中的 trace 信息利用起来，根据一条 trace 直接把该链路上的异常直接展示给开发人员，不需要一个系统一个系统地排查下去，提升排查问题的效率，未来还大有可为！











# 第十三章	异常诊断

**一站式Java应用诊断解决方案 - Bistoury**



## 13.1	背景

在线应用的诊断一直是日常维护中的难点和痛点，2018年下半年，Alibaba 开源了 java 应用诊断工具 arthas ，并多次登顶 GitHub Trending 。作为基础架构团队，我们自然也不会忽视这个东西。在研究后发现，arthas 确实是一个非常优秀的 java 诊断工具，但是也有一些不足。

1. **arthas 更像是一个工具，而不像一个产品**

   如果要使用它，我们首先要登录相关机器，然后在机器上下载 arthas，再执行一些命令来运行。这整个流程里，下载可能出现问题，运行 arthas 也需要具有目标进程相应的权限，还需要先看看对应进程 id 等等...这些确实只是一些小问题，但我们也可以选择让这些问题不存在，让整个使用过程更加流畅。

2. **arthas 缺少 web 界面**

   命令行界面用起来确实很酷，但不可否认在相当一部分情况下web界面更直观更友好，很多需要查文档的情况在 web 界面下都可以直接操作，降低了使用门槛。

3. **arthas 所有功能都针对单台机器**

   实际上很多时候我们需要考虑和观察整个应用的运行情况，需要一个应用级的视角。

4. **arthas 是一个独立的工具**

   对于一个开源工具，这不是一个缺点，但如果能和公司内部的应用中心、发布系统等做一些适配的话，在使用上会更方便，也能够做出一些独立工具做不到的功能。  

基于以上的原因，我们决定做一个更强大、更好用的java应用诊断工具 - Bistoury。Bistoury 集成了 arthas ，拥有 arthas 的所有功能，并提供了在线debug、线程级 cpu 监控等 killer feature，目前也已经在 github 开源。



## 13.2	设计与实现

这一节内容首先会对 bistoury 整体设计进行介绍，让读者对 bistoury 有个大体的认识，接下来再对一些重点的设计和功能点进行说明。

### 13.2.1	整体设计

Bistoury 涉及到的组件有用户系统、agent 、proxy 、ui 、注册中心、负载均衡器、应用中心等，其中agent 、proxy 和 ui 直接归属于 bistoury，其它都属于外部系统。  

1. 用户系统就是待诊断的正在运行的应用。

2. agent 和待诊断用户系统部署在同一台机器上，接收来自 proxy 的命令，并根据其类型直接执行一部分命令，并负责另一部分命令与用户系统之间的交互。

3. proxy 负责维护与 agent 之间的长期 netty 连接，并以 websocket 的方式维护与 ui 在命令执行期间的连接，它将ui传来的命令发送给具体的一个或多个agent ，并且根据网络情况对对应的连接进行管理。

4. ui 则提供图形化和命令行界面，接收用户请求并发送给 proxy，并将结果展示给用户。

5. 注册中心负责 proxy 的注册，ui 通过注册中心获取 proxy 地址。

6. 负载均衡器负责 agent 到 proxy 的负载均衡，agent 在每次启动时通过负载均衡器获取单个 proxy 地址，并与之建立长期连接。

7. 应用中心提供应用和机器的相关信息给 bistoury，用于简化操作，并提供一些特殊功能需要的信息。

8. 其它：bistoury 还会访问 gitlab 等系统，但这些都是具体功能所需要，并不在 bistoury 整体的系统设计上。

   


   ![design](/medias/images/bistoury/input.png)

   <center>13-0 一次请求</center>

![output](/medias/images/bistoury/output.png)

 <center>13-1 一次响应</center>

### 13.2.2	字节码插桩

Bistoury 的大量功能都使用了字节码插桩，同时因为 bistoury agent 与应用系统分开运行，所以使用 agentmain 方式动态 attach 到正在运行的用户系统上。  

字节码插桩和 agentmain 在网上有大量文章，这里不再进行具体说明。

### 13.2.3	ClassLoader设计

ClassLoader 的设计是 bistoury 系统中非常重要的一个部分，也是 bistoury 能够透明升级，各项功能能够正常运行的基础。Bistoury 集成了arthas，ClassLoader 的设计也和 arthas 存在一些一致的地方，但只说明差异反而无法说清楚整个设计，这里会一起说明。
下图13-2所示的是 bistoury agent 动态 attach 后，应用内部的 ClassLoader 结构图，其中 bistouryClassLoader 是一个 Bistoury 专有的 ClassLoader。  

![simple_classloader](/medias/images/bistoury/simple_classloader.png)

<center>classloader结构</center>

<center>13-2 </center>

- [ ] #### BistouryClassLoader

可以从上面13-2图中看到，BistouryClassLoader 加载了attach jar，这个jar包含了 Bistoury 各个功能具体实现以及依赖 jar 包。
为什么要使用一个专有的BistouryClassLoader呢，这是因为attach jar中包含的各个jar包在用户系统中也可能存在，如果版本不一致很可能会出现问题；bistoury agent 会进行升级，它的功能实现代码、依赖的 jar 包都可能变化，需要对它们的影响范围做一个限制；用户系统可能非常稳定，甚至一年都没有重启过，而agent 可能在这一年中升级了几十个版本，每个版本都需要在用户系统里面加载一大堆类，这些jar包和类都需要进行卸载。

配合从 agent 加载到 BootstrapClassLoader 中的 instrument jar，每次 agent 升级或卸载时，做完清理工作后将 instrument jar 中的 ClassLoader 引用重置，就可以将整个 BistouryClassLoader 和里面的 attach jar 回收。

- [ ] #### MagicClassLoader

下图是完整的 bistoury 的 ClassLoader 结构图，可以看到比前面的 classloader 结构图多出来一个 MagicClassLoader，这是 bistoury 中的一个特殊的ClassLoader，用来解决同名类加载优先级问题。

首先来说一说这个问题的场景。在 bistoury 的开发过程中，为了满足需求，发现需要对 arthas 和 jackson 的源码的进行少量修改。可以选择的解决方案有自己 fork 一个分支，针对 jackso n也可以选择不使用序列化框架自己写一个。但要修改的代码比较少，笔者不想大动干戈也不想长期维护fork分支，只想要简单依赖j ar 包就好，于是就有了 MagicClassLoader 的出现。

MagicClassLoader 作用是 bistoury 可以指定一些类，把这些类委托给 MagicClassLoader 加载，MagicClassLoader 会优先加载 Bistoury-magic-classes.jar中的类文件。这样的话，只需要把需要修改源码的少量几个类放入 Bistoury-magic-classes.jar，就可以达到修改 jar 包中源代码的目的。  

![classloader](/medias/images/bistoury/classloader.png)



<center>13-3 完整classloader结构</center>

### 13.2.4	在线debug

一直以来，调试都是在线应用的痛点。

曾经在微博上流传着这么一个程序员才懂的笑话，NASA 要发射一个新型火箭，火箭发射升空后发现不行，NASA 把火箭拖回来加了两行 log，再次发射，发现又不行，又加了两行 log 发射，发现又不行... 

当然这只是一个笑话，但这样的场景在我们的实际开发中却屡见不鲜，多少次我们解决故障的时间就在不断地加 log，发布，加 log，发布的过程中溜走。 
Arthas 的 watch 命令让我们可以观察函数的入参、返回值、异常等等，然而似乎每次watch都需要看看文档里参数该如何设置，面对函数中的本地变量也是无能为力，特别是行数较多的方法，方法内部的情况还是难以明了，想象一下面对上百行的方法，你需要脑补出其中各个本地变量值的情形，这个时候，我们需要的是ide 的 debug 功能。 

Bistoury 的在线 debug 功能正是针对这个场景而生，它模拟了ide的调试体验，在功能上和远程调试，或者说你在 ide上debug 本地代码几乎一致。你在代码某一行打一个断点或条件断点，断点触发就能看到本地变量、成员变量、静态变量以及调用栈；与 ide 远程 debug 不同的是，它不需要在系统启动就带上调试相关参数，对应用完全透明，同时在断点触发时不会暂停整个系统，而是只打印断点处快照信息，打印后继续执行代码逻辑，完美符合我们对在线应用的 debug 需求。

- [ ] #### 原理

下面两段代码表现了在线 debug 打断点前后的差异。  

```java
userSystem.preDo(); 
userSystem.do();
userSystem.afterDo();
```

```java
userSystem.preDo();
if (hitBreakPoint()) {
    captureSnapshot();
}
userSystem.do;
userSystem.afterDo();`
```

当用户添加断点后，bistoury 会在断点处添加字节码，判断是否需要触发断点，捕捉断点处上下文信息。 

函数的入参、返回值、异常、静态变量等信息我们通过 arthas 也可以获取，bistoury 更进一步的是获取到了本地变量的信息。 

这里涉及到两个问题：断点设置在源码处，如何对应字节码里的位置；这个位置有哪些本地变量，它们的名字和值如何获取。 

通过查阅 java 虚拟机规范，我们可以发现，java类字节码里用来表示方法的 method_info 结构有一个 code 属性，code 属性的属性表里有一个叫做LineNumberTable，这里用 java 代码来近似描述 LineNumberTable 的一部分结构：

```java
class LineNumberTable {
    LineNumber[] lineNumbers;
}
class LineNumber {
    short start_pc; // 方法body字节码数组的索引
    short line_number; // 源文件的行号
}
```

可以看到，每一次源文件行号发生变化都会在 lineNumbers 里添加一条记录，那么我们可以对字节码文件进行一次扫描，就可以得出每一个源文件行号所对应的字节码索引范围，也就知道了字节码改添加在哪里。

同样是在 code 属性的属性表中，我们还可以找到一个名为 LocalVariableTable 的属性，同样用 java 代码来对其中一部分结构进行描述：

```java
class LocalVariableTable {
    Varible varible;
}
class Varible {
    short start_pc; // 变量在body字节码数组的起始索引
    short length; // 变量在body字节码数组存在的长度
    short name_index; // 变量名的索引
    short descriptor_index; // 变量类型的索引
    short index; // 变量在局部变量表的索引
}
```

可以看到，变量的范围就是[start_pc, start_pc + length)，而通过 index 字段我们可以获取到变量的值。 

结合前面的 LineNumberTable 信息，就可以获取断点处有哪些本地变量，达到获取断点处本地变量信息的目的。  



## 13.3	结尾

本文对 bistoury 的开发背景和整体设计做了简单介绍，并对一些重点设计和功能实现进行了具体描述，希望能给读者在java应用的诊断方面带来一定的启发。











# 第十四章	监控告警



## 14.1	背景

### 14.1.1	行业的监控告警有哪些

对于当前的互联网企业来说，监控是必不可少的，不管你的应用运行在物理机还是虚拟机或者容器上，你都需要知道它是否在正常的提供服务，它当前有没有一些异常的状态，它所运行的环境当前是否是稳定的，依赖的外部资源是否正常等等。

而目前整个业界来说，开源的监控系统也越来越多，不同的系统针对的侧重点和特性也不同，像 Zabbix / Nagios  这种老牌的监控系统侧重于主机系统层监控和告警，比如 Zabbix 和 Nagios 都自带有一套完善的系统层面监控插件，而且还允许运维很方便的利用 Shell 脚本或任何其他脚本语言来扩展自己想要的插件。同时 Zabbix 还提供了比较便利的 Discovery 功能，创建一套模板后，便能自动发现和检测相应主机状态，省掉了繁琐的配置过程。而 Graphite/Prometheus 这样的则更兼顾业务应用层监控，它们提供了一套机制，应用可以在代码里记录自己在运行时的状态数据，然后通过 Exporter 或者 Push 的方式将状态数据暴露或推送到 Server 端，Server 将数据存储在时序 DB 中用于之后的分析、查看和告警等。

![image01](/medias/images/monitor/monitor01.png)

<center>图14-1 行业产品举例</center>

很多企业在用开源软件的一个路径大概都是这样的，纯开源使用 → 少量的定制化开发或外层封装 → 深度的二次开发 → 自研。 

去哪儿的监控平台早期的时候用的也是用的纯原生方式，是用 Nagios+Cacti 的方式结合来做的。Nagios 用作基础设施的监控和告警，Cacti则用来看一些趋势图。随着之后的发展这种形式已经不能满足我们的需求，在2014年的时候我们开始设计自己的监控系统，从用户角度看我们要提供统一入口的一站式的监控平台，避免开发在各个系统间跳来跳去，要更友好的支持应用监控，要有更便捷的指标查看和快速定位，要更方便的让用户自己配置告警以及告警升级之类的告警操作等。 基于此我们开发了图14-2的 Watcher 监控系统。

![monitor02](/medias/images/monitor/monitor02.png)

<center>图14-2 去哪儿的Watcher监控系统</center>

Watcher 是去哪儿网内部的一站式监控平台，目前每分钟收集存储的指标总量在上亿级，检测和处理的报警量是百万级的。它的开发方式是部分主要组件在开源软件之上进行的深度二次开发加上部分自研组件的方式完成的。由于开发较早当时 graphite 比较流行，因此后端收集和存储的选型是用的 graphite+whisper 做的二次开发，前端控制面则基于 grafana 做的二次开发。因为考虑到数据量比较大，所以我们要求监控体系中任何组件都能够很好的水平扩展。

### 14.1.2	去哪儿监控告警演进

Nagios 自身有比较优秀的监控插件扩展系统，很方便运维或开发自己去扩展监控，而且 Nagios 的告警策略也比较丰富，可以自定义通知策略和通知升级之类的告警操作，但是 Nagios 自身并不能绘图，也不能查看数据历史，因此早期监控如图14-3所示是割裂的，报警和看图是分别在不同的系统。

![monitor03](/medias/images/monitor/monitor03.png)

<center>图14-3 Nagios</center>

这里有几个明显的痛点：

1. 监控面板和告警面板是割裂的，甚至在不同系统，开发要想查一个问题，经常需要在不同的系统间跳来跳去。
2. Nagios 和 Cacti 的扩展性都不好，不能进行方便的横向扩展。
3. Nagios 的告警配置比较复杂，学习成本搞，因此需要专门的人来修改配置文件 然后 reload 才行，开发是不能自己上去配置的，这样要加一个告警效率太低了。
4. 外部系统无法与 Nagios 进行联动来动态的处理告警问题，比如在发布期间我们想临时把相应的告警关掉发布完成后在打开告警。

首先为了解决专人手工配置告警和外部系统联动的问题，我们引入了 Nagios-Api ，并在 Nagios-Api 之上封装了自己的配置管理和告警处理。 Nagios-Api 自身也是一个开源项目，Nagios-Api 通过与 Nagios 自身暴露出来的 nagios.cmd 管道文件与 Nagios 通信，可以执行一些 Nagios 的命令，如开关报警、设置downtime（ schedule_downtime ）、取消downtime（ cancel_downtime ）等。

配置管理则是由一个应用监听配置请求，当有配置变更请求时，动态变更配置更新 Nagios 配置文件，在一个窗口期内去 Reload Nagios 使其生效。这个阶段我们的监控系统变成了图14-4这样：

![monitor04](/medias/images/monitor/monitor04.png)

<center>图14-4 监控系统变更</center>



除此之外，早期去哪儿快速发展阶段，各个部门的业务、应用和机器也都在快速扩张，这个时候为了不把鸡蛋放在一个篮子里，也为了降低单系统压力横向扩展，就给各部门部署一套这样的监控系统，因此我们的监控架构就变成了下图14-5这样：

![monitor05](/medias/images/monitor/monitor05.png)

<center>图14-5 监控系统变更</center>

到了这个阶段也暴露出了一些问题：

1. 多套系统对于运维和后期开发维护成本都增大了不少.
2. 监控面板和告警面板仍然是割裂的，并且在不同系统中，对于开发排查问题难度很大。
3. 多系统之间数据不互通，没办法基于此做一些数据分析类的动作。
4. 应用监控的需求并不能很好的满足，Nagios毕竟擅长的是系统层监控。



## 14.2	打造符合需求的企业级监控平台

由于背景中遇到的一些痛点，在原有开源监控已经不能满足我们的需求了，因此我们需要考虑建设自己的监控平台。这个平台要能解决几个问题：

1. **提供一站式监控告警的能力**
   提供统一的监控告警查看和配置页面，不需要让用户在使用或排查问题时跳来跳去。

2. **支持系统层监控的同时，提供更友好的应用层监控能力**

   支持用户能够自定义自己的监控面板和告警配置，不需要特定的人来管理面板和配置，同时要有权限划分。

3. **使用简单**
   因为用户是面对全公司的，其中有技术同学和非技术同学比如产品、运营，所以要避免过高的学习和使用成本。

除了这些之外还要考虑一些非功能性要求，比如：要横向扩展能力强，平台不能成为最后排查问题的瓶颈；数据高可用，不能因为挂一台机器导致数据丢失。

### 14.2.1	平台核心功能设计

watcher 作为当前去哪儿使用的监控平台，目前监控的应用数三千以上、主机+容器量在十万左右、指标数上亿级、配置的告警量百万级。
前面介绍过，由于前期多个系统，造成我们的数据和控制面都是分散的，开发查指标查告警都是极不方便，所以 Watcher 基于开源的 Grafana 做了深度的二次开发，统一整合了监控和告警的配置和查看。由于 Grafana 本身就提供了很优秀的监控绘图能力和多数据源的支持，并且是非常的灵活，允许用户从各种数据源选择指标数据绘制各种类型的图表并保存下来。但 Grafana 在企业的使用中也是会有一些不便的，比如：

1. **查看指标不够便捷**

   早期的 Grafana 是没有 Explore 功能的，用户在 Grafana 中查看指标必须要先新建一个 Panel，然后在 Panel 中输入对应的查询表达式才能看到自己的指标，这对于有很多指标，并且只是临时性的查看一次的这种场景来说就极为不便了，因为你不会为每一个指标都创建面板的，尤其是一些异常类型指标，这类指标值需要在异常时才有查看的需求。

2. **面板管理不方便**

   新版的 Grafana 给面板增加了目录和标签功能，但是对于面板比较多组织架构比较深的时候管理仍然不够便捷，watcher 开发了面板树来管理面板和面板权限，同时由于树形结构权限也可以很好的继承。

3. **没有模板功能**

   比如在一些主机监控的场景中，我们要查看主机的监控内容基本是一样的，比如主机的 CPU 、内存、Load 、网络、磁盘等等，在这种场景如果每台机器都要手工的配置出来一个面板，那工作量还是很大的，即便是使用 Variables 能力（早期叫 Templating ），在主机量很多时，在下拉框里去搜索使用也很困难。

4. **报警功能无法满足企业需求**

因为这些原因我们还是选择对 Grafana 做二次开发，下面介绍下我们平台核心功能设计。

- [ ] ####  树形结构管理面板


![monitor06](/medias/images/monitor/monitor06.png)

<center>图14-6 树形结构管理面板</center>

如上图14-6看到的，我们将 Grafana 扁平化的面板管理改造成了目录树结构的面板管理，这在面板比较多的时候非常有用，而且层级分明对于人记忆和查找都比较友好，同时也提供了搜索功能，可以针对关键字快速搜索自己的面板。同时可以给目录树做更细致的权限管理，比如是否仅查看，查看和编辑，还是无权限访问等，这些权限也可以从父节点继承到子节点。

- [ ] #### 升级版的Explore


其实严格来讲，我们并没有用 Explore 的功能，在 Grafana 的框架上我们自研了适合公司自身的指标查看方式。

在 Qunar 的应用管理方式中，我们给每一个应用会起一个唯一标识叫做 AppCode ，AppCode 是对一个应用的抽象，它整合了这个应用所有的资源，包括代码、主机、环境、依赖的资源、使用的外部服务、监控和相关 Owner 等。抽象出 AppCode 后，我们可以对这个 AppCode 可以在多个系统进行连通，多系统可以数据共享，而且在各系统设计时统一基于 AppCode 的概念来设计，这样对于所有人在使用这个系统时候也能降低很多理解成本，比如一个开发在自己的应用中埋了一些监控数据，上报的监控平台，那他在查看自己的监控时只需要知道自己的 AppCode 是什么就能查看自己的指标数据，如果指标过多时再进行关键字搜索等操作，如图14-7。

![monitor07](/medias/images/monitor/monitor07.png)

<center>图14-7 搜索功能</center>

比如上图14-7中，用户只需要在左侧的应用树（注意这里的应用树并非上面的面板树，一些公司里面可能叫服务树）搜索或者定位到自己的 AppCode，右侧便自动展开对应的环境的指标，自动组装成一个只读面板开提供给用户查看，此面板是即读即消的。可以看到右侧可以选择自己的环境，查看不同环境下的指标数据，同时支持根据正则表达式来搜索自己的指标名，简单快捷。

1. ##### **指标浏览**

   如果实在忘记了自己的指标名，也是点击搜索框后面的指标树来浏览自己当前应用当前环境下的所有指标的。如下图14-8：

   ![monitor08](/medias/images/monitor/monitor08.png)

   <center>图14-8 指标浏览</center>

   没错，指标也会被组装成一棵树形，这是因为 Graphite 的指标命名方式不同于现在的 Prometheus 的 Tag 指标的扁平化形式，Graphite 的指标命名也是层级化的，是以 "metic.name.xxx" 这种形式组合成一个 Metric，每一个点便是 Tree 的一级。

2. ##### **指标收藏**

   ![monitor09](/medias/images/monitor/monitor09.png)

   <center>图14-9 指标收藏</center>

   如图14-9用户可以将自己关注的指标收藏的自定义的分组里，以便以后快速查看某一类指标，比如一个应用的核心指标，我们建议收藏到核心分组里，以便快速查看。

3. ##### 指标模板

   正如上面提到的，当你监控了很多主机、容器、交换机路由、防火墙等等设备以及 Mysql、Redis 等基础中间件信息时，在查看这些设备状态监控时基本关心的东西是相同的，如果每次查看都有手动建一个面板人工成本太高，因此基于这类监控，我们做了模板化，模板化后用户不需要关心怎么创建出来这种面板，你只需要输入自己关心的主机，那么当前主机的所有监控信息就都会展示出来。而当你定位到自己的 AppCode 时，便会自动列出当前 AppCode下主机的监控信息。

4. ![monitor10](/medias/images/monitor/monitor10.png)


<center>图14-10 指标模板</center>

- [ ] #### 告警管理


1. 告警管理模块，解决了两个问题：

   - 统一了告警配置和告警查看，用户不在需要到多个系统查看自己告警，告警依然支持根据 AppCode 来搜索自己配置过的告警。

   - 用户可以根据自己的需求自己添加告警，不需要像早期需要特定的人来添加。

2. 告警管理分为业务告警和主机告警：

   - 业务告警是指用户应用埋点监控的 Metrics 添加的告警，用户可以任意增加、删除、修改、设置通知联系人等。

   - 主机告警这块我们实现了类似 Zabbix 的主机模板和 Discovery 功能，用户可以配置一类主机监控模板，同时设置 Discovery 规则，一旦当有匹配类别的主机上线，则自动应用这些监控和告警。

![monitor11](/medias/images/monitor/monitor11.png)

<center>图14-11 告警管理</center>



![monitor12](/medias/images/monitor/monitor12.png)

<center>图14-12 告警配置</center>

### 14.2.2	架构选型和设计

Graphite 是一套优秀的指标处理和存储解决方案，它自带了数据接收器（ cabron-relay）+聚合器（ carbon-aggregator ）+存储器（ carbon-cache）+时序DB（whisper）这样的一套组件，是基于 Twisted 异步框架开发，最主要的是它的各个组件都可以任意横向扩展，扩展性非常好。因此能够很好的支持大规模指标采集存储。

其数据收集协议也非常简单，协议形式是："metric_name value timestamp" 便是一条指标数据，例如： echo "my.metric.name 1 `date +%s`" | nc serverIP serverPort 就能将 my.metric.name 这个指标上报给 Graphite Server 端。

原生 Graphite 仅支持 TCP/UDP Push 的方式采集指标，客户端必须封装 TCP/UDP 协议来进行数据 Push，客户端上报数据有时会变得不可控，加入客户端封装有 Bug 或者单位时间内需要上报的量非常大时，对于网络消耗和性能消耗都非常大。因此我们自研了 Qmonitor，qmonitor 类似 Prometheus 的 Exporter，客户端只需要引入一个 jar 包或相关依赖，在单位时间内数据只记录在本地内存中，同时向外暴露一个 http 协议的 url，Server 端会定时去抓取数据并做聚合计算，然后在 push 到 graphite 集群存储下来。

下图14-13便是客户端使用 Qmonitor 采集数据的流程，客户端只需要引用 qmonitor-client 相关依赖包，调用 API 生成和计算指标即可。Server 端则会定期去拉取 client 端的数据，拉取后会进行聚合计算，然后 Push 到后端存储。后端存储使用了 Graphite 和 Clickhouse，Graphite 接收和存储普通指标，存储周期长，支持多种存储策略，而 Clickhouse 用来存储所有的单机指标，这类指标特点是查看的需求少，存储周期短，但是量很大。

![monitor13](/medias/images/monitor/monitor13.png)

<center>图14-13 使用 Qmonitor采集数据的流程</center>

对于主机的指标采集，我们使用了开源的 collectd 配合采集，如图14-14，collectd 内置了采集主机信息的各种能力，比如 cpu、load、disk、swap、net 等等数据，并且支持 graphite 格式的写入插件，这样我们就可以在每台主机上安装 collectd agent，collectd 可以指定某一个间隔时间比如是30s，每30s 采集当前主机的所有数据信息，并且 Push 到后端存储。

![monitor14](/medias/images/monitor/monitor14.png)

<center>图14-14 主机的指标采集</center>

下图14-15是整体的存储结构图，不管是 Push 还是 pull 的方式的采集的指标都会放入 whisper时序DB中，同时数据通过旁支 mirror 到 relay-index 应用，relay-index 探测出新增指标，将指标放入索引 DB 提供给 API Server 使用。

![monitor15](/medias/images/monitor/monitor15.png)

<center>图14-15 存储结构图</center>

### 14.2.3	告警治理

#### 14.2.3.1	报警升级

报警升级分为提醒升级和联系人升级，我们的报警升级没有强制联系人向上升级，但用户可以方便的在树节点上设置联系人（一般在树节点上设置相关负责人），树节点上设置的联系人，其子节点上的告警都会接收到，而且是前面的联系人都没人处理告警的时候（比如没有接听告警电话），才会通知树节点联系人， 因此利用这种形式做联系人升级。

提醒升级是当一个报警开始发送告警通知时，一开始我们只会发送 Qtalk 提醒，Qtalk 属于弱提醒，但如果过了一段时间这个报警仍然没有人处理，就会升级成电话提醒，电话提醒则是属于强提醒。

![monitor16](/medias/images/monitor/monitor16.png)

<center>图14-16 报警升级流程</center>

#### 14.2.3.2	报警降噪

当配置的告警越来越多，很多告警都不是重要的告警，经过几轮人员更替后，这些告警都会成为极大的噪音，当它报警后，新来的同学不清楚这个告警是监控的什么东西，也不敢随便处理比如关闭告警，于是放任不管，那这些报警就会一直报着，长时间的没有人来出来。时间长了很容易引起相关人麻痹，导致错失重要告警的处理。

针对这种情况，我们开发了降噪算法，算法只根据报警开始时间和设置的报警间隔，来计算出当前是否处于发送窗口，只有在发送窗口期，告警通知才能真的发送出去，否则不发送通知。以此达到的效果是 比如：当一个告警设置通知间隔是5分钟，但是持续了30分钟还没有人处理，那么我们会动态拉伸他的通知间隔，逻辑上将通知间隔变成了10分钟，1个小时没人处理则变成20分钟等等依次类推，这个告警持续时间越长，那么通知窗口也会被拉的越来越长，通知的频率就会越来越低，即便是他处在告警中。

![monitor17](/medias/images/monitor/monitor17.png)

<center>图14-16 报警降噪机制</center>

#### 14.2.3.3	闪报抑制

在去哪儿，一个报警持续时间小于5分钟的报警我们称为闪报。很多时候我们刚开始配置报警时，尤其是一个新的报警，大家可能对阈值估算不准，或者由于很早之前配置的报警，之前设置的阈值已经跟现在的业务波动不太匹配了，就会导致报警波动、闪报。这种闪报通常来说不影响业务，但是频繁打扰会导致报警接收人麻木。因此抑制的目的就是减少无意义的打扰，尽可能提升报警的精准性。

当前报警抑制的实现方式是观察一个报警一段时间内（目前设置的是24h）报警状态改变的频率，如果频率高于某一个点（这个阈值点是系统根据算法算出并默认提供的，用户可以自己修改）则会进入抑制状态，频率低于某一个点则会退出抑制状态。其中半个小时内的状态多次改变则状态权重递增。

1. **快速退出抑制机制**

   如果最近半小时内检测的状态改变频率低于快速退出频率阈值， 则立即退出抑制状态。

2. **强制退出抑制机制**

   如果在抑制状态中发生了报警，且持续时间超过一定时间，目前设置5分钟，则强制退出抑制状态将报警报出来。

#### 14.2.3.4	报警收敛

当某一个时刻突然出现了大量告警，这会导致告警刷屏，在去看 Qtalk 告警消息时，根本就看不过来，一些重点的告警会被淹没掉，或者根本无法确定哪个告警最有可能是需要重点关注的。 在这些告警里一定会有一些告警是因为其他告警造成的，比如一台宿主宕机，主机上的 load、disk 等告警都会报出来，甚至可能影响到业务指标，导致某个业务指标告警。所以报警之间会有一个隐形的依赖，通常主机或机房层面的告警依赖非常清晰，属于物理依赖，而应用之间也有依赖，应用之间的告警就是逻辑依赖。

所以在做告警收敛时，依赖拓扑是很重要的，物理依赖可以很简单的计算或者定义出来，而多应用间依赖在去哪儿可以通过 Qtrace 来拿到依赖拓扑，因此目前在去哪儿当某一时刻出现大量告警时，会触发告警收敛，告警收敛会先在 Appcode 内收敛，根据依赖拓扑一层层收敛，AppCode 内收敛完成之后会进行AppCode 间收敛，最终只会将叶子告警发送通知，其他的只在详情页展示，并不真实发送通知。

这里面有一个难点，就是 AppCode 内配置的多个业务告警怎么拿到依赖关系（依赖树），目前去哪儿将 Metrics 和 Qtrace 进行了结合，Qtrace 在记录方法间调用时如果经过的方法内有记录 Metric，那么会将这个 Metric 信息带入到 Trace 信息里，最后经过洗数分析就能拿到指标间的依赖关系，根据指标间的依赖关系就能拿到告警的依赖关系。比如 一个 http 的入口方法 foo，此方法里记录了一个指标就是 foo.access.time，foo 调用了 bar 业务层方法，bar 业务层方法内记录了一个指标 bar.exec.time，那么指标 foo.access.time 就可以认为依赖 bar.exec.time，如果 foo.access.time 和 bar.exec.time 同时告警，那么只有bar.exec.time 的告警会通知出去。

![monitor18](/medias/images/monitor/monitor18.png)

<center>图14-16 报警收敛示意图</center>

### 14.2.4	Trace结合

Qtracer 是去哪儿自研的 Trace 系统， Qtracer 首先是一个全链路追踪系统，可以用来定位跨系统的各种问题；另外，通过收集链路中的各种数据达到应用性能管理（ APM ）的目的。而监控就是通过 Qtracer 收集链路数据的特性来做到跟 Qtracer 结合的。

首先 Qtracer 提供了 QTracer.mark() 方法，此方法能够将自己想要的信息关联进当前 trace 的 context 中， 然后在我们自研的 Qmonitor agent 中，在记录指标数据时调用 mark 方法，将当前指标标记进当前的 Trace 信息里面（如果当前有 Trace 信息的话），以此能达到的效果就是一个带着 Trace 信息的请求从入口进来（如果没有携带 Trace 信息，可以生成 Trace），经过了 foo() 方法，而 foo 方法中记录了指标 foo.access.time，那么 foo.access.time 指标会被 mark 到当前 Trace 信息中。

数据被mark进来后，当前会以日志的形式落盘，然后经过洗数，最后会将Trace的Trace Id 和当时经过的Metric Name的关联信息存入到ElasticSearch中便于后期检索使用。

当这些数据信息都没有问题的时候，此时比如我们在 foo.access.time 指标上加了一个告警，一旦指标告警，我们就可以检索出告警的这一段时间内所有的Trace 信息，通过 Trace 信息能够快速的反应出当时请求的状态，以此来协助开发或应用 Owner 快速定位问题。

下图是在 Watcher上拉取当前告警时间段的 Trace Id 信息。

![monitor19](/medias/images/monitor/monitor19.png)

<center>图14-17 Trace Id信息</center>

点击Trace Id 可以查看具体的 Trace 信息，以及调用拓扑等。

![monitor20](/medias/images/monitor/monitor20.png)

<center>图14-18 Trace信息</center>

## 14.3	云原生时代监控平台的演进

2021年时，去哪儿网在公司内部进行了大规模容器化部署，到目前为止去哪儿网大部分的应用都已经运行在内部的多个 Kubernetes 集群上。在这样一个背景下早期我们许多根据 kvm 特性的相关设计也要跟着变化。比如容器跟 kvm 最大的一个变化便是动态 IP 问题，kvm 的主机一旦申请后 IP 是固定的，且 kvm 主机的生命周期远远长于容器，通常是跟 AppCode 同生命周期，而容器生命周期则非常短，每一次变更发布都会导致容器 IP 变化，这种频繁变化给周边系统带来了许多问题，因此周边系统也需要进行改造以应对这种变化。

### 14.3.1	业务指标数据采集

上面提到过去哪儿的业务监控使用的 Qmonitor 进行埋点和暴露数据，虽然 Prometheus 有自己的 Client，但是想要让全公司的应用从 Qmonitor 改成Prometheus Client 显然不现实。而 Qmonitor 早期设计是针对 kvm 的流程设计的。比如用户在通过 Qmonitor 监控时我们自动拿到对应的需要监控的主机名或IP ，然后通过 Pull 当前主机对应的 url 才能拿到监控数据。而容器的 IP 是经常变化的，因此我们修改流程，增加了 Discovery 的功能。

首先我们增加了事件监听器，用来监听和收集所有 k8s 集群的事件，然后将这些事件存储到DB的同时发送到 mq 中，其他应用可以监听 mq 来拿到自己想要的事件。然后在 qmonitor 增加 client-discovery 模块，此模块可以监听对应的消息事件，然后动态的更替对应的 ip 和 meta 信息，更新的数据提供给 qmonitor server 来使用。这样便能动态的发现和变更对应的 client 地址，而且对业务无感知。

![monitor21](/medias/images/monitor/monitor21.png)

<center>图14-19 业务指标数据采集</center>

### 14.3.2	基础设施层监控

云原生时代，我们的基础设施层由kvm云变成了容器云时我们使用的 collectd+graphite 来收集基础设施相关的监控。比如 cpu、磁盘数据。

collectd 是一个 agent 需要安装在 kvm server 上，但是容器内通常只有一个进程，agent 的形式是不友好的。而 k8s 层的相关监控我们使用 prometheus+cAdvisor，k8s 自身便是使用 prometheus client 来 export 内部的状态数据再配合使用 cAdvisor 监控容器运行时的状态数据比如 cpu、内存。

- [ ] #### 指标模板集成


我们的 Dashboard 是根据 Grafana 做的二次开发，而 Grafana 本身就支持很多的数据源插件，其中便包括 Prometheus，因此能够很方便的将 prometheus 集成到 Watcher Dashboard 上，集成后依然提供对用的 Pod 模板查看，允许用户直接通过自己的 AppCode 就能查看到自身应用下当前的pod状态，以及应用级的 pod 状态。

![monitor22](/medias/images/monitor/monitor22.png)

<center>图14-20 业指标模板集成</center>

- [ ] #### 容器告警

容器的告警像 kvm 一样，也支持模板化，容器通过监听事件来将模板应用到对应 AppCode 上实现动态添加告警，模板包括了检测指标、检测规则、告警阈值等信息。由于 Prometheus 自身支持异常检测，因此可直接将告警规则通过 prometheus crd 同步到 prometheus ，通过配置，Prometheus 检测到异常后，会将异常信息 Push 到我们自研的 Alert-API 模块，Alert-AP I可以说是 prometheus 到 icinga 的一个转换器，Icinga 是我们真正的告警管理模块，支持开关报警、告警升级等策略，Icinga 类似 Nagios，但是 icinga 提供了更友好的 API 操作入口。

![monitor23](/medias/images/monitor/monitor23.png)

<center>图14-21 容器告警</center>



## 14.4	总结规划

上文详细讲述了去哪儿网监控告警平台的实践过程，从一开始的 Nagios+Cacti 到现在的 Watcher，我们以企业需求为出发点来设计和落地监控平台。其实可以看到监控从最初的只要能帮助我们发现问题这样的基本需求，到现在我们更期望它能帮我们快速定位问题甚至是解决问题，比如很多公司在实践的根因分析和故障自愈等，近期我们也在做这方面的探索。

另外的在当前高度自动化时代，监控系统的用户很多时候会从人变成程序，监控系统慢慢的会像 CMDB 一样演变成更高层工具的数据基石，向外提供出自身的数据和分析能力，比如在去哪儿网实践的混动工程、全链路压测、发布驾驶仓等都需要用到监控的时序数据和告警数据来做断言或者熔断。











# -----------------

# Part7	稳定性保障

去哪儿网是典型的旅游电商平台，类似阿里的双11，京东的618，旅游业也有其专属的热卖季，主要分布在五一、十一、春运、暑期等法定节假日。在这些节假日期间，国人出行的诉求会大大增加，直接产生的效果就是对公司机酒服务的压力成几何倍数的增加。每年的这些时候，数倍甚至数十倍于往常的流量涌入系统，总会有业务系统因为这种激增的流量导致出现性能问题，而系统负责的团队又因为没有做好事情的筹备、预案，导致最终实际影响用户的产品使用体验，更严重的，甚至会使用户安排好的出行计划泡汤，钱也打了水漂。

那么针对这种常见且棘手的性能问题，公司内部从2021年年初开始以全链路压测、混沌工程为主的稳定性建设，目前我们已经实现了节假日等流量高峰日零故障的稳定运行，而且通过这些手段让稳定性保障的成本极低，本章将会详细介绍这两个模块的设计和实践过程当中的一些解决方案，包括对于一些通用难点的创新思考和落地经验。











# 第十五章	混沌工程



## 15.1	前言

从 Netflix 提出混沌工程的概念，到如今各个公司都在探索混沌工程的实践，混沌工程作为服务韧性治理的系统性解决方案，已经证明了它的价值。但作为一门新兴的技术学科，从理论到落地实践，再到达到预期价值，还有着很长的路要走。

去哪网从2019年开始进行混沌工程实践，三年多的时间里，经历了从探索、发展到创新的不同阶段。并且每个阶段都与业务紧密结合，力求在每个阶段都做到业务价值最大化，秉持着从业务中来到业务中去，走出了一条与去哪儿业务特点及内部基础设施紧密结合的落地之路。如今台已经建设成了从测试到线上，从机房层到应用层，从简单策略到复杂组合策略，多维度、多场景自由组合的混沌演练平台。

回顾这几年去哪儿网的混沌平台建设，中间遇到过很多困难，也走过弯路。尤其是到了后期的创新阶段，没有了业界成熟案例的参考，挑战越来越大。但我们始终坚持了以业务实际作为出发点，以内部生态作为落脚点，最终混沌平台与压测平台，自动化测试平台一起组成了公司服务韧性治理三剑客，保障了去哪儿庞大业务持续稳定运转。下面我将详细介绍混沌平台建设过程，希望能给到同样期望落地混沌工程的朋友一些启发。



## 15.2	背景

### 15.2.1	频发的故障

去哪儿网成立于2005年，作为一个老牌互联网公司，业务经历了多个阶段的发展，内部架构也在持续为了适应业务发展而在做调整。变化的业务，变化的架构给服务韧性带来了极大的挑战。2019年，公司发生了多次 P1 故障，几个典型故障列举如下：

1. 携程机房断电，影响时间长达40个小时。
2. ZK集群故障，故障时间15分钟。
3. QMQ部分消息发送失败故障持续69分钟。
4. KV系统主库导致金融服务不可用持续18分钟。

以上问题对去哪儿造成了重大的损失，但可以评估的是订单损失，不可评估的是用户信心，品牌形象。基础平台作为公司重要技术支撑部门，这些问题，更是对我们提出的拷问：

1. 为什么应急预案没生效？
2. 为什么解决时间这么长？
3. 如何避免？

因此我们下定决心要找到服务韧性治理的系统化解决方案，对造成线上重大故障的问题，做到御敌于千里之外。

彼时混沌工程理论已经成型，国内外都有一些成功实践，我们也在很早之前也有了用混沌进行稳定性治理的想法。但鉴于虽然已经有一些先驱公司进行了不同程度的探索，但实际投入产出比很难评估，我们对比国内外一线大厂在可投入资源上有着比较大的限制。因此我们的课题，是要用最具性价比的方式，解决核心问题，获得公司信任和支持，进而能够持续投入更多资源。

基于以上，我们分析了公司技术和业务现状，期望找到最佳的切入点。

### 15.2.2	技术&业务现状

- [ ] #### **不可靠的基础设施**

![2.2.1](/medias/images/system_resiliency/2.2.1.png)


<center>图15-1 失败case</center>

- [ ] #### **复杂的集群**


这里列举去哪儿旅行一些数据，如图15-2，线上跑着的活跃的应用有 3000 多个，dubbo 接口有 18000 多个，网关上注册的域名有 3500 多个，qmq 13000 多个，技术栈有 5 种语言，大规模的系统群和生态很难保证完全可靠，任意一个系统有问题，都可能影响最终的结果。

<img src="https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8X9iby5jLXBPLeRCJ2P1ico8Cu3ia6fiaic0b8YM8DdUL9z6uSiaKRl8yTibOA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:80%;" />

<center>图15-2 集群数据</center>

- [ ] #### 常见故障类型

  1. 机房问题：机房断电、网络不通、 网络延迟。

  2. 中间件问题：zk集群故障、mq故障、 数据库故障、缓存故障。

  3. 机器问题：load高、cpu满、 磁盘满、IO满。

  4. 应用问题：fullGC、服务下线、 日志拖慢、线程池满。

  5. 依赖问题：下游dubbo/http接口延迟、抛异常。

     

### 15.2.3	我们的切入点



![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8X5ribvdZ8AGmJWrXwxuxfrOwM5oCLRaqOVjnCibNmAydomACDLb3xqTQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)      

<center>图15-3 架构层次划分</center>

 基于以上图15-3架构层次划分，我们分析发现越是影响重大的故障，就越是底层问题，其中P1故障基本都与机房或中间件故障有关。综合以上我们优先做的是机房演练、中间件演练。



## 15.3	混沌工程探索 

### 15.3.1	技术选型

在确定了机房演练、中间件演练的目标后，我们开始了技术调研和准备。在当时的环境下得出如下结论：

1. **开源工具完善度低**

   尤其对于大规模的网络尤其在我们非容器化的环境下，进行大规模网络、机器，以及特定中间件故障模拟如zk等很难找到有效的支持工具。

2. **成本高，低覆盖度**

   完全借助模拟的方式实现，成本大且覆盖不完整，不能实现我们快速实施混沌工程，为业务快速贡献价值的目标。

因此我们决定，采用关机这种简单粗暴的方式直接进行演练，以最真实的效果，最快速的方式直接落地。当时我们的应用都是在 kvm 环境下，决定利用OpenStack API 来进行批量关机演练操作。根据混沌工程理论，最主要挑战就是：如何控制爆炸半径控制以及如何快速止损恢复。

### 15.3.2	问题及解决方案

- [ ] #### 爆炸半径控制

1. 机房聚合信息查询，方便应用改造。
2. 演练规模从小到大，梯次递增。
3. 业务低峰期进行。
4. 充分的的前期主备，详细的 checklist（单机配置、本机的资源文件、服务的热点机群、外网域名等十几项前期需要 check 的点）。

- [ ] #### 止损恢复

1. 自动建立沟通群，进度周知。
2. 人工值守。
3. 接入告警，告警事件关联推送。
4. 一键恢复脚本。
5. 与业务线联动，提前做好用户周知和补偿准备。

### 15.3.3	落地实施

<img src="https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8JcmhMTbWMygxTAGmdGibe7wmJ7GBl04w1AiaAwgxPQbKvsxgzEc9s6fA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:67%;" />

<center>图15-4 落地流程</center>

### 15.3.4	演练总结

发现 10+影响业务稳定性重要问题，比如：

1. zk 稳定性问题。 

2. 客户端 QP 发布服务跨机房高可用问题。

   

## 15.4	混沌平台建设 

### 15.4.1	概述

有了以上关机演练的成功实施，以及对服务韧性立竿见影的效果。让我们确信通过混沌工程的实践，可以实现我们提高服务韧性的目标，让我们有了投入更大资源持续发展的信心。但上面演练中也暴露出了我们演练中存在的各种问题，自动化能力不足、人工成本大，故障模拟能力不足，稳态检测能力不足，爆炸半径控制精准度差。基于此，我们又进行了更深入的技术调研，并确立从场景覆盖度，底层到上层，线下到线上发展路径，从现如今的视角总结为如下阶段性成果。

### 15.4.2	混沌平台建设的三个阶段

1. **基础能力建设-应用演练**

   - 丰富故障场景，从中间件到依赖层场景全覆盖。

   - 完善的稳态检测能力。

   - 搭建控制平台，以应用为维度进行故障编排。

2. **线上常态演练-攻防演练**

   - 精准控制爆炸半径，可以在线上做到自由演练，屏蔽对真实用户的影响。	

3. **架构治理-强弱依赖演练**

   利用第二阶段自动化演练的能力，对线上复杂的业务通过创新性的演练编排发现架构的不合理之处，推进从架构层面进行持续改进，防止架构腐化。


### 15.4.3	基础能力建设-应用演练

有了目标，还要找到落地的抓手，做到底层建设与业务价值相结合，才能有的放矢，目标聚焦，我们这一阶段选择的落脚点为应用演练。总结起来就是：

1. 选择合适的开源工具，进行故障场景模拟。
2. 基于开源二次开发，支持公司内部自研中间件故障场景以及缺失的企业级场景。
3. 开发控制平台，以应用为组织维度，进行演练编排。
4. 打通公司 watcher 内部监控平台，radar 自动问题发现平台，支持人工录入演练监控指标。

- [ ] #### 技术选型

​         当时维护比较好、用的比较多工具有 Chaosblade 、 Chaos Mesh，对比如下图15-5，综合考虑：我们选择了 Chaosblade 。

1. 我们重视的场景丰富度上相近。
2. VM/K8S双平台支持只有Chaosblade，我们当时线上都kvm但正在规划容器化，需要双平台都支持。

| 组件       | 支持平台 | 支持场景 | 开源 | 整体性            | 侵入型 |
| :--------- | :------- | :------- | :--- | :---------------- | :----- |
| Chaos Mesh | K8S      | 丰富     | 是   | 好                | 无     |
| Chaosblade | VM/K8S   | 丰富     | 是   | 差(当时只有agent) | 低     |

<center>图15-5 技术选型对比</center>

- [ ] #### 场景覆盖


Chaosblade 支持多层面的故障演练，比如基础资源层面的、应用服务层面的。同时还支持 k8s 。并且它支持的场景非常丰富，基本上涵盖了需求的各个方面，但是还有一些场景是缺失的。主要如下：

1. HTTP超时
2. fullgc
3. 日志拥堵
4. 调用点区分
5. 链路匹配

对于这些不支持的场景，在落地的过程中，我们支持了这些形态，并且将这些改动提交到社区，参与开源共建。

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8ogHOkN3SnlQa9s70j6dib7V054kV59N12OqAQiax6DdmDSJk8Cs6SXDA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

<center>图15-6 开源共建</center>

- [ ] #### 自研控制面

Chaosblade 提供了控制面 Chaosblade-box ，支持以 host 为组织维度，进行故障编排。但我们公司内部业务是以应用树-应用 host 的层次结构进行组织的，我们实际的业务场景中可能会需要根据某个业务节点筛选演练的应用。直接使用开源方案，灵活性差，观测性差，并且还会涉及到很多数据同步的问题。

我们后面规划中会在依赖层，做很多与公司内部实际业务强相关的演练编排、动态交互，开源架构并不适合我们的业务场景。

基于以上两点，我们选择自研控制面板的方案，设计了以应用为组织维度，host 为最小组成粒度的灵活的演练控制面板，具体如下图15-7：

![4.1.3.3](/medias/images/system_resiliency/4.1.3.3.png)

<center>图15-7 自研控制面板方案</center>

- [ ] #### 完善的稳态观测能力


 在混沌理论中，稳态观测主要起到两个作用：

1. 确定系统运行是否符合演练预期。

2. 系统状态超出预期，快速止损。


因此完善的稳态观测能力，不仅是演练正确实施的前提，更是能够降低线上影响的核心手段。基于此，我们设计了基于 Qunar 内部监控系统 watcher 并结合公司 AIOPS 工具 Radar 做双重稳态观测手段。其中 watcher 指标我们接入了：

1. 业务核心监控面板即公司的各个业务线的监控报警大盘。

2. 机器稳定性报警。

3. 人工单独标记的混沌报警。

- [ ] #### 小结

通过以上关键基础能力的建设，我们具备了对任意应用，进行从机器到依赖层面全场景的演练能力，将核心能力抽象到平台，为 QA、DEV 提供了强大的演练武器库，可以随时、随地自定义演练。做到了功能强大，成本低廉。

### 15.4.4	线上常态演练-攻防演练

- [ ] #### 思考

应用演练，在公司运行一段时间以后，统计平台数据后发现。90%以上的演练是在测试环境做为测试场景的模拟工具来进行使用的。即使线上进行的演练，也基本都是只对单应用、单机器、单策略的演练。但实际故障发生时，更多是全量机器，甚至多个策略、多个应用组合的下故障场景，但我们也很容易理解以上结果出现的原因：

1. 我们目前线上演练无法屏蔽对真实用户的影响，线上演练即有损演练。
2. 更大规模的演练，意味着对线上更大的损害。
3. 没有闭环机制保障。

如果能够降低演练对真实用户的影响，并建立一个闭环保障机制，便可以真正在线上推行常态化演练，充分发挥混沌平台的价值。

- [ ] ####  降低演练影响


- **方案选型**

  结合公司内部生态，经过调研，我们有如下图15-8两套可行的方案：

  - 为混沌演练单独建立一套运行环境，该环境类似灰度环境，可以灰度用户比例。我们所有的演练运行在这套环境之上，对用户的影响便可以通过控制用户灰度比例来进行控制。

  - 模拟用户请求，故障注入只针对模拟的用户请求。

|  方案  | 优势                                       | 劣势                                                         |
| :----: | ------------------------------------------ | ------------------------------------------------------------ |
| 方案一 | 来自用户真实请求，能够完全模拟线上真实行为 | 1.仍然对灰度的用户造成影响。 <br>2.构建单独的环境需要额外的成本，在公司当时的环境下，需要动态路由功能的支持，动态路由方案只在测试环境支持，迁移到线上方案重新论证，以及较大的改造成本和风险（关于动态路由可参考-测试环境治理实践-环境治理-动态路由设计）。<br>3.用户流量比例受限，可能因为流量低，不能发现真实问题。 |
| 方案二 | 完全消除用户影响 <br>                      | 并非真实用户请求，可能存在差异 。                            |

<center>图15-8 方案对比</center>

基于以上方案对比，我们认为模拟的请求与真实请求的差异可通过技术手段尽量降低，并且模拟请求可以做到完全消除影响，因此决定选择用户请求模拟的方案。 

- **方案详情**

1. **case 生成**

   通过自动化测试平台生成压测 case。

2. **流量染色**

   压测脚本发压时，在公司链路追踪工具 qtrace 中埋下流量标识，通过 qtrace 将流量标识全链路透传。

3. **开源 java agent 改造**

   支持故障模拟匹配流量染色。

 出于回馈开源的考虑，我们将此功能开发成了通用的业务参数匹配。并支持了两种匹配模式：  1）SPI 模式，用户可以写代码自定义匹配逻辑。2）固定 key匹配模式，不用写代码只需在配置的 http header 、dubbo attachment 携带相应业务数据，即可自动读取匹配。

- [ ] #### 闭环机制-攻防演练


1. 攻击点编排：选择历史高频故障进行场景设计。

2. 攻击点上报：防守方定位排查后，上报给攻击方 攻击方确认，正确则得分。

3. 攻击终止：防守方定位成功或者超时自动终止。

4. 积分：根据定位时长、故障难易程度进行积分排名、公示。

5. 复盘：过程中发现的问题进行修复。

6. 线上各业务线定期执行。

   

### 15.4.5	架构治理-强弱依赖演练

- [ ] #### 强弱依赖定义

![强弱依赖](/medias/images/system_resiliency/强弱依赖.png)

<center>图15-9 强弱依赖示意图</center>



如图15-9所示， A、B、C、D、E 代表应用，A1-E2 分别代表各应用对外提供的接口。

1. 调用链路： 分别有 A1->B1,C1->D1 ，A1->B1,C1->E1，A2->B2,C2->D2，A2->B2,C2->E2  四条调用链路。
2. 依赖：  A1->B1->D1 调用链路中分别有 A->B1,B1->D1 两个依赖。
3. 入口： 最外层接口我们成为入口，即上图 A1、A2  。
4. 强弱依赖： 当依赖出故障时，对入口返回数据有影响即为强依赖，无影响及为弱依赖。
5. 举例说明：  当 B1 调用 D1 出现故障时，如果 A1 可以正常提供用户服务则将依赖 B1->D1 称为入口 A1 的弱依赖。   

- [ ] #### 基于强弱依赖的架构治理


我们统计线上故障，相当一部分比例是因为某个依赖问题导致的级联故障，这种级联的影响，在整个调用链路中是否合理，是否存在不重要的接口影响了核心业务。在几百个应用在错综复杂的链路中，通过人工保障非常困难。如果能够通过自动化的方式分析出核心业务下游各个依赖的强弱情况，并不断推进研发降低强依赖比例，可以达到提高线上稳定性的效果。

基于公司分布式链路追踪工具 Qtrace，我们能基于入口接口拿到到调用链路和依赖信息。再加上混沌平台已经具备的线上演练能力，我们可以为每个依赖分别注入故障，对比注入故障前后相同请求参数下入口返回结果是否一致，来判断依赖的强弱。

基于上面思考，我们设计强弱依赖演练，完整演练流程如下图15-10 ：

![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8sCh5dvooZW2ks4TK7Xoz6Y76oaTTbaq3iaaPdFyj7uicOhfVicf7hIECQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

<center>图15-10 强弱依赖演练流程</center>

- [ ] #### 难点

1. **依赖命中率** ![图片](https://mmbiz.qpic.cn/mmbiz_png/YE1dmj1Pw7l8xJW5PKqCxE4KFzias3fl8WHibdX2uiahQmuyb8nZoTFB2TI0kiafCQeDvZ12tktnhwjafWgtlxJywQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

   

   <center>图15-11 某入口依赖拓扑图</center>

   先来解释下依赖命中的概念，在故障注入期间，有压测流量打到当前的依赖，才算命中。上图15-11是某一个入口依赖的的拓扑图，我们的演练范围是要覆盖整个链路的每一个接口的，如果要保证命中率，那 case 筛选逻辑就很重要，我们希望的是通过筛选之后，能保证命中率在 90% 以上，这样演练才是有效果的。

   - #####   精准case匹配

     case 筛选策略有两种：

     - 第一种是从入口处的应用，随机取若干条，从入口开始请求，这个方式比较简单粗暴，但是问题是覆盖率很难保证。举个例子，比如说用户在我们平台买机票，它可以买北京到上海的单程，也可以买北京到上海的往返。单程和往返这两种类型的报价，在服务端，是不同的系统来提供服务的，如果我随机筛选的 case 里，只有单程的请求，没有往返的请求，那就只能命中单程的链路，往返这条链路上的所有依赖就都没办法命中。

     - 第二种策略是精确匹配，比如系统 d 调用系统 e 提供的接口 f ，准确的找到系统 d 调用系统 e 的 f 接口的 trace，和入口做关联，发起请求，这样就能保证依赖被覆盖到。

     精准 case 匹配的难点是需要链路追踪系统的支持，相当于要链路追踪系统收集到线上所有trace请求并记录相关的接口数据，然后提供查询功能。描述起来简单，但在数据量上对链路追踪系统是一个非常恐怖的要求，性能上存在较大问题。我们为此做了很多工作，来满足性能要求。具体改造点如下：

     - trace span 存储方案：从 hbase 改为 clickhouse，可以通过简历多维度索引的方式，支持类 SQL 的多字段匹配查询。

     - 在自动化测试平台获取到的 case 作为原始 trace ，将这些 tarace 给到 clickhouse 作为原始过滤数据集，极大的降低了 clickhouse 的压力。

   - **增量演练**     

     有了精准 case 匹配后，依赖命中率有了很大程度提升，但仍然无法满足90%命中率的要求。根据无法命中的 qtrace 深入业务代码分析后，发现业务缓存对代码链路命中有着很大的影响。举例来说：同一个 case 命中缓存时一条链路，缓存失效时又是一条链路。另外就是业务以外的问题比如：agent 安装失败，故障模拟失败以及演练过程中应用发布等一系列问题都会导致当前演练的依赖失败，导致无法命中。

     因此我们又在之前演练的基础上，设计了增量演练，针对无法命中的情况。通过多次对未达命中要求的依赖，自动演练的方式，来屏蔽某次的缓存、发布、故障模拟导致的偶发性问题。

2. **自动化断言**     

   自动化断言的核心逻辑是根据下游依赖注入故障前后，diff 入口返回结果，来判断强弱依赖。这个问题的前提是，未注入故障的情况同样的请求能保持幂等性。要求是相同的请求参数，在任意时克请求，返回结果都是一致的。实际业务中并非所有的请求都是幂等的，有的与接口设计相关，比如接口中返回了时间戳字段等。有的则与业务相关，比如同样是北京到上海的航班列表，不同时间的返回结果在业务逻辑上就是不一样的。

   综合以上影响业务结果不一致的主要有两种情况：

   - 接口设计中一些非核心业务字段影响了断言结果，比如时间戳等。这种情况我们可以通过将时间戳这类非关键字段排除 diff 的方式解决，但如果人工在众多入口接口中识别这些非关键字段，工作量很大且容容易判断出错。

     我们的解决方案是：通过未注入故障的情况同时请求两次，自动 diff 结果差异，来进行智能降噪。

   - 业务本身在不同的时刻请求就是不一致的：

     这个在设计之初我们对这个问题的考虑是不足的，设计的基准请求和故障请求之间是有2分钟左右的时间间隔的，后期我们做自动化断言准确性分析，才发现这样在2分钟间隔下违反幂等性的接口量非常大，会极大的影响断言的准确性，后面的改进的目标就是让基准请求和故障请求可以同时进行。在这里联合压测平台进行了从发压层面的改动，来适配我们整体方案。

   ![请求对比](/medias/images/system_resiliency/请求对比.png)

   <center>图15-12 请求对比</center>

   请参见图15-12前后方案对比： 

   - 上图左侧，基准请求利用智能降噪的请求做为基准请求，与对比请求中间间隔了 agent 安装，故障注入步骤，大概间隔有2分钟。


   - 上图右侧，我们在故障注入后，新加了一次请求做为基准请求。消除了基准和对比请求的时间间隔。


   这个改动成立需要：压测平台适配，能够分别对两次请求区分染色，并通过上文我们提到的根据开源 agent 二次开发的业务参数匹配故障的能力，去做故障匹配，根据染色标识只匹配对比请求，进行故障注入。




## 15.5	总结

本文为了方便理解，将以上阶段进行了严格划分，在实际的逻辑过程中，以上各阶段实际是交织在一起的。我们故障场景的支持，很多是随着业务需要和技术发展不断完善的。攻防演练和强弱依赖演练，在实际建设中也是并行进行的。至于具体落地步骤，总体是遵循从底层到上层，从线下到线上，从单一场景到复杂场景，并紧密结合业务当前需要。建议同样进行混沌落地的朋友可以根据自己公司业务实际需要，灵活调整。

每个阶段有不同的复杂度，其中基础能力建设主要集中在底层技术，比如熟悉开源架构，agent 源码改造。线上自动演练主要是充分利用公司的内部成熟技术，需要与公司其它平台一起做好架构规划，从公司整体技术路径上做好架构规划，需要公司整体的有力支持，在我们内部压测平台，watcher，radar 等平台都在基础平台维护，因此从组织架构上也比较好的保障了协同规划。

架构治理这块的难点主要是业务复杂度，需要深入公司各个业务线的业务实际，在不断踩坑，不断纠错调整中，不断提高数据覆盖度和准确性，除了技术外，更重要的是业务挑战，而这些恰恰是做技术平台的研发欠缺的，本人也从这个阶段的建设中，对公司整体业务有个更深入的理解。

​    







  

# 第十六章	全链路压测



## 16.1	全链路压测平台- CSP

### 16.1.1	背景

去哪儿作为互联网公司，存在节假日业务突增的场景。各业务线存在自己的压测方式，并深度分化。不仅造成业务线之间规范不统一，还给跨团队之间的交流带来阻力。2020年开始，公司开始推行统一工具任务，相同功能的工具仅保留一个，在此背景下我们开始了全链路压测平台的建设。调研过程发现业务线主要存在两大痛点：

1. **业务线使用压测工具不统一**

   往年各业务线采用自己的压测方式，如机票使用 jmeter、酒店使用流量调权、其他业务线还存在拦截流量压测与 beta 压测情况。这种场景下出现报告不统一、压测规范不统一、排期不统一等问题，报告给领导的结论也是五花八门。

2. **压测成本高**

   作为旅游互联网公司，每年有四五个节日需要做大型压测演练。每次演练开发、测试、产品、tl 等均需在场，大业务线的参与人数与非诚勿扰相亲相当，每个节日演练2~3次，粗略统计每年有几百 pd 用于压测。


### 16.1.2	收益

1. 历时两年的开发落地，业务线核心场景均进入常态化压测；当前执行任务总量 10w+。

2. 单一压测任务排期时长由 1pd 降低至 0.1pd；大业务线的压测任务排期由 50pd 降低至 2pd。


### 16.1.3	系统设计

- [ ] #### 需求


业务线最基本的要求有图16-1这三点：协议多、能力强、记录全。

![image](/medias/images/csp/requirement.png)

<center>图16-1 用户需求</center>

- [ ] #### 方案选型


选型过程，考虑了 NGrinder、Jmeter、LoadRunner 工具，并与数列科技、京东等公司进行技术交流，最终自研自己的压测平台-csp 系统；

1. **压测环境**

   经过一系列内部讨论以及大厂互联网压测技术交流，最终决定采用线上压测方案；读场景使用线上真实数据作为参数直接请求，写场景采用录制回放模式请求，db、redis 操作使用影子库方案，读写场景均做异常监控，阈值触发熔断，确保异常停止能力。

2. **工具对比**

| 类别         | Jmeter                                      | NGrinder               | **LoadRunner**     |
| ------------ | ------------------------------------------- | ---------------------- | ------------------ |
| 实现语言     | Java                                        | java/python            | java/VB/C/.NET     |
| 使用方式     | C/S 或 Command                              | B/S                    | C/S                |
| 支持分布式   | master/slave                                | controller/agent       | master/slave       |
| 开源方式     | 免费、开源                                  | 免费、开源             | 收费               |
| 支持协议     | 多协议                                      | 多协议                 | 多协议             |
| 是否需要编码 | 基本不需要                                  | 需要，Jython/Grovvy    | 需要               |
| 可扩展性     | 可增加 plugin，输出结果可以再加工，扩展性强 | 可增加plugin，扩展性强 | 通过扩展函数库实现 |
| 平台化       | 有开源或云端的压测平台                      | 本身具备               | -                  |

<center>图16-2 工具对比</center>

选型过程首先排除 LoadRunner，由于常态化压测过度依赖第三方不满足公司价值需求；公司平台化开发过程更倾向于 BS 模式，考虑到完全自研前提，最终研发 CSP 平台，参考 NGrinder 架构并实现无损扩展能力。

### 16.1.4	平台建设

![image](/medias/images/csp/design.jpg)

<center>图16-3 架构图</center>

1. **任务流**

   用户操作 Web 页面，任务入库，分配模块计算分发给发压机 Agent。

2. **脚本流**

   发压机 Agent 拉取脚本、数据文件启动引擎，发压。

3. **数据流**

   发压过程生成的请求数据、日志、采样等上报并存储 ES，提供用户 web 页面查询。

- [ ] #### 数据源


此处数据源特指压测参数，获取主要分两种方式：数据文件与脚本实时读取。

![image](/medias/images/csp/param-data.jpg )

<center>图16-4 数据源获取的2种方式</center>

1. **附件文件形式**

   - TC-日志：基础架构开发的agent，线上应用默认按照，发布平台配置即可实现日志落地；操作方便，推荐使用。

   - 业务日志：页面配置日志参数分隔方式；老版本的灭霸自动化压测数据获取方式。

   - 数据文件：业务线自己上传数据文件，按照给定的数据模板。

2. **脚本调用第三方接口**

   - 数据接口：业务线提供数据获取接口，数据获取与解析有业务线自行完成；特殊逻辑数据，实时变化数据等。

   - redis：第三方接口获取。

3. **其他**

   - 多接口数据依赖：多个接口调用时，后面的接口参数依赖前方接口数据返回，这种需要用户自己调整脚本适配。（工具平台后期支持）



- [ ] #### 引擎与脚本

  ![image](/medias/images/csp/script-engine.jpg )

<center>图16-5 引擎与脚本</center>

1. **脚本与附件：**

   - 数据参数：数据文件生成，根据参数配置，在 ES 中获取日志中的参数数据，生成特定文件供脚本使用；脚本读取文件时确定文件名、读取方式（随机、顺序、循环）。

   - 同步异步：同步脚本，请求等待上次请求返回再进行下次请求，此时用户设置的 RPS 值为最大 RPS，有低于设置的可能。
   - 异步脚本，主线程严格准守用户配置的 rps 发送请求，返回结果由特定线程池处理。

2. **发压引擎：**

   - 引擎初始化：配置发压机器的线程数、RPS值、发压时间、梯度变化、执行次数等。

   - 引擎加载：脚本与附件加载，初始脚本对象。

   - 引擎执行：根据配置的压测规则，执行脚本，执行完成自动停止，同时接收停止命令与心跳检查自杀停止。

   - RPS实现：注意此处不能使用Guava的RateLimiter实现，存在初始化不同步与变更速率延迟问题，并且没有区间补偿策略，建议自行实现。

- [ ] #### 熔断


做压测与学开车相同，上路首先要学会刹车，除了正常压测结束还需要考虑熔断策略。靠人的操作均存在不可靠性，尽量使用工具与流程代替人工，减少疏忽带来的故障。熔断设计如下：

![image](/medias/images/csp/fusing.jpg )

<center>图16-6 熔断设计</center>

1. 机器报警：是业务线与 ops 设置的机器异常指标，如  cpu、load、io 等。
2. 业务报警：是业务线监控 dashboard 的数据监控。
3. 压测数据熔断：失败率、rps 过高、响应时间等。
4. 业务指标：异常日志、自定义指标覆盖业务部分报警等。
5. 工具熔断：发压机心跳检测，超时自熔断。

- [ ] #### 报告


统一工具的很重要目标是统一数据规范，是报告的价值体现。报告包含4块内容：

1. **概览**

   压测目标、QPS 倍数、接口 QPS 达标率、链路覆盖度、核心应用覆盖度等。

2. **压测数据**

   事物、qps、响应时间、失败率、成功数等

3. **链路详情**

   入口进入后，请求到的链路数据；包含压测前与压测过程两部分数据，数据内容包含应用、接口类型、接口地址、响应时间与 qps；通过对比可以获取压测是否满足需求；实现需要依赖 tc 提供的链路计算服务与监控 watcher 数据获取接口。

4. **指标数据**

   机器指标、业务指标、自定义指标、压测指标是否有报警记录。



- [ ] #### 发压流程


![image](/medias/images/csp/pressure-process.jpg )

<center>图16-7 发压流程</center>

1. **备注**

- 场景配置接口、参数、熔断信息；任务配置发压参数，熔断特殊配置；压测记录是当时发压过程参数信息；场景与任务、任务与记录均是一对多关系。

- 发压机上报数据包含：包含压测数据（事物、响应时间、成功数、失败数）、采样数据（ trace、入参、返回结果等）、日志数据。

- [ ] #### 踩坑记录


1. **历史任务自启动**

   系统启动发压机前会有一组校验逻辑，其中一项是校验发压机是否足够，当前可用发压机不够时不启动压测，等待发压机足够时启动压测。恐怖事件过程：

   - 业务线启动压测任务a

   - 系统校验发压机不够用，等待发压机

   - 业务线查看任务没有启动，去食堂吃饭

   - 其他发压任务b释放发压机

   - 系统校验任务a发压机足够，并启动压测

   - 系统报警

   **处理方案**

   - 压测未启动，告知用户，等待发压机过程。

   - 等待超时时间由1天减少至1分钟，超时停止任务（停止后用户手动触发才可启动）
   - 启动压测，消息周知用户。

2. **压挂系统**

   - 业务线希望严格使用配置的 RPS 压测，使用异步脚本发压，超过一定压力时，系统无法对外提供服务

   - 也有部分业务未配置有效的报警、或报警未来得及触发就已经把系统打死

   **处理方案**

   - 梯度压测，一定压力增幅压测，并且保证梯度时间报警可以发出

   - 客户端熔断配置，当响应时间平均值、T90 等超过一定值，并且连续5s 时，立即停止
   - 使用同步脚本，发压时只有上次请求返回后才进行下次请求
   - 低峰期压测，即使系统异常可以减少损失

3. **发压不同步**

   - 发压过程需要使用多台发压机同时启动的情况，存在发压机发压不同步现象，秒级监控中用户疑问较多


   **原因**

   - 启动耗时不同：如 git clone、数据文件拉取、初始化引擎等存在一定的时间差

   - 任务分发使用 dubbo 调用，发压机过多时存在一定的时间差
   - 机器时间不统一，出现过机器时间差异造成启动时间由差异问题

   **处理方案**

   - 派发任务预留一定时间再启动压测任务，默认2s

   - 派发任务数据时，携带数据库的时间（也可以使用 dispatch 的时间），发压机计算时间偏移量

   - 通过预留时间与时间偏移量计算可以达到同时启动，此处进一步优化是 wait 住所有线程，最后统一命令执行

     

### 16.1.5	应用落地

- [ ] #### 推广落地


落地过程先使用一个大业务线验证，在去哪儿使用自上而下的推行方式，限期完成，整个过程相对顺利。此处简化流程如下：

1. 机票业务线试用

2. 就平台数据导入到平台（前期推广时使用，后期清空重新创建），人力3pd

3. 废弃旧平台

4. 基础平台团队协助业务线接入使用

5. 二期开发

6. 常态化压测


- [ ] #### 使用效果	


- ##### 平台数据


| 数据类型           | 值    | 备注                           |
| ------------------ | ----- | ------------------------------ |
| 场景接入           | 1h    | 接口自动化接入，调试验证1h     |
| 业务线压测执行排期 | 1~2pd | 往年排期30pd+，当前降低至2pd； |
| 入口接入量         | 853   | 入口接入量                     |
| 压测执行频率       | 7k/月 | 常态化压测                     |
| 启动耗时           | 3.5s  | 校验、分配、资源拉取、启动等待 |
| 数据延迟           | 5s    | 秒级数据展示、延迟5s展示       |

<center>图16-8 平台数据</center>

- ##### 报告部分截图

![image](/medias/images/csp/report-1.png)

<center>图16-9 报告部分截图-1</center>

![image](/medias/images/csp/report-2.png)

<center>图16-10 报告部分截图-2</center>

![image](/medias/images/csp/report-3.png)

<center>图16-11 报告部分截图-3</center>




### 16.1.6	总结与展望

总结：压测平台的搭建与落地历时大半年时间，针对公司特性做了较多定制化功能，如入口一键接入，参数自动生成，目标自动获取与计算，trace 链路问题分析等。不仅统一规范、降低成本的，还很大程度团建各业务线合作的意愿。当前工具相对薄弱的模块是问题自动定位与优化建议，计划在2022年底完善。

展望：目前全链路压测平台结合灭霸自动化平台，获取数据 case 与请求结果 diff 功能能力。并且作为混沌工程的请求触发工具，断言请求子调用的强弱依赖。让平台赋能额外能力，会发现新的天地，我们在不断探索中。



## 16.2	录制回放 agent

### 16.2.1	背景

在全链路压测中，流量录制和回放的必要性不用多说，而去哪儿网主要技术栈为 java，因此在内部开发了一个基于 java 字节码插桩的录制回放 agent —cinema。
Cinema agent 已经在内部使用近一年，在实际的全链路压测中经受住了考验。

### 16.2.2	技术选型

在调研阶段，笔者就意识到，单纯的网络流量录制回放无法满足实际的压测需求，对 java 代码的修改必不可少，首先就排除了只做网络流量的录制回放方案。

当时业内开源的有一定知名度的 java 录制回放工具为阿里巴巴的基于 jvm-sandbox 实现的 jvm-sandbox-repeater，公司内部的自动化测试系统也使用到了这个 agent，在一开始也对 jvm-sandbox-repeater 进行了调研。

调研下来，jvm-sandbox-repeater 能做到 java 方法级的录制回放，在功能上基本符合要求，但有如下几个缺点。

1. 逻辑流四五分裂，导致开发和维护体验非常糟糕。排除框架性代码，只谈具体录制回放的部分，整个录制回放逻辑流被拆的到处都是，处理代码非常反直觉，看代码的过程让人苦不堪言。
2. Classloader 没有做合适处理，录制回放代码中充斥大量反射调用，同样严重影响了开发维护体验。
3. 难以和去哪儿中间件结合。去哪儿的全链路压测基于内部的全链路追踪系统 qtrace，并且需要在很多地方使用到内部的配置中心 qconfig 对整个流程进行控制，这需要对 jvm-sandbox 和 jvm-sandbox-repeater 都进行较为深度的修改，并且由于前面两个缺点修改的难度和工作量就更大了。  

基于以上的原因，加上自主开发也能够更好的自主可控，选择了重头开发一个录制回放 agent。

### 16.2.3	设计与实现

本文主要说明的是录制回放 agent 的设计与实现。

- [ ] #### 字节码插桩

字节码插桩相关的东西网上有大量的文章和文档，这里不再进行具体的技术说明。

在插桩实现上，cinema agent 主要采用了 bytebuddy 做为字节码插桩工具，在少量因为 jvm 限制（不能动态增减类的字段和方法）无法使用 bytebuddy 的地方使用了 asm 进行插桩。

- [ ] #### 录制回放代码编写

有句名言说的好，代码是给人看的，顺便才让机器执行。

Jvm-sandbox-repeater 中录制回放的逻辑流非常分散，导致了最终的代码晦涩难懂，但实际上录制回放的逻辑本身是非常简单的，cinema agent 对此做了针对性的改进，可以以非常自然符合直觉的方式编写相关代码。
在 cinema 录制回放的编写中，排除掉错误处理部分，简化后的核心代码如下图16-12：  

```java
if (needRecord()) {
    Object result = originCall.call(arguments);
    storeRecord(arguments, result);
    return result;
} else if (needRepeat()) {
    return repeatStore.Call(arguments);
} else {
    return originCall.call(arguments);
}
```

<center>图16-12 核心代码片段</center>

其中 originCall.call（arguments）就是对原始方法的调用，整个处理流程非常清晰，即使是新人，也可以参考已有代码对新组件进行快速兼容。

同时，得益于精心设计的初始化流程和 classloader 处理，在录制回放代码编写中，不需要使用反射，在 agent 的开发中可以和开发业务代码一样直接使用要进行插桩的组件的类和方法，也可以直接调用去哪儿内部的中间件（有人会说平常写代码不都是这样的吗，没错，平常写代码就是这样，就像空气一样，失去时你才会更理解它的重要性）。

这里也提一下，cinema 本身是一个透明的无侵入 agent，但如果内部工具实现方有意愿，也可以直接在代码中使用 cinema 的接口参考 agent 里代码进行实现。

- [ ] #### 初始化与 classloader

在前面说过，cinema 的录制回放中可以直接使用待插桩组件和内部中间件的类，以一种自然的方式编写代码，同时还能提供 api 给外部组件来实现录制回放逻辑，这些都是由特殊的初始化流程与 classloader 决定的。两者相辅相成，所以放到一起来进行说明。

整个 classloader 体系如下图16-13所示，cinema agent 有两个 jar 包，一个是 spy，一个 lib，其中 spy 部分由 SystemClassloader 加载，lib 部分则由CinemaClassloader 加载。

![cinema_classloader](../../themes/matery/source/medias/medias/images/full_link_performance_test/cinema_classloader.png)

<center>图16-13  classloader体系</center>

- [ ] #### Spy

1. **Cinema 接口**

   先说 spy，spy 分为两部分，一部分是 cinema 的接口，由于接口由 SystemClassloader 加载，所以这些接口除了可以被 lib 里的实现使用，也可以在业务代码或者说工具提供方使用。同时由于实现被全部委托到 lib 包，即使使用了 cinema 的 api，agent 的升级对应用和工具方也完全透明。

2. **Premain 初始化**

   Spy 的另一部分是 cinema 在 premain 阶段的初始化代码，注意这只是 premain 阶段的初始化，并不是 cinema 全部的初始化。叫 premian 初始化是因为这里的初始化由 java agent的premain 方法开始执行。

   在 premain 初始化里，cinema 会对多个类和组件进行字节码修改，这里只说最核心 premain 插桩与 preonline hook 注入。  

   整个初始化流程如下图16-14所示：

   ![cinema_hook](/medias/images/full_link_performance_test/cinema_hook.png)

   <center>图16-14 Premain初始化流程</center>

3. **Premain 插桩**

   一是确定性的组件比如java servlet，各种httpclient以及dubbo，虽然具体的插桩实现由lib部分实现，但实际上的插桩在初始化就完成了。插桩后的代码大致如下图16-15：

   - ```java
     if (needUseCinema()) {
         lazyIntercepter.intercpt(object, method, arguments, originCall);
     } else {
         originCall.call(arguments);
     }
     ```


   <center>图16-15 插桩后的代码</center>

   其中 lazyIntercepter 就是具体插桩实现的包装，此时应用的业务 classloader（一般而言就是 tomcat 的 WebappClassloader ）还没有加载，lib 部分还无法进行初始化，使用懒加载的方式来对代码进行插桩。

   那么为什么要设计这么一个有点绕的插桩机制，是因为通常来说我们只有在 premain 阶段才能对 java 类进行字段和方法的增删，只有这样才能够实现前面“录制回放代码编写”所说的简洁清晰的编写机制。

   而 needUseCinema() 方法则会进执行判断逻辑，只有当 cinema agent 已经完全初始化（不止是 premain 初始化），并且当前类的 classloader 符合要求才会执行。

   这里简单说下当前类的 classloader 的具体要求，就是要属于业务 classloader 或者说业务 classloader 的祖先 classloader，这是为了过滤掉额外的 java agent 中的类（去哪儿的应用往往带有多个 agent，而 agent 的 classloader 加载的类都是不需要进行录制回放的）。

4. **Premain hook**

   在 premain 初始化里另一个核心的字节码修改就是 preonline hook 的注入。

   这里的 preonline 指的是在应用启动后，中间件已经加载完成，但应用还没有对外暴露提供服务的阶段。
   Preonline hook 注入就是指对在 preonline 阶段运行的某个方法进行插桩（不同的公司应该具有不同的插桩点，这里不进行具体类和方法名的描述），将hook 加入到这个方法的运行代码中。

   具体的 hook 实现就是一个前文提过的 lazyIntercepter 对初始化代码的包装，它的任务就是做 preonline 初始化，只有当 premain 初始化和 preonline初始化都完成的时候，cinema agent 才算是完全初始化。

5. #### Lib

   Lib 包同样包含两个部分，一部分是各种字节码修改的相关具体实现，这个不多说，一部分就是 preonline 阶段的初始化代码，也就是 preonline 初始化的实现。

   为什么在 premain 初始化外再执行一个 preonline 初始化呢，有三个目的：

   - 一是要获取业务 classloader，而执行 preonline 代码的 classloader 就是准确的业务 classloader，同时也是 cinema classloader 的父classloader，是 cinema 各种操作的基础，相当于 agent 在业务 jvm 中的锚点。

   * 二是在此时我们才能够确定各种内部中间件已经初始化完毕，从而可以获取去哪儿配置中心上的各种相关配置，组装 agent 内部各种实现。前文所说的 cinema 在 SystemClassloader 的各种接口的实现也都是在此刻被组装，进而将整个 cinema agent 初始化，设置全局初始化完成标记。
   * 三是根据从配置中心获取的配置对应用的 java 方法进行字节码修改。录制回放的应用往往跑在线上，是不能够对全量的 java 类进行插桩的，这里需要根据远程配置进行选择性 java 方法插桩。

- [ ] #### 灵活的配置

Cinema agent 被设计为一个基于远程配置的非常灵活的 agent 。大到一整条链路的录制回放开关，小到某个方法调用录制参数的序列化方式，几乎所有设置都可以通过远程配置动态修改。

具体有哪些配置属性这里不进行详细说明，只是说一下 cinema 中的配置层次设计。

在 cinema 中，配置分为公共、任务、应用、调用四个级别：

1. 公共配置指全局配置，对所有地方生效。
2. 任务配置代表某一次全链路压测，不同的压测任务有不同标记，只对指定的任务相关生效。
3. 应用配置代表一个指定的业务应用在指定的压测任务下的配置。
4. 调用配置就是在指定的任务和应用下的某一个具体调用的配置，比如一次 http 或者 java 方法调用。  

其中下级属性默认使用并可以覆盖上级属性，通过公司的配置中心对配置进行动态修改，可以达到对整个全链路压测各个粒度上的精确控制。

- [ ] #### Jvm trace

因为自动化子调用挡板设置（见其它文章）的需要，cinema agent 基于去哪儿内部的全链路追踪系统 qtrace 实现了 jvm trace。顾名思义，与全链路追踪不同，jvm trace 就是在 jvm 内部的 trace，trace 链路不跨 jvm，每一个 java 方法调用作为一个 span。

因为涉及到另外一个内部系统 qtrace，这里不再进行详细说明。

### 16.2.4	结尾

Coder lives matter！

当然这是夸张的说法，写代码和看代码最多只会让人头疼。但代码的可维护性确实也非常重要，也许贴近底层后有时候会变得不太容易，但我们还是可以在这方面努力做点工作。

需要注意的是，文章里确实介绍了一套可行的可维护性比较好的 java agent 实现方式，但在具体实现中对 classloader 的使用要非常小心，对 java classloader 体系有深刻的理解，agent 本身的依赖和打包也要仔细安排，否则非常容易陷入 classloader 错误的噩梦。

最后，本文介绍了去哪儿内部自研的基于 java 字节码插桩，可以通过配置灵活控制，对开发和维护友好的 cinema agent，并对设计和实现的一些关键点做了相关说明。



## 16.3	中间件实现



![framework](/medias/images/full_link_performance_test/framework.png)

<center>图16-16 中间件实现</center>

### 16.3.1	压测标识透传

在生产环境进行全链路压测，需要对压测流量进行染色，使用一个标识来标记流量是正常流量，还是压测流量，且这个标识需要在整个调用链路中透传。标识需要满足以下条件：

1. 压测的标识在调用过程中不丢失
2. 压测的标识在调用过程中不改变

基于去哪儿网内部的分布式链路追踪系统（ QTrace ），可以达到以上两个要求，压测标识在整个调用链路中透传，在需要的地方进行获取。

QTrace 是去哪儿网内部的分布式跟踪系统，可以记录分布式场景下调用链路，类似于 zipkin。目前该组件支持 dubbo 调用，http 调用，内部的 mq 系统。可以将压测标识放置在 Qtrace 中，在整个调用链路中传递。

### 16.3.2	数据隔离

在生产环境进行全链路压测，最核心的是线上写操作不能影响正常的线上数据。因此需要将压测的数据进行隔离。隔离方案可以分为数据隔离和流量隔离。

- [ ] #### 数据隔离 VS 流量隔离

1. **流量隔离**

   主要是通过构建压测环境来解决，如线下压测环境，或泳道化/ Set 化建设，将压测流量与线上流程完全隔离。线上流量和压测流量走不同的集群实例。

2. **数据隔离**

   主要是通过对压测流量进行染色，让线上服务能识别哪些是压测流量，哪些是正常流量，然后对压测流量进行特殊处理，以达到数据隔离。线上流量和压测流量走同样的应用实例，但是在数据存储上做了特殊处理。

公司最后选择做数据隔离，压测流量覆盖的覆盖路径与生产流量相同，能够达到更好的测试效果。

- [ ] #### mysql 隔离

mysql  的数据隔离方案选择使用影子库进行数据隔离，数据操作时根据压测标识操作不同的数据库。

1. **影子库生命周期**

   - start：当执行测试计划时，创建一个新的影子库的链接池。

   - end：执行计划结束时，关闭链接池，释放资源。

2. **实现**

   去哪儿网内部的 mysql 均使用了中间件团队封装的 DataSource，为了实现根据压测标识进行数据隔离，我们将隔离逻辑放在了数据源实现上，具体实现如下：

   - 原有的数据源类（ QunarDataSource ）对外暴露的方法不变，将具体的实现转移到新增类:  RealDataSource 。

   - 原有的数据源类( QunarDataSource ）作为代理类，在其中维护两个 RealDataSource 的实例：一个是生产库的实例（ prodDatasource ），另一个是影子库实例（ shadowDatasource ）。
   - 在调用 getConnection 方法时，判断当前请求是否为压测请求，如果是的话，使用 shadowDatasource 实例调用；否则使用 prodDatasource  调用。

3. ![mysql_class_uml](/medias/images/full_link_performance_test/mysql_uml.png)


<center>图16-17 mysql隔离 </center>

- [ ] #### redis 隔离

redis 的数据隔离方案选择使用影子库进行数据隔离，数据操作时根据压测标识操作不同的数据库。

1. ##### 影子库生命周期

   - start：当执行测试计划时，创建一个新的影子库的链接池。
   - end：执行计划结束时，关闭链接池，释放资源。

2. ##### 实现

   去哪儿网内部的 redis 均使用中间件团队封装的 RedisClient，可以在其中根据压测表示选择不同的 redis 实现。

   - 原有的 RedisClient 类对外暴露的方法不变，将具体的实现转移到一个新的类：RealRedisClient。

   - 原有的 RedisClient 改造为一个代理类，在里面两个 RealRedisClient 的实例对象：一个是生产库的实例（ prodRedisClient ），另一个是影子库的实例（ shadowRedisClient ）。
   - 执行操作 redis 的方法时，判断当前请求是否为压测请求，如果是的话，使用 shadowRedisClient 实例调用；否则使用 prodRedisClient 调用。

3. ![redis_class_uml](../../themes/matery/source/medias/medias/images/full_link_performance_test/redis_uml.png)


<center>图16-18 redis隔离 </center>

- [ ] #### MQ隔离

1. ##### 目标

   mq 的隔离目标是要保证压测的流量（打了压测标识的消息）只有指定的消费组才能消费。

2. ##### 实现

   去哪儿网使用 MQ 系统为内部开发的 MQ ，所以隔离功能可以自由修改，我们把实现逻辑放在 server 侧：

   - 监听配置中心的配置变更，获得压测主题指定的消费组

   - 根据消息中的qtrace链路信息获得压测标识
   - 若是压测消息，根据配置判断哪些消费组可以消费

- [ ] #### 日志隔离


1. ##### 目标

   日志隔离的目标是保证正常流量的日志与压测流量的日志分开（写到不同的日志文件目录）。防止压测流量日志被日志收集组件收集，影响数据组依据日志的统计报表数据。

2. **效果**

   在 log 目录（去哪儿网的部署环境中有个专门存放业务日志的路径）下新建一个 tc_shadow 目录，压测流量产生的日志放在此处。

3. ##### 实现

   - 创建 IsolateFileAppender，继承自 RollingFileAppender。IsolateFileAppender 中保存两个 RollingFileAppender 实例，分别是shadowAppender（影子实例，影子实例的文件对应着 tc_shadow 目录）和 prodAppender（生产实例，原有文件目录），覆盖实现RollingFileAppender 中的方法。

   - 在执行 doAppend（ ILogEvent）方法时，判断当前日志是否为压测流量，如果是压测流量则使用 shadowInstance 执行，否则用 prodInstance执行。

   - IsolateFileAppender 在 Java 应用启动时，由 javaAent 替换原有日志框架中的 FileAppender 为 IsolateFileAppender，用户不用修改 logback.xml


   <img src="images/full_link_performance_test/log_uml.png" style="height:550px">

   <center>图16-19	日志隔离 </center>



## 16.4	写压测实践-自动设置挡板自动设置挡板

### 16.4.1	背景

在全链路写压测中，需要流量的录制和回放、影子库或影子表这些基本技术要素。这些技术在去哪儿的实现在其他章节详细介绍。有了基本技术能力后，全链路写压测最麻烦、繁琐的地方莫过于设置需要录制和回放的挡板了。

对外部有实际写影响的调用是一定要提前设置挡板的，比如在去哪儿比较常见的就是调用支付操作、调用代理商接口实际生成订单、给用户发确认短信等等。还有一些不可重复调用的接口、在内存中有状态维护的系统等也需要配置录制回放。这些要录制、回放的挡板是需要在全链路写压测执行之前配置好的，我们调研过有些公司的方案是：需要业务同学提前梳理对外依赖，给出需要配置的挡板信息。这种方式缺点显而易见，人工梳理耗时久、容易梳理不全。去哪儿一个链路可能涉及几百个应用，很难通过人工把这些挡板梳理出来，我们想要探索一种自动化的挡板设置方案，节省梳理配置时间，也避免了遗漏，保证可靠性。

### 16.4.2	技术选型

了解到目前还没有公司实践过自动设置挡板相关的技术，所以我们是完全自研。

### 16.4.3	技术难点

1. **如何自动识别哪里需要设置挡板**
   - 所有对外网的调用
   - 写压测不可穿透的系统（支付、代理商生单、给用户发短信）
   - 导致录制流程和回放流程有差异的子调用（内网 http、rpc 调用）
   - 导致录制流程和回放流程有差异的内部调用（ java 方法级别）
2. **如何发现录制流程和回放流程中导致差异的子调用**
   - 识别内网 http、rpc 调用链路差异
   - 识别内部调用（一个系统内 java 方法调用链路）差异
3. **相同子调用多次回放怎么选择**
   - 按录制时的调用顺序回放
   - 按参数匹配回放
4. **稳定性问题**
   - 不同的 case 有不同的链路，可能会导致不同的结果
5. **效率问题**
   - 链路太长，设计几百个系统，怎么能提升效率

下面会介绍，每个技术难点的解决方案。

### 16.4.3	设计与实现

- [ ] #### 主流程介绍

整体流程（精简版）：
![write_1](/medias/images/full_link_performance_test/write_1.png)

<center>图16-20	整体流程 </center>

以机票生单流程为例：

1. 开启录制后，真实用户下单时，整个流程走过的系统间的设置了挡板的调用都会被录制下来，录制数据存储到 ES 中。同时 trace 系统把整个调用链路也记录了下来。
2. 全链路压测系统从 ES 中获取到符合条件的录制的 case 后，使用同样的参数向一台机器发起回放请求。
3. 服务中的 cinema agent 识别到是回放请求，如果识别到是配置了挡板的下游调用，会进行回放，不会真实把请求打下去，如果是对 db 的写操作，会写到影子库中，如果是对 db 的读操作，会优先读影子库，影子库没有会读线上库。

4. 回放过程中，trace 系统也会记录调用链路，回放请求完成后可以得到整个回放的 trace 链路，将录制时的 trace 链路和回放时的 trace 链路做 diff，可以得到两个链路的差异。

5. 通过分析 diff 结果，通过一个 calculate 算法，可以得到要设置哪些挡板。后面会详细介绍 calculate 算法。

6. 将计算出的挡板信息真正设置成挡板后，再进行下一轮的录制->获取 case ->回放-> diff -> calculate -> addBaffle 流程。

7. 直到最终的录制和回放链路没有差异，且返回值也没有差异时，自动配置挡板流程结束。


### 16.4.4	链路Diff和算法介绍

链路 diff 和计算挡板算法是密不可分的。调用链路可以保证是一个树状结构，不会有环。首先入口会分配一个 traceId ，初始 spanId 是1，然后每一次系统内部对外调用会增加1，每一次跨系统调用 spanId 会增加一级。链路图如下16-21：
![write_diff_1](/medias/images/full_link_performance_test/write_diff_1.png)

<center>图16-21	链路图 </center>

由于同一个链路上的 traceId 一定是相同的，就省略了 traceId，只用 spanId 来表示每个调用，图中展示的是回放缺失部分链路的情况，1.2->1.2.3是缺失的，设置挡板的时候如果我们把1->1.2设置成挡板，那么录制和回放链路肯定是没有差异的，但是这样的话1.2及其下游系统全链路压测就全都覆盖不到了，为了更精确的找到导致差异的子调用，我们首先分析，可能导致1.2->1.2.3是缺失的，是1.2->1.2.2的返回结果不正确或1.2->1.2.1的返回结果不正确，甚至是1->1.1的返回结果不正确，或者是1.2系统内部流程发生了变化导致的。

1. **情况1**

   我们认为是1.2->1.2.2的返回结果不正确或1.2->1.2.1的返回结果不正确可能性更大一些，优先把它们设成挡板，看下一次录制和回放的链路diff时是否能正常走到1.2->1.2.3,如果能走到并且入口返回结果正确，说明把它们设成挡板是有效果的。因为设置了两个挡板，为了更精确地找到是哪一个子调用，我们可以依次去掉一个挡板看看效果，首先去掉1.2->1.2.1挡板，如果链路和入口返回结果仍然相同，那说明1.2->1.2.1不是影响回放结果的子调用，1.2->1.2.2是影响回放结果的子调用，一个有效挡板设置完成。

2. **情况2**

   如果把1.2->1.2.2和1.2->1.2.1都设成挡板后，回放链路仍然缺失1.2->1.2.3，则设置1->1.1为挡板。如果链路和入口返回结果相同，表明设置1->1.1为挡板有效，但1.1还有下游调用，可能是下游调用影响了1.1的返回结果，因此要继续延1.1向下寻找。删除1->1.1挡板，设置1.1->1.1.1、1.1->1.1.2、1.1->1.1.3为挡板，后续流程和之前类似，不再赘述。如果链路和入口返回结果不同，表明设置1->1.1为挡板无效，可能是1.2内部调用的问题，进行情况3。

   

   

   ![write_diff_2](/medias/images/full_link_performance_test/write_diff_2.png)

   <center>图16-22	链路图 </center>

3. **情况3**

   当 trace 链路中差异边前面没有可mock的子调用时，只能是节点1.2系统内部 java 方法的调用有差异了。我们将进行系统内 java 方法间调用链路的对比。开启录制 java 链路开关后，重启一台机器，打到这台机器上的录制请求会上报调用的 java 方法及 jSpanId（类似于系统间的 spanId ），通过这些jSpanId 数据可以构造出一颗树。

   在计算要设置的 java挡板时，考虑到复杂度，和效率问题，精确性可以不要求那么高，当发现1.2->1.2.3缺失后，直接将1->1.2设成挡板，会更快。当然，如果想讲求精确度，是可以延续使用 trace 链路的算法，精确计算的。

![write_diff_3](/medias/images/full_link_performance_test/write_diff_3.png)

<center>图16-23	链路图 </center>

### 16.4.5	实践过程中遇到的问题及解决方法

1. **如何快速将不可穿透的外部调用提前统一设成挡板？**

   - 方案1：通过 T 值获取链路，识别出外网域名和 ip，将之一个一个设为公共挡板，这种方式的缺点是如果链路变化，不能及时识别到，有遗漏的风险；


   - 方案2：在中间级别改造，在中间件内部识别外网域名和 ip，定义一个统一的配置，只需配置一次。

2. **相同子调用出现多次，回放时如何选择？**

   最初我们只是提供了方案1，后面发现有入口时一次请求，到下游是并发生单的场景，如果按录制顺序回放会导致回放的结果和请求参数对应不上，导致后面生单流程异常。于是我们又支持了方案2，由于方案2有一定的性能损耗，默认使用方案1，特殊子调用支持配置成方案2。

   - 方案1：按录制时调用顺序（index）回放，简单高效，并发的子调用不适合；


   - 方案2：支持配置特殊子调用按参数自动模糊匹配，可以配置最多对比层数，默认对比前2层，比如最外层有10个字段，第二层有5个字段，最终会从录制的子调用中选择出和回放的参数能够匹配上最多的来返回。

3. **线上的一个入口请求会由于参数的不同而走不通的流程，可能会导致链路差异较大，从而影响自动设置挡板算法的准确性**

   对录制 case 进行筛选，比如只筛选返回值中带"生单成功"关键字的 case，保证不会因为多次捞取的 case 不同而影响后续运算。

4. **一个链路涉及几百个应用，每个应用又提供多个接口，假设精确找到一个有效挡板需要15min，假设每个系统要设1个挡板，那么设挡板过程可能需要几天时间，如何能提升自动配置的效率。**

   如果完全按顺序地自顶向下计算设置挡板，整个大链路可能要1个月的时间才能调试完成。我们想到的方案是，将整个大链路拆成若干个小链路，小链路之间可以并行调试，最终再整合到一起。











# -----------------

# Part8	数字化实践

数字化不仅是解决问题的有效手段，同时也是寻找突破点有力工具，还是我们进行决策的重要依据。基于此背景，我们将研发过程关注的核心要素：质量、效率、成本、技术先进性、用户体验度这五个方向进行数字化落地，通过建模：包括数据收集、指标模型建立、统计分析等，在每个方向对不同的对象（团队、项目、应用）进行度量，最后通过趋势图、排行榜、雷达图等呈现，最终让我们的整个研发过程可视化、可追溯、可衡量。

本章将遵循 DMAIC 方法论，从定义、测量、分析、改进、控制几个维度介绍我们的数字化建设过程，希望能给大家的数字化建设带来思路。











# **第十七章	组织效能数字化落地实践**



## 17.1	前言

数字化浪潮下，去哪儿网基于公司运营发展的诉求，摸索出行之有效的组织效能数字化提升方案。整体基于六西格玛（DMAIC）方法，从0-1建立产研效能度量体系，从多不多、快不快、好不好三个维度，通过合理有效建模、可视化、数据分析&预警、深度review机制落地、有效的改进解决方案、匹配的管理&运营手段，真正有效做到了让数据说话，帮助产研及时准确地暴露效率问题、定位瓶颈点&问题程度、快速反馈改进效果、沉淀最佳实践、快速复制推广，并通过运营机制做到了持续的改进闭环，带动组织效能呈现上旋式的增长。



## 17.2	背景

疫情对于去哪儿来说，是挑战也是机遇，给了组织修炼内功的机会，那么应该如何决策从哪里着手改进，如何确保组织持续改进，是组织面临的重要课题。驱动因素具体有两点：

1. **管理驱动**

   管理者需要客观数据来了解团队的效能水平：如何评价产能怎么样？资源配比有什么有问题？ 诊断哪些问题需要全局牵引提升，哪些团队、方向需要重点review和改进？改进有没有效果，怎么反馈？

2. **问题驱动**

   产研活动中，已经暴露出一些实际效率问题，比如产品需求长期堵塞，堵塞程度如何？堵点在哪儿？什么原因导致的？对此产研两侧只有主观感知，没有客观数据来统一语言，就更加没有抓手来解决问题。

基于上述诉求，启动了全司产研效能数字化的专项，确定了项目目标：通过数字化，呈现现状、诊断问题、落地改进、持续提升。



## 17.3	实践成果

1. 建立了效能数字化平台，呈现和评价 资源效率、项目交付效率、项目价值度量三大维度的数据表现，高亮呈现效率问题。支持全司-部门-总监-tl-个人多层级数据下钻；支持自定义数据看板等拓展能力。全局概览见下图17-1。
2. 基于数字化成果，经过半年的持续改进，产研交付效率提升15%（交付时长均值10个工作日优化到8个工作日左右）；产能提升10%（单位时间内资源有效利用率从75%提升至85%-90%）

![](/medias/images/efficiency_digital/image2022-9-21_17-31-49.png)

<center>图17-1	概览图 </center>

## 17.4	构建思路

启动数字化时，面临如图17-2的四大挑战，是构建过程中需要重点解决的问题。

![](/medias/images/efficiency_digital/2.png)

<center>图17-2	面临挑战 </center>

1. **数据乱**

   项目管理工具配置不统一导致填写不准确，数据缺失；各部门基础数据处理规范不一致等，主要通过配置统一，必填校验，字段联动校验、按照统一规范初始化数据并建立运维规则（比如有效人数口径）等动作来解决基础数据问题。

2. **指标杂**

   不同部门相同指标的定义、计算口径不一致；各部门信息孤岛，无法横向对比；管理层、tl层、一线同学看到的数据不一样，解读也不一致；主要通过建立指标体系及评价模型来对齐认知（后面在实践案例中重点讲如何构建有效的指标体系）。

3. **定位问题难**

   没有结构化的数据解读思路；关键指标没有合理且定量的基线目标，导致分析依赖主观判断 ；数据颗粒度过粗，问题难以准确定位；主要通过将数据逐级下钻、指标体系分级明确，提炼北极星指标，通过行业调研、历史数据对比等，给出基线指导值；明确指标间的逻辑关系，基于此输出解读思路（比如产品需求资源投入比例要结合当前堆积来看是否合理）。

4. **不可持续**

   间歇性的关注某个指标，靠人肉来统计&驱动，不仅数据质量和效率难以保证，且无横向&纵向数据供观测，常常导致”大运动“式改进，然后改进效果无法保持 ； 人工统计时效性差，导致改进效果反馈延迟，缺乏持续改进的动力；主要通过建立数字化产品，并强化运营管理动作（如设立相关OKR并逐级拆解、周度月度监控等）来保障持续性。



## 17.5	实践案例

- [ ] #### 底层模型

以下将展开呈现2个实践案例，呈现去哪儿的数字化的实践思路和关键动作与成果。在开始讲解案例前，先向大家介绍两个思考模型，如图17-3，是后面具体开展度量的底层逻辑。

![](/medias/images/efficiency_digital/3.png)

<center>图17-3	底层模型 </center>

- [ ] #### DMAIC五步循环改进法

1. 定义[Define]——辨认需改进的产品或过程，确定项目所需的资源。
2. 测量[Measure]——定义缺陷，收集此产品或过程的表现作底线,建立改进目标。
3. 分析[Analyze]——分析在测量阶段所收集的数据，以确定一组按重要程度排列的影响质量的变量。
4. 改进[Improve]——优化解决方案，并确认该方案能够满足或超过项目质量改进目标。
5. 控制[Control]——确保过程改进一旦完成能继续保持下去,而不会返回到先前的状态。

当使用数字化解决具体问题时，我们遵循如上模型来实施整个过程，确保摸清诉求、有效测量、分析、改进、并将改进成果长期保持下去。

- [ ] #### GMQ模型

1. 目标：什么角色，期望是了解？控制？改进？ 什么维度（成本？效率？质量？体验？） 的什么内容（如成本中的 QA资源使用情况）。
2. 问题：提问建模，建立起一个在目标维度上刻画目标对象的模型。
3. 度量：先假设，然后思考要充分可靠地证实假设，最终确定结果指标是什么？支撑指标是什么？制约指标是什么？

在测量这个环节，GMQ 模型能够很好的帮助我们把一个抽象的事情具象化，抽丝剥茧，最终澄清测量的目标是什么、为了回答清楚目标，有哪些问题？针对每个问题如何度量？

基于此可避免进入一个常见误区：从指标出发开始收集数据，而不是从具体明确的目标出发，通过度量来回答一个本质问题。

- [ ] #### **实践案例1-降业务需求堆积**

降业务需求堆积情况是我们使用数字化解决的第一个共性问题，我将围绕以上模型来展开实践过程。

1. **定义：要解决的具体问题是什么？服务的对象是谁？有哪些要求与期望？**

   ![](/medias/images/efficiency_digital/4.png)

   <center>图17-4</center>

2. **测量：基于这些需求与期望，为了刻画现状，需要测量哪些指标？哪些是结果指标，哪些是过程支撑指标，哪些是制约指标？**

   大家跟着我的思路走一遍，我们的目标是减少堆积，那么问题来了，当前堆积现状如何，导致堆积的原因有哪些，这些原因是否真实成立，其中哪些可以治理。那么要回到清楚这些问题，都需要度量哪些数据呢？

   将这个思考过程落实到GQM的结构中，如下图，我们就可以推导出需要看哪些数据，构建哪些指标。对指标做分级分类处理，可得出

   - 核心指标是 堆积量=未排+超排（提前占用资源）

   - 关键过程指标有 排期饱和度（资源有没有用满）、排入产品需求占比（投给产品需求的资源有多少）、非产品需求的资源重点占比（未投入产品需求的资源是否在做重要的事儿）

   - 制约指标是 产品团队的产能（即如何识别堆积的下降不是躺赢，那就需要持续监控产品产能保持高水平；如果产能下降明显，则堆积下降是自然现象，非改进带来）

   ![](/medias/images/efficiency_digital/5.png)

   <center>图17-5</center>

3. **分析——分析在测量阶段所收集的指标数据与目标的关系，找到导致现状的根本原因，以确定后续的改进方向**。

   举例来说，排期饱和度对降堆积的影响是什么？排期buffer越多，会导致饱和度越低。那就代表团队预计可消化的需求越少，意味着 堆积的需求越多。所以首先我们要找出饱和度低的团队，推动其通过合理控制buffer等动作，确保已有资源的充分利用。

   

   ![](/medias/images/efficiency_digital/6.png)

   <center>图17-6</center>

4. **改进——制定可执行的改进方案，并监控执行、验证方案落地的效果。**

   改进这块，大的基调基本是确定基线-找到偏差原因-建立改进规则-回收改进结果。

   ![](/medias/images/efficiency_digital/7.png)

   <center>图17-7</center>

   去哪儿实际改进效果及关键动作

   ![](/medias/images/efficiency_digital/8.png)

   <center>图17-8 去哪儿实际改进效果及关键动作</center>

5. **控制——确保过程改进一旦完成，能通过运营手段保持下去。**

   这一块的具体操作主要是定期报告来监控和暴露问题，但如果确保报告信息的有效传达和高度关注，是做到有效控制的关键。

   去哪儿的实践中，值得一提的是以各级 tl 的周报周会做为抓手，将效能情况列为固定议题，将tl定义为第一负责人，推动各级tl保持自检和问题的高度关注。

- [ ] #### 实践案例2-交付效率提升

交付效率表征了产研交付业务价值的能力，单点的效率提升不足以影响全局，站在业务角度，期望的是需求能够快速流转，高质量的快速的上线验证。所以我们启动了交付效率提升专项。

1. **定义——要解决的具体问题是什么？服务的对象是谁？有哪些要求与期望？**

   ![](/medias/images/efficiency_digital/10.png)

   <center>图17-9</center>

2. **测量——基于这些需求与期望，为了刻画现状，需要测量哪些指标？哪些是结果指标，哪些是过程支撑指标，哪些是制约指标？**

   刻画交付快与慢，我们采用交付时长来看，但是由于去哪儿非固定版本迭代节奏，故需要考虑需求规模、开发并行度对于时长的影响。

   ![](/medias/images/efficiency_digital/11.png)

   <center>图17-10</center>

   为了准确反馈改进效果，屏蔽需求大小变化等干扰因素；其次为了明确指导改进，建立基线目标，所以我们建立了目标GAP值模型来实现这一目的。

   模型建设的思路是将项目大小分多个区间，依据历史数据，测算一个理想环境下的目标时长，将其作为优秀目标指导团队看齐。这样的话每个区间的均值和目标的 gap 值可表征单区间的表现，再结合各区间工作量占比，加权平均后得到一个总 GAP 值，表征团队整体的表现。这样就避免了项目大小变化波动很大对结果指标的影响。

   

   ![](/medias/images/efficiency_digital/12.png)

   <center>图17-11</center>

3. **分析——分析在测量阶段所收集的指标数据与目标的关系，找到导致现状的根本原因，以确定后续的改进方向。**

   为了能够更加系统的分析各数据指标与目标的关系，基于整个项目交付系统，通过拓扑图的方式，梳理清楚底层逻辑。方便我们分析数据，得出结论。

   

   ![](/medias/images/efficiency_digital/13.png)

   <center>图17-12</center>

   

   由图里可以得出，对于结果有明确正向的我们需要强化，负向的需要重点改进，中性因素需要找到平衡点，避免矫枉过正，比如自测自发占比，在我们的改进实践中，综合考虑了其对带来的故障率变大和时长缩短的影响，取某个范围的项目提高自测自发比，而不是全局。

4. **改进——制定可执行的改进方案，并监控执行、验证方案落地的效果。**

   ![](/medias/images/efficiency_digital/14.png)

   <center>图17-13</center>

   

   ![](/medias/images/efficiency_digital/15.png)

   <center>图17-14 去哪儿实际改进效果及关键动作</center>

5. **控制——确保过程改进一旦完成，能通过运营手段保持下去。**

   这部分的实践经验基本和堆积部分的一致，区别点在于由于交付效率是一个系统性工程，故专门成立了专项组，由各部门的研发tl作为负责人+效能专家组成。专项组除了保障交付效率的长期稳定在一个高位外，还需要对数据保持敏感，下钻分析挖掘到效率低洼，分析其改进空间和价值，推动改进。还需要保持定期的交流，将结果显著的工具或者实践传导到其他团队，扩大其影响范围。



## 17.6	总结展望

- [ ] #### 实践心得

没有完美的度量，能够回答问题本质的，就是有效的度量。

没有完美的团队，持续自我反思、成长、沉淀的，就是优秀的团队。

没有完美的流程，具备必要性、可行性、拓展性、灵活性，就是好的流程。

没有完美的管理，能够找准目标、牵引行动、帮助团队和员工成长的，就是好的管理。



- [ ] #### 未来展望

组织效能的终极目标是助力有效的业务价值持续高效的交付，在去哪儿的实践路径分为三步走：

1. v1.0 资源效率，回答团队忙不忙的问题。

2. v2.0 交付效率，回答项目做的快不快的问题。

3. v3.0 业务价值度量，回答做的效果好不好的问题。

   这一块仍将基于 DMAIC 的思路构建。首先是定义，要解决的问题是业务负责人需通过数字化，把握业务重点的效果如何，过程中有没有问题。那么对应的测量需要选定合理的度量对象，如项目集？需求？合理的核心指标，如项目达标率？项目增量收益？如何选择指标要依据具体诉求及团队实际情况来定。除了核心指标，也需要制约指标来确保客观，如项目负向收益、项目ROI等。除此以外，还需要拓展支持效果的重要过程指标，比如资源投入、价值类型分布、需求达标率等，辅助洞察项目效果的好坏以及达成逻辑是否合理。

要做好业务价值度量，是一个系统性工程。数字化仅仅是一个切入点，更大的价值在于向前延申，推动团队进行合理的战略制定和拆解，项目价值&所需资源的充分预估，项目落地的规范化和定期review的质量提升；向后影响，确保项目过程监控的可视化，过程问题&目标问题的及时暴露&及时调整，以确保项目成功。

组织效能度量之路任重而道远，吾将上下而求索，与大家共勉~













# 第十八章	研发过程数字化实践



## 18.1	背景

​        随着互联网走向成熟，新的增长点和机会需要我们进行更加深入的挖掘，这时候数字化就成了我们非常好的抓手，通过数字化的手段我们不仅可以摸清企业当前的现状，还可以发现研发过程的薄弱点并进行改进治理，最终得到长久的提升，助力业务发展。

​        然而所有的技术方案都是为了解决问题而诞生的，来看下我们在数字化之前遇到的问题：

1. **现状说不清**

   我们常常面临的终极三问：1）当前我们的现状怎们样；2）公司横向对比怎么样；3）我们处于行业什么地位。这些不仅是企业管理者关心的问题，也是我们所有岗位管理者、所有岗位执行者应该关注的问题。

2. **重要决策没依据**

   我们助力业务发展，提升研发效能，需要深入挖掘主要痛点并根据优先级进行资源的合理分配，但是优先级和重要性需要客观的数据支撑，但是观察现状，通常是我们发现一个痛点但是并没有量化的支撑就进行改进执行，很多时候发现投入了大量的资源最终却是解决了一个非常边缘的场景，投入产出极低，这在当前严峻的发展形势条件下是是无法接受的。

3. **治理效果难衡量**

   当我们对某一领域进行深入改造治理之后，经常遇到效果难衡量的情况，例如环境这个问题，虽然我们每年都会花较大的成本进行治理，但还是会受到技术人员的吐槽，环境问题也常常会成为项目delay的背锅侠，追根溯源就是我们没有统一的标准定义环境的可用性，也没有有效的手段证明环境的可靠性。

基于以上问题，早在2019年我们公司内部就尝试过数字化，当时研发过程的基础建设也比较完善，我们有标准的项目管理流程，同时也有基于流程搭建的项目管理平台，同时还有基于应用的一站式交付平台，我们将这些平台积累的数据进行简单的清晰处理并对业务、技术同学公开，成为了某些项目的决策依据，也是项目完成情况的评价依据，但是经过一段时间发展我们遇到了以下3个问题：

1. **公司的管理层和决策者无法获取信息的全貌**

   原因是不同的项目定位不同、目标不同、项目成员不同，因此会根据不同的需要获取不同的数据，并对数据进行加工处理，即使相同的数据最终的解读可能也是不一样的，所以加工出来的都是碎片化的信息，对于决策价值有限。

2. **数据信息分散，无法形成统一的标准**

   在去哪儿，我们按照业务区分了不同的部门，每个部门虽然按照公司统一的研发流程执行，但是根据业务规模、团队规模大小不同，还是有一些管理策略、优先级、侧重点的不同，因此基于某个团队现状的治理方案和效果很可能无法在别的部门落地的问题；

3. **没有统一的标准，指导提升能力较弱**

   当时我们的实践是将数据进行统计，提供了非常多的指标，包括过程的、结果的，没有因果分析，没有重要性排序，也没有指导的标准，最终业务线看到一堆的指标却无从下手，失去了度量的意义。

除此之外，当时公司正处于快速发展阶段，业务部门更多的是偏向于业务的提升，对于过程的治理心有余而力不足，最终导致了我们初次尝试以失败告终。然而这两年来，由于疫情的影响，整个行业都进入了稳定运营的阶段，因此在外部驱动力不足的情况下我们应该也有条件花更多的经历提升自己的能力以应对严峻的形势，同时为未来的高速发展做好充足的准备，因此从去年开始，我们结合数字化转型理念，将数字化重新提上日程，同时总结之前失败的经验进行了研发过程模型化度量，并结合管理和运营机制最终实现"摸清底线、建立基线、不断提升中位线"的目标。



## 18.2	实践框架

我们整体的实践框架分为两个步骤：评价体系建立和基于评价的运营提升，以下将从这两个方面分别介绍。

### 18.2.1	评价模型建立

总结之前失败的经验，关键是没有建立统一的模型框架，导致数据使用者没有系统的感知，因此在新模式的探索中，我们首先开始进行模型的搭建，主要分为研发过程角色和度量对象分析、指标模型、完整的度量模型、研发领域的度量场景四个部分。

- [ ] #### 角色和度量对象


![image-20220926203857490](/medias/images/digital/develop_digital_03.png)

<center>图18-1 角色和度量对象</center>

1. **研发活动中的角色**

   在研发管理活动中，我们主要有管理层、一线TL、基层员工三个角色，三个角色的目标和关注颗粒度都是不一样的，管理层关注的是全局、团队现状和趋势以及方向结论性的指标，而基层员工关注具体的改进措施，一线TL既关注改进方向也关注具体的指标。

2. **研发活动中的度量对象**

   我们采用项目进行研发过程的活动管理，而项目交付的载体是一个个微服务（我们内部称之为应用），项目过程中的活动都是人参与的，但是为了避免个人度量导致的反向效果，因此我们只对团队进行度量，因此最终的度量对象分为三个：项目、应用、团队。

- [ ] #### 指标模型


![image-20220926201830875](/medias/images/digital/develop_digital_02.png)

<center>图18-2 指标模型</center>

1. **指标层级**

   整个指标模型分为三层，维度、类型、指标，类型是某些指标的聚合，维度是某些类型的聚合，这种方式满足了不同角色从不同颗粒度进行关注的需求，同时又不会层级太深，避免下钻分析困难。

2. **指标级别**

   根据指标的重要性和当前整个公司的重点关注领域我们对指标进行级别定义，包括P1，P2，P3三个层级，同时给每个级别按照好、中、差、极差进行了阶梯分数定义，类型得分=该类型下的指标得分之和，维度得分=该维度下的类型的份之和，通过这种我是不仅可以让管理者更直观的了解短板，同时也可以更有倾向性的指导一线技术人员提升。

- [ ] #### 度量模型


![image](/medias/images/digital/develop_digital_01.png)

<center>图18-3 度量模型</center>

结合上述的度量角色、对象和指标模型分析，最终我们建立了完整的度量框架，从下到上总共分5层：

1. **数据源**

   数据源包括我们的发布系统、监控告警系统、资源池、数据系统等，为上层的数据湖提供数据来源。

2. **数据湖**

   将数据源的数据按照标准要求进行清洗并做统一的存储，供后续指标的计算。

3. **指标集**

   按照度量维度和场景进行指标的建立，并对指标进行类型、维度的聚合。

4. **评价模型**

   按照上述的指标模型进行分析计算，最终聚合出度量对象的评价结果。

5. **展示层**

   根据不同角色关注的重点，进行分析汇总，包括趋势、对比、聚类等，最终以应用、项目、团队的维度进行分层展示。

- [ ] #### 度量场景


![image-20220926210232436](/medias/images/digital/develop_digital_04.png)

<center>图18-4 度量场景 </center>

我们最终要达到的目标是提升研发过程的效能，因此根据研发过程的具体实践，我们定义了技术五要素：效率、质量、用户体验、成本和技术先进性：

1. **效率**

   即交付速度，交付能力，是研发过程关注的核心，主要需要根据不同角色分开度量，此块在《组织效能模块》中详解，本文不再赘述。

2. **质量**

   在保证交付速度的同时需要保证质量，需要包含过程质量和结果质量供不同的角色使用。

3. **用户体验**

   我们所有的研发过程活动最终的目标是向用户交付价值，用户体验是衡量我们交付效果重要依据，而且用户体验需要关注不同的终端。

4. **成本**

   评价我们的收益需要包含产出，因此成本是我们需要度量的重要模块，包括机器成本、网络成本等。

5. **技术先进性**

   为了保证以上四个维度，我们不仅要有标准的管理规范，同时也需要有过硬的技术，因此我们将技术先进性作为一个度量场景，其中的指标为一些保障架构、效率、质量先进性的工具落地情况，这样既可以推进先进技术的落地，又可以激发技术团队的先进性意识。

### 18.2.2	案例实践

度量的目标是为了指导提升，因此我们所有的实践都是基于以下图18-5的框架。

![image-20220927180625795](/medias/images/digital/develop_digital_08.png)

<center>图18-5 度量框架 </center>

以下我们以研发过程的质量度量介绍完整的实践过程。

- [ ] #### 指标模型定义


质量度量两大重要目标：

1. 日常治理活动效果可衡量，比如 redis，msql 慢查询的治理等。
2. 发现质量薄弱点，提前质量规避风险。

基于以上目标，我们按照如下思路建立度量模型：

1. **责任团队**

   需要在质量领域具有权威性，同时对指标的范围和标准进行定义、解读，后续也负责基于度量效果的持续改进，因此我们建立了虚拟的质量保障组织，由各业务线的QA负责人、主要对接人和基础架构同学共同组成。

2. **度量对象**

   我们研发过程涉及的主要对象包括团队、应用、项目、个人，因此我们需要对所有这些对象进行度量，但是避免度量的负向影响，暂时将个人的质量度量舍弃。

3. **指标范围**

   根据不同的度量对象和研发生命周期（开发，测试，发布，运维）收集质量指标，指标不易太多，避免指标的堆砌导致治理无从下手同时没有重点；主要聚焦结果指标，相关的信息作为下钻分析的依据。

4. **指标层级**

   根据不同的度量对象对指标进行层级聚合，最上层是一个维度的聚合，比如说团队的业务，系统，项目，在维度下系统对应的类型和下钻指标，指标再给详情支持分析下钻，这样能让不同角色快速的获取核心信息。

5. **指标权重及评分等级**

   不同指标定义不同的权重及设置不同的评分等级，方便识别问题的严重性，同时也可以按照优先级进行治理；同时权重评分等级也不是拍脑袋定义，而是结合日常的实际情况，比如本次我们分析了21年各团队的故障情况发现DB相关的故障比例较高，因此我们给其定义了较高的权重。

基于以上分析，我们定义了质量的指标模型，最终的效果如下图18-6：

![image-20220927184828496](/medias/images/digital/develop_digital_09.png)

<center>图18-6 指标模型 </center>

- [ ] #### 模型实现

模型搭建我们采用共建的方式，由各业务线和基础架构团队共建，这样在建设过程中大家可以对模型进行更深入的思考，对于不合理的地方提前发现；我们将这个范围的成员定义为领域专家，这样可以更大程度提升积极性和参与度；

整体的实现架构包括四层：

1. **数据层**

   指标的数据源，包含我们的发布系统、质量门禁、资源系统等。

2. **清洗层**

   根据上层平台的要求将数据进行清洗，此处我们采用了脚本的方式，每个指标都是单独的一个脚本，这样后续如果有扩展，只需要按照标准要求提供脚本就行，避免从上到下的完全变更。

3. **分析计算层**

   包括级别、标准配置和结果分析计算两部分，这种模式很好的解决了基于运营策略动态调整的问题。

4. **展示层**

   根据关注的重点进行结果展示，包括对比、趋势、明细等。

- [ ] #### 试运行

​      模型搭建完成需要经过验证，因为很多场景是我们在前期难以预测的，比如项目BUG率指标=项目BUG个数/项目case个数，当我们真实获取到数据之后发现很多项目都是好的，但是业务线反馈跟实际不符，最终定位原因是很多项目的 case是建在父PMO上的，所以子PMO都没有数据；再比如项目多次发布率，技术同学为了节省流程，常常使用一个PMO重复发布，这样就导致这个指标很差等等，基于以上原因，我们进行了试运营阶段。在试运行阶段，我们建立了专门的用户群，同时每个部门都征集了多名开发、测试人员以及一线TL对我们的模型进行验证，同时对于试运行的用户反馈积极响应，快速解决，最终实现模型的打磨。

​       除此之外，我们基于当前的数据现状质量保障组织的成员也对自己团队的数据进行了摸排和问题分析，为后续的提升奠定基础。     

- [ ] #### 专项治理

度量结果发现了很多问题，因此为了真正发挥度量的效果，我们进行了专项治理。

首先我们基于公司整体的发展目标和各业务线的数据现状，我们对度量对象应用、项目、团队都进行了基线标准定义，思路是不同优先级指标设置不同的达标系数，以下是具体推导过程到过程标准：

![image-20220927201344036](/medias/images/digital/develop_digital_11.png)

基线定义完成，但是由于业务特性、团队规模不同，各个团队的差异性较大，因此我们对不同团队设置了不同的提升目标，对于当前现状较好的要求达标即可，对于差距较大的团队提升10%即可。

其次我们建立了提升的项目组织，组织成员由各业务线对接人和工具负责人共同组成，工具负责人负责解决提升过程的问题解答和工具支持，业务对接人负责各自业务团队的整体提升，这样打破工具和业务团队之间的壁垒，也有利于更好的驱动业务团队实现整体提升的效果。

经过一个月的专项治理，最终我们所有团队都实现了当初定义的目标，同时几个核心业务团队完成90%以上指标的治理；其次大家也形成了通过数据观测团队现状的意识。


- [ ] #### 持续运营

虽然经过专项治理取得了不错的效果，各团队基本达到了基线标准，但是我们期望的公司基线的持续提升，同时为了避免热度过去后的腐化问题，我们建立了持续运营的机制，主要包括三个方面：

- 多等级定义：除基线外，定义优秀和卓越等级，不断激励各团队向上突破；
- 月度复盘及目标定义：按照月度制定目标，我们在每个月末会制定下个月的提升目标，同时会将上个月的治理效果以及之后的提升建议进行分析总结，指导提升；
- 周例会跟踪：每周组织例会进行本周的数据和目标数据对比，及时收集问题并改进；
- 半年度效果激励：为了避免完全管理命令的运动，我们制定了半年的激励策略，包括最终效果、提升比例两个维度，同时也会激励到团队和个人。

## 18.3	难点分析

1.**怎么定义合理的指标标准**

我们的度量模型并非直接指标数据的获取，而是定义等级标准最终进行分数计算，这种方式带来的好处是可以有直观的量化认知，同时也便于整体横向对比，但是也增加了度量的难度，因为每个指标都需要给出合理的标准，而且需要兼容不同的场景，一旦定义标准就会导致大家对模型的不认可，影响后续的持续提升

解法：

- 基于现状数据定义基准：比如当前我们整体测试的行代码覆盖率在75%左右，我们就以此为准，首先保证低于标准的团队达标，后续在持续提升标准；
- 支持不同的团队定义不同基准：比如机票和酒店业务体量和团队规模不同，我们就给这两个团队定义不同的故障率标准；
- 尽量采用增量指标：对于我们这样发展历史比较悠久的公司，有很多历史包袱，虽然会有隐患，但是治理投入产出比不高，比如sonar历史问题，因此我们采用增量指标，比如说月环比、某个时间段之后的增幅等

2.**怎么避免虚假繁荣**

“度量什么就会得到什么”，这是做数字化的人经常听到的一句话，主要是不同层级的诉求和目标不同，管理层希望度量能作为决策的依据，而一线技术人员觉得跟自己无关，因此为了完成管理目标就会对数据进行造假，举个例子：我们开始的指标里覆盖率拦截率指标，开发同学虽然开启了拦截，但是把标准设置为0，这样虽然导致了数据美观，但是没有达到真实的观测提升效果

解法

- 尽量采用复合指标：指标尽可能是综合结果，比如上述问题中的覆盖率拦截率，我们要保证覆盖率合理标准下的拦截率；
- 尽量采用逻辑性指标：比如说代码的健康度度量，我们尽量避免使用代码行而是采用复杂度、问题数、重复度这种逻辑性指标；
- 保证度量维度的指标覆盖度：这样能够从多角度去体现度量效果，也能保证结果的准确性
- 建立配套规范标准：我们的很多指标是基于规范采集的，因此需要有通过的规范标准，而且还要保证规范的落地执行，比如说配置权限回收、步骤卡点等，这样才能保证最终效果

**3.度量是否要纳入考核**

有些度量实践常常会将度量纳入考核，这样对员工无疑是一种压力，也无法保持长久的提升效果

解法

建立统一的度量模型：通过模型建立实现上下的术语对齐，这样度量就成为了有效的日常管理工具，比如说管理汇报、团队周会、项目效果评价等；同时模型可以运动式的调整，根据团队所处不同阶段进行动态调整，这样也实现了持续提升的效果；

一线TL的考核标准：管理层思考的是战略方向，基线员工关注的是解决问题，而一线的TL是需要两者兼顾，因此我们可以将度量做为其绩效考核一部分，这样既可以对上有较好的成果展现，对下也可以实现标准化的管理

## 18.4	总结规划

  总结我们的数字化的建设过程，有以下三点感悟：

- 我们要做的是基于数字度量的整体提升，而不是高分攀比；
- 度量只能是一个结果的展示，具体的问题下钻还需要更多的数据支撑；
- 度量不是目标，推进改进提升才是目标。

​     目前我们已经通过上述实践过过程落地了技术先进性、质量度量和用户体验三个场景，而且经过半年的运营，各个场景都实现10%以上的效果提升，而且已经形成了基于数字化的提升机制，如果有其他场景可以快速的适配建立，后续我们也将基于此模型扩展更多的场景。









# 第十九章	数字化办公



## 19.1	数字化办公的背景与意义

### 19.1.1	背景

数字化转型是互联网时代，企业重塑核心竞争力的必由之路。数字化已然来到，企业考虑的已经不是需不需要数字化，而是数字化如何落地应用。所以，各个行业都在筹划，如何在企业运营与管理的各个环节，实现与数字化的深度融合，理解数字化对原有业务的赋能原理、赋能方法，将数字化技术这个全新的生产要素的融合、创新的价值发挥到最大，从而解决企业运行过程中的效率提升和成本降低问题。

基于此背景，去哪儿网针对于协同办公效率方面进行了数字化变革,具体背景主要体现在以下两个方面：

1. 通过观测产研交付过程中交付效率与交付质量的指标数据，识别出项目交付过程中各个阶段效率低下且待提升的点，借现象深挖原因，提高各个阶段的效率。 例如：

   - **在需求交付时长中：**ideaFR 与 需求FR 耗时过长，需求FR 的通过率较低等。

   - **在研发交付时长中：**排期等待时长耗时过长，联调耗时发布 delay 率过高等。

2. 通过实际深入业务团队的调研，识别出项目流程和协同过程中的阻碍效率提升的痛点。

   - **项目管理流程：**项目流程断点，不规范、不闭环，流程人工驱动、信息触达率低、数据缺失且不集中等。

   例如：项目管理流程存在断点；审批流程主要是靠人通过电话或邮件的形式驱动，信息触达不及时且处理结果无反馈无感知；对于人的管理 主观驱动居多，对人和团队数据分析展示少且不完善。

   - **项目协调方面：**项目跳步、关键节点无感知、信息不共享。

   例如：项目关键流程节点无感知，导致项目跳步；项目消息多且分散且不透明，项目成员对项目整体的进度无感知；项目过程中发现的问题责任不明确，问题不能及时解决的现象层出不穷，从而导致整体项目效率低下，进度缓慢；

   - **日常办公率低下、日常事务多而杂乱、提醒无感知。**

   例如：RDQA 会议与需求FR 会议人员无法准而全的到达；日常待办事务处理不集中且提醒不明显，导致重要事项的遗漏。

### 19.1.2	意义

1. 实现数字化智能化的工作方式，解决企业的内部沟通与协作，提高企业整体运行效率。

2. 实现企业场景化安全沟通协作，提升协同办公效率，降低运营成本，赋能员工和团队，激发组织活力，助力企业数字化转型。

   

## 19.2	数字化效率办公的作用

![image](/medias/images/digital_office/qunar产研交互流程.png)  

<center>图19-1 产研交互流程</center>

在如上图19-1公司整体的产研交互流程中，协同效率办公贯穿整个产研交互流程的方方面面，去哪儿网针对于协同办公效率方面的数字化变革，主要建设方向为：项目协同与日常办公。



## 19.3	数字化效率办公的设计理念 

### 19.3.1	去哪儿网项目协同整体建设思路 

去哪儿网产研交互流程中，项目协同贯穿整个项目价值交付流程的各个环节，并起到重要的作用， 针对于此 去哪儿网项目协同整体建设思路如图19-2：

![建设思路](../../themes/matery/source/medias/medias/images/digital_office/建设思路.png)

<center>图19-2 项目协同流程</center>

### 19.3.2	总体业务模型

![总体功能](/medias/images/digital_office/总体功能.png)  

<center>图19-3 业务模型</center>

如图19-3所示，我们的总体业务模型是基于现有的公共服务系统，通过一系列信息触发工具，制定恰当的通知机制，打通项目流。主要实现以下几方面的目标：

1. **智慧沟通，精准协作**
   - 创建集中式的项目沟通渠道，精准沟通，保证项目交付过程中信息的集中度与透明度。
   - 项目关键节点自动通知提醒、跟踪、反馈、催办、升级等，提高项目沟通协同的高效性。
   - 项目工作台：一站式的项目操作控制台，无需切换，保证项目协同效率的提升。    

2. **赋能团队，激活组织**
   - 赋能团队，保证项目流程的顺畅度，提高团队的自组织与自驱动能力。

3. **在线化移动化办公，智慧协同**
   - 线下流程线上化移动化，提升项目过程中的办公、日常事务处理、审批效率等。

4. **业务协同，连接开放**

- 可被其他业务应用灵活接入，实现业务与协同真正无缝融合，让业务协作无界限。

### 19.3.4	项目协同提供的功能与特色

去哪儿网针对数字化效率办公的落地建设过程中，主要包含三个方向：

1. 实现项目流与工程流之间的完美结合。
2. 建立高效便捷的团队沟通、通知、升级、反馈机制。
3. 打通整个项目流程，把线下流程线上化移动化。

针对于此，我们深入业务团队，同时结合数字化建设思路，精准识别项目流程和协同过程中的痛点和断点，最终通过一系列行之有效的建设和实践，让整个项目流程更加顺畅，团队协同更加高效与和谐，交付效率更是得到了可观的大幅提升。

下面针对打通项目流程与协作沟通这两方面的建设进行详细介绍。

1. **打通需求阶段项目流程，实现流程的线上移动化，状态自动流转**

   - **针对痛点**

     项目流程断点，不规范、不闭环，流程人工驱动、信息触发率低、数据缺失且不集中等

   - **具体方案介绍**

     ![特色1](/medias/images/digital_office/特色1.png)  

     <center>图19-4 方案模型</center>

     - 识别阻碍需求产出效率的因素

       1）比如项目需求阶段必要流程：IDEA FR 、产品需求FR 、技术需求 FR 及需求变更等流程，通过邮件形式审批，沟通审批及处理时长增加，导致需求产出时长中 ideaFR 与需求 FR 耗时过长。

       2）需求状态的流转需要通过人工手动修改驱动，操作繁琐费时；需求阶段过程数据没有自动记录，导致过程数据缺失，导致后期效率度量数据的不完整性。

     - 摆脱阻碍

       梳理现有线下审批流程，借助现有的基础服务（项目管理系统、审批系统）、即时通讯工具（ IM、Email、IVR ）、信息处理渠道（待办系统、工作通知），将链路中的问题纳入系统，从项目管理系统（ JIRA ) 触发审批流程开始到审批结束自动流转需求状态，打通项目的流程，实现了审批流程的线上化移动化、自动记录审批流程、流程闭环以及流程数据的缺失，摆脱阻碍，提升了需求产出的总体效率。 

   - **最终收益**

     - 智能化线上化移动化的审批流程使用范围覆盖去哪儿网全部业务系统。

     - 项目阶段总体审批效率整体提升50%。

     - 提升了需求产出效率。

     - 实现了流程打通与闭环：流程自驱动、状态自流转、过程信息自动周知记录。

       

2. **高效便捷的团队沟通、通知、升级、反馈机制**  

   - **针对痛点**

     项目流程跳步、关键节点无感知、项目信息分散且不透明、项目流程繁琐、流程人工驱动。

   - **具体方案介绍**        

     ![特色3](/medias/images/digital_office/特色3.png)  

     <center>图19-5 方案模型</center>

     -  建立了集中化与透明化项目消息中心--项目群，从新建项目开始，自动创建项目群，自动监听项目人员变更自动拉取项目成员；项目成员信息、项目关键流程节点信息、重点事项周知都会在项目群中进行，项目关键流程一键操作，省事省力；解决项目信息分散不透明，关键节点无感知、项目流程人工驱动问题；
     -  借助项目群功能，建立了一站式项目工作台，满足多角色、多角度的一站式的操作，无需跳转系统，一键操作处理；项目总体进度及各个子项目的进度一览无余，解决了项目流程繁琐的问题，提升项目协同的效率；
     -  为了保证项目信息有效性，梳理和精简了现有项目流程中的关键节点信息，区分消息类型（普通提醒消息、需要后续操作的消息），制定了不同的消息提醒模板；例如：项目协作过程中，针对与提测delay消息的提醒及后续操作处理；为了保证项目过程中消息触达及问题处理的效率，制定了一系列灵活可配置的通知策略，自动跟踪记录和将处理结果周知给问题发起人及相关方，改善了项目人工驱动的问题，提高了问题解决的效率和质量。

     **（3）最终收益**

     - 项目协同效率提升10%。

     - 提升了项目交付效率。

     - 节约了沟通协作的成本。

     - 简化了项目操作流程，防止流程跳步。

     - 赋能团队，实现了自驱动的项目协同模式。

   

## 19.4	未来规划

随着业务数量与规模的扩大，协同办公在项目交付过程中的作用越来越重要，公司对数字化办公的依赖和要求也越来越高，针对与目前的现状和未来的挑战，去哪儿网数字化办公未来的规划的方向主要体现在以下四个方面：

1. **更加健全完善的高效沟通机制**

   聚焦具体的沟通场景与实践，让信息更加高效的流转，借助公司现有的 IM 工具，保证重要信息流更加清晰且不被遗漏，工作消息更加智能合理的分类，让项目协作变得更加顺畅，更加高效。

2. **项目管理**

   打造一站式的项目管理模块，打通现有的工具平台（项目群聊、知识库、项目管理系统等），保证项目流程能够更加顺畅，提高项目交付效率。

3. **日程管理**

   - 精准定日程，可同时查看多人、每小时段的闲忙状态，轻松避开冲突时间；选中匹配所有人的空闲时间，立即发送日程邀请，更高效。
   - 将日历行程与项目排期相结合，实现项目自动排期，保证项目任务有条不紊的进行。
   - 更加丰富的提醒方式，保证每一个日程都能准时提醒与参加。
   - 对接现有的待办系统，保证重要事项不被遗漏。

4. **文档协同和知识库管理**

   文档协作和知识沉淀，帮助团队向知识性组织迈进，支持文档关联项目任务，项目文档与知识信息条理清晰的呈现。





<img src="/C:/Users/miaoi.liu/AppData/Roaming/Typora/typora-user-images/image-20220927125736261.png" alt="image-20220927125736261" style="zoom:150%;" />
