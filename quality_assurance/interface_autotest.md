---
typora-root-url: ./..
---

# 自动化测试实践

## 1 背景

### 1.1.旅游业务概览

![image](/images/autotest/1.1-1.png )

qunar是典型的互联网旅游电商平台，那么它就有着旅游业业务复杂的特点。首先，它的版图比较大，涉及到的业务种类繁多，举几个例子，比如出行服务，有的用户会选择坐飞机，那么就涉及机票业务，有的会选择坐火车或者汽车，那么就涉及火车票汽车票的业务，还有选择自驾出行的同学，那么就涉及租车用车业务。再比如住宿服务，大部分同学会选择酒店，那么就涉及酒店业务，有一部分同学为了体验风土民情，可能会选择当地的客栈，还有一部分同学考虑到资金成本，尤其是一些学生，可能会选择民宿。

除了这些大类之外，在每个类别内部，业务也是突飞猛进的发展，我们以机票业务举例，以前我们可能只是卖票，现在更多的可能是关注用户体验，为用户提供完整的商品包的概念，定向的为用户提供一些服务，提高用户的购票流程体验。与此同时，还有不断探索的旅游新玩法，比如机酒服务，甚至门票服务。机票和酒店虽说是旅游行业中很重要的两个元素，但是在qunar内部，这是两个独立的业务部门，也被当做独立的业务线单独运营。但是现在，我们逐渐让这两个业务产生交集，用户可以在qunar的app上感受一条龙服务，比如如果用户想来北京环球影城玩，那么使用qunar的产品可以一条龙买完机票订完酒店并搞到门票，体验方便简洁的购票流程。

![image](/images/autotest/1.1-2.png )

![image](/images/autotest/1.1-3.png )

那我们具体看下，在5年甚至10年间，我们的业务有哪些变化，我们以机票业务举例：

几年前用户买张机票那就是买张机票，很纯粹的需求，付钱买票，按时出行。但是现在，随着服务意识的增强和提高，在用户体验提升为目标的鞭策下，各平台提供了更多的贴合用户的服务，比如包装产品，自助值机，附加服务提供和附加商品搭售等等，可以说只有你想不到的没有平台没法提供的。用户在购买界面上能看到更多元化也多样化的产品形态。

我们对比一下以前和现在业务的关注点，几年前，用户更关注的是基础信息，第一个维度，比如起降地，出发日期，仓位信息，第二部分，航程信息，比如是直飞还是中转，航司信息，因为有的同学可能在固定的航司办理了会员，所以就会有积分和里程服务。第三部分，航站楼信息，经停点信息，以及起降时间点，这和用户的行程安排息息相关。

![image](/images/autotest/1.1-4.png )

那么现在的机票业务，如我们刚才提到的，更关注用户的体验，我们将机票当做一个商品来看待，形成定制化的打包服务。目前qunar已有的打包服务很多，这里我们只列举几个典型的套餐，让大家感受一下它的具体含义和价值体现。这里我们列举了四种典型的套餐，分别是商旅套餐，退改套餐，极速出票和机酒套餐。

![image](/images/autotest/1.1-5.png )

商旅套餐是专门为商旅人士打造的套餐，他们的特点就是不差钱，但是对于整个旅行过程当中的体验很看重，所以对于这样的用户我们主要为他们提供了一些增值服务，提高旅行体验。退改套餐主要是为旅程灵活度较大的用户准备的，他们有很大的倾向或者可能退票或者改签，退改套餐能够给这类用户最大的出行自由度，同时能够以较低的价格进行退票或者改期行为。极速出票套餐是给那些临时起意出行的用户准备的，比如老板突然派你明天去出差，那么你可能希望你的机票很快出票完成，因为临近出行，能尽早拿到票就能避免一些意外情况发生。机酒套餐是一个组合套餐，用户在qunar上购买机票的时候可以获得酒店的优惠券，可以在当前购买机票的页面直接用券订酒店，不止操作流程简单，也更加划算省钱。

### 1.2 系统和业务迭代

为了承载上述的业务逻辑，我们系统也变得越加的复杂，我们可以对比看下我们业务系统的迭代变化：

![image](/images/autotest/1.2-1.png )

左图是几年前的一个近似大单体的服务，叫做tts，翻译过来说the total solution，顾名思义，所有的解决方案都在这里完成。再看右图，现在的业务系统是分层结构的，我们可以看到按照功能聚合划分之后，整个系统分为6层。当然tts的功能还是存在的，只是沉在了现在系统的最下面一层，提供基础报价信息，在它的上面，我们可以做各种花样玩法，包括刚才提到的业务上的演进，还包括一些技术上的优化，比如缓存，报价计算优化等等。

除了系统的维度，承载这复杂业务的后台数据也同样变得复杂起来，比如上面提到的套餐，为了承载套餐提到的功能，那么我们就需要这么些个数据节点去包含这些业务信息。所以用户看到的报价信息对应的背后的数据结构也愈加的复杂：

![image](/images/autotest/1.2-2.png )

与此同时，公司的研发流程其实也越来越规范，比如说敏捷开发，OKR管理，MVP模型的使用，milestone里程碑的使用等等，导致项目的交付速度加快，交付的周期大大缩短，做到了真正的持续交付。那么交付频率的提升带来的是测试频率的提升，按照之前人工的测试的回归过程，不仅要分析代码的改动范围，还要去决策回归的范围，然后得出回归的一个成本。同时，在那些人为决策之后，决定不需要回归的部分，其实经常发生漏测导致线上问题的状况。

![image](/images/autotest/1.2-3.png )

![image](/images/autotest/1.2-4.png )

真正进入到测试过程中之后其实问题更多，比如说你要测试的系统的依赖很繁杂，它的上下游也不稳定，数据时有时无，构造case也会消耗大量的精力，但是覆盖度却无法得以保证。

![image](/images/autotest/1.2-5.png )

除了上下游依赖之外，测试对于流程的依赖程度也很高，举个例子，对于机票购票流程来说，它其实包含了从搜索到booking到生单再到支付，如果有后续退票改期的话，还需要走售后流程，整个流程环环相扣，很难只从其中的某一个环节单独测试，因为比如说booking依赖于搜索得到的数据，生单又依赖于booking得到的数据，这里的数据不只是接口返回的数据，还有一些其他存储介质中暂存和缓存的数据，导致的结果就是比如说我想测试生单，那么势必要把搜索和booking先走一遍。

![image](/images/autotest/1.2-6.png )

最后我们再通过数据，更直观的感受下，图里显示的是我们qunar某事业群一个季度的故障数据统计，漏测占比达到了66%。而公司线上高级别故障因为漏测导致的占比也在21%。可以看到这个数据是比较高的。

![image](/images/autotest/1.2-7.png )

### 1.3 概述

经过分析，我们可以看到，测试和发布的工作量比较大且重复。并且在dev负责质量的背景下，问题更加凸显，主要有几个痛点：

- 回归型测试占比大
- 人工验证成本高
- 环境管理困难

![image](/images/autotest/1.3-1.png )

总结一下，大概分为五大类：

首先是测试维度，我们如何能够将测试维度，也就是我们所谓的checklist，沉淀下来，能够代代相传，同时随着业务变化能够不断更新。然后测试的有效case，我们如何能够用较低的成本去生成实时性较高的测试用例，并尽可能的保证case的覆盖度。第三点是测试环境，我们如何能够降低测试环境的维护成本，尽可能保证测试环境的可用性和线上的一致性。第四点是测试分析，我们如何能够降低用户排查问题的难度，缩短排查问题的流程，能够让用户精准定位问题所在。最后一点是持续集成，我们如何能够将自动化测试融入到线上的CICD流水线当中，用户只需要在几个关键步骤上感知自动化测试的执行和结果即可。

除了痛点之外，我们再看下qunar的测试工具现状。总结下来，可以有如下几种，手工测试，qunit接口测试，querydiff接口测试：

手工测试（以机票售卖场景为例）：

- 编写checklist
  - 5种航程类型*70个产品tag*乘机⼈*……
- 构造⽤例
  - 从订单数据等来源找到⽤例，并修改测试配置
- 部署环境
- 执⾏测试
- 分析结果

![image](/images/autotest/1.3-2.png )

整个过程枯燥且让人焦头烂额。

qunit接口测试：

- 维护⽤例
  - 把⼊参和断⾔维护到qunit中
- 部署环境
- 执⾏测试
  - 判断结果是否符合断⾔
- 分析结果

querydiff测试工具：

- 线上流量录制
  - 按照⼀定的采样率录制参数和结果
- 部署环境
- 执⾏测试
  - 把前⾯录制的参数和结果回放
- 分析结果

针对这些情况，有必要实现一个自动化平台来辅助dev/qa做到更好的交付结果，降低因为回归漏测导致的bug甚至故障，从根本上解放人力，让dev和qa能够投身于更重要的事情上，而不是机械的去做一些重复性的枯燥的，完全能够被代码取代的工作。

根据上面的痛点可以知道，自动化平台至少需要满足几点要求：

- checklist和case管理
- 接口对比测试和结果分析
- 环境管理
- 应用元信息管理

那么灭霸应运而生。

“灭霸”这个名字的含义，按照复仇者联盟灭霸带上原石手套之后的能力：随机消灭一半bug！

![image](/images/autotest/1.3-3.png )

在下面的篇幅当中我们逐一的阐述我们所做的探索，包括踩过的坑，有过的创新，总结的教训以及积累的经验等等。

## 2 case智选

![image](/images/autotest/2-1.png )

我们首先来看一下灭霸的系统结构。灭霸的整体架构比较复杂，这里只列出几个核心模块，包括灭霸本身的四大模块，以及和外部交互的模块。最上层的黄色部分的模块是用户能够比较能直观看到的模块，即UI部分，它包含了配置管理部分，用户可以在这里配置需要进行测试的应用以及接口信息，还有测试管理部分，用户可以根据自己的需要去进行测试的调度。还有项目报告部分，用户可以看到自己执行的测试的结果信息和各个不同维度的分析，还有测试历史，用户可以看到某一个应用在某一段时间内执行的情况和趋势。

第二部分中间靠左边蓝色的部分是测试管理控制模块，这一部分对于用户是黑盒的，这里定义了测试执行流程中的各个步骤，从测试的诞生到测试的编排，从case的生成到应用的部署，再到case执行和结果diff，最后到测试报告的生成和结果分析展示。

第三部分是中间靠右边蓝色的部分，是数据控制模块，里面包含了ELK数据，用户文件上传的数据，API对接接口数据等，这些数据都是用来生成case的。后面还有mock数据，主要是用来进行一种更特殊更复杂的测试场景，就是录制回放模式的测试，需要mock数据的支撑。

第四部分是绿色部分，包括了录制回放模块需要的各个组件，主要是以阿里开源的jvm-sandbox-repeater组件为核心，在上面进行定制和二次封装，包括了入口的录制回放，子调用的录制回放，入口的白名单和采样配置，子调用的黑白名单配置，以及最重要的各个不同组件的录制回放插件，比如redis，db，rpc（http/dubbo）等等。

第五部分红色部分是qunar内部的其他组件，包括了QSSO单点登陆，邮件，IM系统qtalk，ELK日志数据，qtrace全链路追踪系统，QMQ消息中间件等等。

看完了整体系统架构，我们接下来看一下细节。首先是case智选，这里主要解决的是之前提到的两个痛点，测试维度的固化，和case生成问题。

### 2.1 case生成方式和测试维度自动更新

对于测试维度来说，比较重要的两个概念就是checklist和case了。对于checklist来说，诉求是能把各系统重要的点都固化下来，支持增删查改，能够方便的检索。

- 多维度：可灵活的自由组合，界面上可勾选。
  - 航程类型
  - 是否带儿童
  - 报价源：政策/旗舰店
  - 政策类型
  - ...
- 初始化：默认初始化部分维度叉乘的checklist点列表。

对于case来说，要尽可能减少维护的成本，并且保证实时性/有效性。

- 至少覆盖无线和www
- 覆盖足够多的产品形式

按照我们对业务的分析，业务系统一般具有如下的特点：

- 业务系统的测试维度是相对稳定，维度中的取值范围可能随业务迭代不断变化
  - 配置到⾃动化测试平台中
  - 多维度：航程类型、终端、乘机⼈…
- 项⽬checklist是笛卡尔积结果的⼦集
  - app端单程的低价特惠的⼉童报价

![image](/images/autotest/2.1-1.png )	

测试维度固化之后，通过笛卡尔积生成checklist

![image](/images/autotest/2.1-2.png )	

![image](/images/autotest/2.1-3.png )	

![image](/images/autotest/2.1-4.png )	

这份checklist就是一个全集，比如一个1成人0儿童，直飞航程的搜索条件就是这个checklist全集中的一条记录。我们将直飞航程换成往返航程，其他约束条件不变，那么它就是checklist全集中的另一条记录。

case生成来源包括如下几种：

- 根据测试维度⽣成⽤例
- 数据来源：
  - ELK业务⽇志(kv解析、json解析等)
  - 自动化测试和排查问题共享数据
  - agent标准化记录请求入参打印日志
- 时效性

![image](/images/autotest/2.1-5.png )	

有了checklist之后，我们就可以用它来生成case了。我们首先看一下数据来源，这里我们使用的是ELK数据作为原始数据，为什么用ELK呢？在之前提到痛点的时候，我们提到了，我们希望用相对较低的成本去获取case，而我们的业务同学为了排查线上问题，已经在线上打印了接口的请求和响应数据，并通过ELK收集起来，那么我们也使用这部分数据，做到自动化测试和业务线排查问题共享数据，减少数据冗余带来的成本消耗。

然后我们这部分ELK数据和checklist一起去生成case，这里我们可以把checklist想象成一个漏斗，ELK数据流经这个漏斗，和checklist中的各个维度进行匹配，匹配上之后就生成了属于该checklist某一条记录对应的case。除此之外还要另一种生成case方式，就是无规则采样，相比之下，这种方式更加的简单粗暴，因为它省略了checklist匹配的流程，这样会导致生成的case对于checklist集的覆盖度不可控，因为它完全取决于采样的结果。当然也不是说这种方式就一无是处，后面我们会提到使用这种case生成方式的场景。

最后是case的实效性，因为ELK里的数据是用来排查实时线上问题的，所以它的实时性得到了保证，我们复用了同一份数据，那么case的实效性也就得到了保障。

case维护的一个重要问题是测试维度的更新问题。有的时候，业务迭代会新增某个维度的内容，就是说扩大了这个维度的取值范围，比如售卖终端又多了一种渠道，叫分销渠道。这种业务分支新增的情况如果都需要人为处理的话，会有一定的人力损耗。我们会在做ES和checklist匹配的过程当中，自动识别出checklist维度取值范围之外的值，同时补充到checklist已有的取值范围中去，实现维度自动扩充的一个效果。

![image](/images/autotest/2.1-6.png )	

![image](/images/autotest/2.1-7.png )	

反过来，如果一个业务维度已经很久没有case覆盖，说明这个业务维度已经不存在了，比如售卖终端中的分销类型，某个代理商只和qunar签订了半年的合约，那么势必半年之后这个渠道就不再有报价了。那么评估一个业务维度的消亡，它不是一锤子买卖，这里我们引入了一个叫做周期覆盖度的概念，如果长时间一直没有覆盖的维度，我们才认为是是消亡的维度。同时，配合上面提到的自动补充逻辑，当一个维度周期的取值没有被覆盖，导致被删掉之后，过了一段时间又重新出现，那么也会重新被补充到checklist当中去。	

![image](/images/autotest/2.1-8.png )	

![image](/images/autotest/2.1-9.png )	

下面说下case的生成策略，也有几种，对应不同的场景和优先级。比如说，用户测试的时候，实时触发生成的case，这部分它的时效性最高，优先级也最高，是生成case的首选。但是因为线上实时产生的日志量非常的大，尤其是搜索场景，所以日志的筛选是在一个时间范围内的，比如说过去的三个小时之内，这三个小时的场景覆盖可能会有遗漏，所以还有一个逻辑是定时补充，线上会有定时任务每隔N小时去补充case，这里的补充当然是定向补充，会选取那些没有覆盖的checklist进行补充。那么经过线上长期的运行，已经证明，这两种策略可以获得足够高的覆盖了。不过有的系统因为有些特性，可能存在长尾的情况，比如一些冷门的产品，用户购买的频次就是很低，那么这些case一旦出现，我们就不能放过，因为后面很长一段时间都可能很难出现了，这种case我们要固化下来，在后面的测试中直接使用，进一步的提高覆盖度。

### 2.2 case生成策略时机以及case有效性

生成的这些case也是需要维护的，这里主要指的是case时效性的检验，举个例子比如说机票搜索场景，用户在今天搜索了一个2月1号的航线，这个搜索条件被我们的系统加工成一个case，那么在2月1号之后，我们测试的时候如果使用到了这条case，它搜索的就是一个过去时间的航线，自然就过期了。 其实对于那些通过ELK实时生成的case，我们不需要太过担心，因为它的实时性能够得到保证。但是对于定时补充或者固化产生的那些case，它们的新鲜度可能会比较低，尤其是固化下来的稀疏case，可能是几天、几周甚至几个月前的数据，这种case是要被剔除的，因为它验证的代码流程是不符合预期的。还有一些场景，比如某条航线的机票售完了，那么结果可能会提示用户该航线已售罄，但是我们关心的核心逻辑是报价的计算和交易流程，这个时候可以配置一些关心节点，比如报价信息节点，如果没有这个节点，我们就会认为这个case也不再有效。 对于稀疏的case其实，它的产生非常不容易，如果只是因为日期过期而舍弃掉这个case的话，代价有些大，这个时候我们会使用类似于数据偏移的策略，修改它的触发日期，来起到续命的效果。这是一种在全链路压测领域中也经常采用的策略。

![image](/images/autotest/2.2-1.png )	

### 2.3 其他特殊场景

最后我们补充说一个相对来说比较特殊的场景，叫做入参单一问题。有的场景入参条件包含的信息量非常有限，比如说我们根据订单号去查询订单详情，订单号本身它只是一串数字和字母的组合，不具备丰富的业务语义，所以我们无法根据订单号这个维度来保证业务维度覆盖的程度。但是其实订单本身是有业务语义的，只是这个维度在结果中而不是参数中，对于这种场景我们就需要将结果和入参绑定起来，通过结果来匹配checklist。 还有一种更特殊的场景，它的业务维度既不在参数中也不在结果中，而是在中间过程中，比如说系统根据参数先去查询出一个中间结果，然后根据中间结果再去做其它操作，那么这种就需要特殊对待，同理，我们需要将这部分信息和入参绑定起来作为case，完成checklist的匹配。

![image](/images/autotest/2.3-1.png )	

## 3 测试环境治理

case的问题解决了，我们来到测试环境问题这个topic。 测试环境的管理，尤其是资源的消耗，其实一直是一个很令人头疼的问题，这里我们主要围绕两个方面去展开，首先会看下我们现阶段的测试执行是如何同环境挂钩的，然后会看下我们的测试平台如何借助公司内的新产物“软路由环境”，来帮助我们解决之前测试环境存在的问题。

最开始先说一下我们的这个测试执行流程，当代码发生变更或者CICD流水线中的某个环节触发了自动化测试，平台会得到通知进行环境的准备，代码的部署等等一系列事项。qunar内部的项目，是通过PMO（也就是所说的jira）来管理的，如果只有一套测试环境的时候，同一个应用有多个PMO同时进行的时候，就涉及到环境的分时复用和资源抢占问题。

![image](/images/autotest/3-1.png )

所以环境池这个最简单的方案就被用来解决这个问题，测试平台会为我们的用户，预先准备好N套环境，这个N的取值大小，会根据该应用项目的热度来决定，也就是说，项目并行度高的应用，就会给它更大的环境池来解决排队问题。但是往往一个应用的测试和它的上下游脱不开干系，所以我们不止要为该应用准备环境，它的上下游都需要准备好，而且能够串联起来，对于机票酒店这种复杂业务场景，一整套环境会涉及上百个应用，那么环境池对于资源的损耗会大大提高。

![image](/images/autotest/3-2.png )

对于为了应对上述问题，“软路由”闪亮登场。我们首先介绍一下，什么是软路由。 图中有三个环境，第一个环境base env我们称之为基准环境，这里我们先假设这套环境中有6个应用。按照普通环境的玩法，只要有一个PMO中包含了这其中6个应用中的1个或多个，我们就需要单独去搭建以这6个应用为一组的测试环境，来做测试使用，N个PMO，就需要N套环境。但是在软路由情况下，我们的测试环境其实不需要是这样六位一体的一整套环境，而是需要测哪个应用就单独搭建该应用对应的环境，而它的上下游会因为软路由的功能统一路由到base环境上。举个例子PMO1的项目涉及应用A和D，我们就单独搭建一套test env1的环境，这个环境中只包含A1和D1两个应用。同时PMO2涉及应用A和D，当然也可以是其他应用，那么我们就单独搭建另一套test env2环境，这个环境只包含A2和D2两个应用，test env2与test env1这两个环境完全隔离。假设我们的业务调用链路就是图中的A->B->C->D->E->F，那么软路由的路由能力，能够使PMO1的测试流量从入口路由到test env1的A1，然后回到基准环境B，再到基准环境的C，再到test env1的D1，然后继续回到基准环境的E和F。对于test env2同理，会走一条A2->B->C->D2->E→F的这样一个路径。在这种架构下，对于环境中包含上百个应用的大环境，但是测试又只测几个应用的场景，软路由占用的资源肉眼可见的少，对于资源的节省是非常非常可观的。

![image](/images/autotest/3-3.png )

## 4 录制回放落地

接下来说一下相对复杂的一个测试场景，录制回放。录制回放落地这个章节主要会从两个方面展开，首先是录制回放模式所应用的场景，以及实现的选型，然后是录制回放模式下排查问题困难这个痛点的一个细化的解决方案。

说到录制回放，我们先聊一下自动化测试的两个维度。第一个维度就是接口的diff，主要应对查询类型的测试场景，也就是“读”场景下的接口自动化diff测试。第二个维度，就是所谓的录制回放，它应用的场景比较多，比如下游接口不稳定导致不幂等，同样的请求有时候能返回结果有时候却超时或者因为其他原因，没有办法返回正确结果；再比如下游没有数据，有些测试环境中，数据残缺不全，导致一些case测试的时候没有数据没有结果；还有的下游接口的调用可能会消耗资金，一些下游接口对接了一些外部公司的应用，有的可能是按次数收费，有的按流量收费，而这些应用有的没有对应的测试环境，走的都是同一套线上；还有一些写场景，比如下单等，同时该场景可能使用了仿真环境，而且隔离做的不够彻底，下游可能存在连接了某个应用的灰度或线上库，那么就可能对仿真或者线上库造成污染。

![image](/images/autotest/4-1.png )

Qunar录制回放的选型，其实经过了时间的推移也有过迭代。第一个版本大概是三四年前，那个时候还没有jvm-sandbox-repeater这个开源组件，只有jvm-sandbox，所以qunar基于jvm-sandbox，在上面自己定制的封装了录制和回放的能力，实现测试当中子调用和其他组件mock的功能。18年阿里新开源了jvm-sandbox-repeater，就是在jvm-sandbox上面将录制回放的能力抽象实现了一把。Qunar为了利用开源社区的红利，我们迅速的使用了repeater去替换掉了qunar自己内部的实现，同时在上面做了一些小小的定制。

![image](/images/autotest/4-2.png )

接下来我们看一下，整个jvm-sandbox-repeater的结构，它大体可以分为两大部分，一块是数据面，一块是控制面，这个分法其实并不是官方的分法，而是参考了同样为阿里jvm-sandbox系的混沌工程利器chaosblade的分法，chaosblade在最后一个章节也会提到。 这里控制面，主要是进行录制回放请求的下发和和路由的分配，数据面则包含了沙箱核心功能的实现，包括录制回放的各种插件，数据上报以及存储的数据中心等等。

![image](/images/autotest/4-3.png )

接下来再说一下qunar在上面的一些定制处理，整个控制面由qunar自己的自动化测试平台来接管的，包括了中控，case录制、回放以及筛选等逻辑。数据面，自研了一些新的插件，包括dubbo的异步回调，ning的异步httpclient等一些异步rpc场景，还包括qunar内部的一些db和redis的组件，因为qunar的dbclient和redis client也是自研的，所以这部分组件的插件需要自己去搞。 那么数据面里面，case存储管理这块，我们按功能划分拆分成了两个不同的类型，我们使用ES管理索引用来做case的筛选，使用Hbase去存储原始的数据，包括入口和所有子调用录制的信息，用来做case执行回放使用。ES和Hbase的数据通过唯一Id实现关联绑定。

![image](/images/autotest/4-4.png )

录制回放模式落地之后，在推广使用的过程中其实会发现，当我们的测试出现预期差异的时候，相对于普通模式，录制回放模式下，问题定位的难度大大增加，因为引入录制回放组件之后，本身整个测试的复杂度就上升了，因为有mock数据这个依赖，用户在排查问题的时候不像之前纯接口diff那样的简单纯粹，比如说这里的数据不同，产生的diff，它就不单单是结果的diff，还包括了子调用入参的diff。而mock的结果和代码逻辑的变动，可能会共同导致代码流程的不同，也就是方法调用链路不同，这都带来了问题排查的复杂度。

![image](/images/autotest/4-5.png )

那么我们为了去解这最后一公里的问题，我们先看看之前一般有哪些排查问题的手段，总结一下，一般有三大类：第一种是最简单的，就是通过异常栈，因为异常栈信息当中，包含了方法的调用路径，所以通过研究这个路径可以定位到异常位置，调用关系以及引发它的问题；第二种是debug，这是开发最喜欢的一种方式，除了能够看到方法调用链路之外还能看到上下文信息，入参结果取值等等；第三种是arthas，因为有些时候debug不能随心所欲的使用，比如在仿真环境甚至线上，所以借助arthas将debug功能可视化。

![image](/images/autotest/4-6.png )

以上这三种都是JVM中链路排查的一些手段。那么对于跨JVM的链路排查，公司也有qtrace这种全链路追踪系统，能够在更宏观的层面，也就是微服务层面，去看到一次请求在哪个应用会出现问题。 那么我们抽象一下，我们的目标是为了在出问题的时候将JVM的内存方法调用链路去梳理出来，那么将上述宏观的这种应用维度降维成JVM的方法，在微服务内这个微观层面下，如果我们也能梳理出一个JVM方法链路调用拓扑图的话，我们将回放时基准环境的拓扑和测试环境的拓扑进行diff，就基本能够找到从哪个方法起，开始出现的问题，这比改造录制回放组件，在diff处去抛出一个异常，或者是用户debug，包括arthas，都更易用也更直观，大大简化了用户排查问题的路径，也可以降低时间和人力的成本。

![image](/images/autotest/4-7.png )

因为一个JVM内链路上涉及到的方法可能有很多，除了我们自己写的应用代码之外，还包括了jdk的，还有一些公共包比如guava，Apache的公共组件，还有一些公司技术部封装的内部组件，如果把所有的方法链路都串起来，整个链路拓扑可能会非常大，信息量也非常大。而其实真正涉及业务流程的大部分都是我们自己的写的那一部分代码，所以我们提供了一个配置功能，用户可以根据需要，按照包路径，类名，方法名等维度去配置，哪些链路是需要关心的，同时支持了黑白名单，在关心之外的调用链路不会被拉出来，这样可以进一步降低用户的排查成本。 

![image](/images/autotest/4-8.png )

## 5 其他探索和实践

之前的三个大章节说的都是自动化测试的一些本职工作，主要是为了业务去保驾护航。那么除了进行业务回归之外，它还能发挥什么其他的作用，在其他方面去为公司赋能？这里就到了脑洞环节了。那么整个2021年，我们结合公司内的一些其他工具和平台，进行了一些所谓的“杂交”实验，在某些方面产生了一些1+1>2的效果，这里就以三个例子来展开阐述，分别是“自动化测试与混沌工程”，“自动化测试与全链路压测”以及“自动化测试与基础组件升级”这三个方面。

### 5.1 结合混沌工程的实践

混沌工程是qunar在2021年投入很大精力去落地实践的一个领域。其中一个和自动化测试结合的比较好的案例就是“强弱依赖”演练。首先先介绍一下强弱依赖的概念，图中A依赖了B、C、D这三个服务和一个数据库。那么假设，我们的服务D挂了，A还能正常对外提供服务，那么D就是弱依赖，反过来B、C和数据库这其中任何一个挂了，A都无法返回正常结果，那么B、C和数据库就是强依赖。

![image](/images/autotest/5.1-1.png )

举个实际的例子，我们将上述的A、B、C、D和数据映射到qunar机票的预定场景，当然这里是一个简化版的预定流程，主要是为了解释清楚这个问题。A对应机票预定的入口，B对应风控校验系统，数据库对应库存，C对应生单数据处理，D对应大数据平台的分析服务，那么这里看下来，只有D是弱依赖，因为即使我们的订单数据无法进入大数据平台，影响的也只是qunar自身的一个数据分析和统计，对于C端的用户来说，机票还是可以正常购买的，反过来，无论是风控，库存还是生单处理，哪一个环节有问题，用户都会受影响。 上面的例子比较简单，我们人工就能比较清晰的去判断这个例子中的强弱依赖关系。但是真实业务中的依赖关系多而且复杂，一个生单的流程可能涉及上百个应用。 那么强弱依赖演练就是为了发现这些繁杂的应用中，哪些是强依赖，哪些是弱依赖，进而推动业务线去根据数据做系统优化，当然我们最终的目的都是减少强依赖，然后对剩下的为数不多的强依赖进行HA处理。

![image](/images/autotest/5.1-2.png )

下面我们看一下整个强弱依赖演练的流程，先看下和自动化测试平台结合之前的一个旧流程： 在准备阶段，先启动强弱依赖治理，选择对应的应用，然后进行依赖分析，先手动标记，这整个过程主要是人去介入，我们会去借助qtrace产生的拓扑图，加上对业务的一个了解程度，完成手工的操作。 接下来在演练阶段，根据标记结果执行强弱依赖演练，也就是根据这些标记的强弱依赖去注入一些常见的故障和错误场景，然后在这些场景下去观测整个演练的结果——大多数是接口的返回值和业务的流程，来生成强弱依赖报告。最后，我们可以根据出来的报告，去修正之前在准备阶段的手动标记的强弱依赖的内容。可以看到整个过程需要人工参与的部分还是比较多的，主要在依赖分析，手动比较，根据断言生成报告和依赖标记的修正。 那么在这里我们加入自动化测试平台的帮助，就可以让整个流程更加简单，也就是我们所谓的强弱依赖自动演练和标注。那具体怎么做的？

![image](/images/autotest/5.1-3.png )

这里，我们不需要进行依赖分析和手动标记，我们直接使用自动化测试作为流量的入口，将同样的请求分别打到一个正常的基准环境和一个准备好强弱依赖演练的一个测试环境，然后我们借助自动化测试的断言能力对接口的返回结果直接进行智能分析，当断言不通过的时候，直接将本次演练的下游接口标记为强依赖，反之标记为弱依赖。这个环节不单大大提高了自动化的程度，减少了人为交互，提速效果明显，同时，通过自动化测试平台本身case覆盖的保证，让整个比较过程的范围面更广，结果也更加可信了。

![image](/images/autotest/5.1-4.png )

### 5.2 与全链路压测的结合

这里的全链路压测主要借助了自动化测试平台的case获取能力，前面抖了个包袱，在说case生成的时候，提到了checklist过滤和线上随机采样的对比，那么在压测场景下，我们看重的更多是量，而不是业务逻辑覆盖，所以这里我们就可以使用线上随机采样甚至全采，来获得case，用于压测这种需要大量case来探测系统性能的这样一个场景。 同时在一些复杂场景下的mock数据功能使用了刚才我们提到的自动化测试平台的录制回放能力，就是图中所有的这些非灰色标注的部分。因为整个链路在压测的过程中有时候有些服务是不能被压的，比如一些外部依赖，像机票行业，涉及到航司或者代理商，除非你能够说服对方和你一起进行压测，否则这部分数据你只能mock。那么mock数据的准备，当然还是用录制回放的模式会更节省人力，虽然复杂度会提升。尤其像我们公司内部，压测的影子库相关功能的agent和录制回放的agent是两个不同的agent，所以还要去解决agent插桩兼容性的问题，比如同一个字节码修改的切点，如果因为提前返回，导致后生效切点无法工作，那么功能就不能正常完成了。我们在这方面也是做了很多工作，这里就不详细去展开说了，后面在全链路压测topic中会展开去讲。

![image](/images/autotest/5.2-1.png )

### 5.3 基建变更和升级

然后再说一下基础组件自动升级这块。基础组件和中间件的升级其实一直是各大公司比较头疼的话题，这也直接体现了技术部门和业务部门之间的最大矛盾。业务部门更关心的是业务架构是否合理，是否满足业务需求，技术部的一些迭代，比如dubbo跟随社区升级，比如一些apache或者guava包的版本，比如jdk的升级，业务其实不太care，反而如果一旦出现了不兼容的升级，可能会增加业务线开发的成本，比如因为API的改动需要改代码，再比如maven版本管理的变化导致依赖冲突等等。更严重的，如果这些升级因为一些场景没有考虑的很周到，导致业务线升级之后发生了线上的故障，这会大大降低业务线对于公共组件升级的热情和信心，让技术部的同学的工作无法去开展和推进，OKR也没法完成。同时对于技术部同学自己来说，怎么样才算回归完全，我可能即使通宵达旦累死累活的，结果好像也并是很让人满意，反而被业务同学去吐槽。 在这样一个背景下，去借助自动化测试平台的能力，做基础组件自动升级这样一个方案走上了舞台。因为自动化测试的覆盖度和成熟度相对来说都比较高，执行效率也比较让人满意，所以技术部的同学只需要将新的公共版本的分支进行自动化回归，断言不过的时候再看下问题所在。一个晚上不需要熬夜即可轻轻松松覆盖所有应用，第二天早上来看测试结果，根据结果做修正，没有几次回归解决不了的，如果有，那么再来几次。最终，达成的效果，就是关系已经很恶劣的技术部和业务线的同学皆大欢喜，大家各自所做的事情能够互相借力，事情做成了，信赖也建立了，是一个happy ending。

然后有了基础组件升级的这样一个成功的经验之后，我们将这一套流程继续复刻在其他类似的场景上，包括在2021年qunar内部重点落地的从KVM迁移容器的项目，也进行了使用。容器的迁移涉及到的应用范围很大，环境上既包含了测试环境，还包含线上环境。如果单纯靠人去搞，会投入很大的成本。同时，容器成熟之后，后续的一些镜像相关的更新和升级也同理，可以用上这整个回归流程。 这就是我们自动化测试平台和基建升级所做的一个有比较好效果的结合案例。

![image](/images/autotest/5.3-1.png )

## 6 总结展望

最后总结和展望一下，整个案例我们介绍了qunar自动化测试平台在落地实践过程中的一些创新、亮点，以及趟过的坑和积累的经验教训。最后还囊括了一些和其他工具的结合，包括和质量保证息息相关的混沌工程和全链路压测，让整个自动化测试不再局限于它自己的圈子，不再着眼于自己的一亩三分地，因为自动化测试就那么些东西，如果我们固步自封的话，这个测试平台最后只会变成一潭死水。包括以后的展望，我们的方向也是在不断优化测试平台本身的同时，去开更大的脑洞，和前沿新鲜技术多去结合，擦出更多的火花，产生化学效果，衍生出更多的这种1+1>2的案例。 
